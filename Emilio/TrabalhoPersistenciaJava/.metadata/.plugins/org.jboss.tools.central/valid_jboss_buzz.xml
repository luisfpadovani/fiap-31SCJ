<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Teiid 10.2.2 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/n1OvAxefZRM/teiid-1022-released.html" /><category term="feed_group_name_teiid" scheme="searchisko:content:tags" /><category term="feed_name_teiid" scheme="searchisko:content:tags" /><author><name>Steven Hawkins</name></author><id>searchisko:content:id:jbossorg_blog-teiid_10_2_2_released</id><updated>2018-05-23T18:52:45Z</updated><published>2018-05-23T18:52:00Z</published><content type="html">&lt;a href="http://teiid.io/teiid_four_ways/teiid_wildfly/downloads/"&gt;Teiid 10.2.2&lt;/a&gt; addresses 14 issues: &lt;br /&gt;&lt;ul&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5336"&gt;TEIID-5336&lt;/a&gt;] - Improve TEIID-5253 &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-4784"&gt;TEIID-4784&lt;/a&gt;] - Provide functionality to perform RENAME table in DDL scripts &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5324"&gt;TEIID-5324&lt;/a&gt;] - MongoDB: SecurityType "None" is not working &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5328"&gt;TEIID-5328&lt;/a&gt;] - regression of org.teiid.padSpace does not affect to the "IN" operator behavior &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5329"&gt;TEIID-5329&lt;/a&gt;] - Problem with salesforce url &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5330"&gt;TEIID-5330&lt;/a&gt;] - FIRST_VALUE/LAST_VALUE/LEAD/LAG functions always try to return integer &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5331"&gt;TEIID-5331&lt;/a&gt;] - LEAD/LAG ignores ORDER BY in the OVER clause &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5333"&gt;TEIID-5333&lt;/a&gt;] - Complex foreign keys set the referenced key regardless of order &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5334"&gt;TEIID-5334&lt;/a&gt;] - Improve pg/ODBC mapping of char type &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5335"&gt;TEIID-5335&lt;/a&gt;] - "No value was available" in ROW_NUMBER while inserting in foreign temporary table &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5339"&gt;TEIID-5339&lt;/a&gt;] - Vertica join query fails due to unexpected ordering of intermediate results &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5342"&gt;TEIID-5342&lt;/a&gt;] - If excel FIRST_DATA_ROW_NUMBER is past all rows, the last row is still used &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5345"&gt;TEIID-5345&lt;/a&gt;] - ClassCastException if multi-column dependent join is pushed to literals &lt;/li&gt;&lt;li&gt;[&lt;a href="https://issues.jboss.org/browse/TEIID-5347"&gt;TEIID-5347&lt;/a&gt;] - low level MetadataFactory properties not honored by DDL import &lt;/li&gt;&lt;/ul&gt; You can expect the next fix release in 4-5 weeks. Thank you, Steve&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/n1OvAxefZRM" height="1" width="1" alt=""/&gt;</content><summary>Teiid 10.2.2 addresses 14 issues: [TEIID-5336] - Improve TEIID-5253 [TEIID-4784] - Provide functionality to perform RENAME table in DDL scripts [TEIID-5324] - MongoDB: SecurityType "None" is not working [TEIID-5328] - regression of org.teiid.padSpace does not affect to the "IN" operator behavior [TEIID-5329] - Problem with salesforce url [TEIID-5330] - FIRST_VALUE/LAST_VALUE/LEAD/LAG functions alw...</summary><dc:creator>Steven Hawkins</dc:creator><dc:date>2018-05-23T18:52:00Z</dc:date><feedburner:origLink>http://teiid.blogspot.com/2018/05/teiid-1022-released.html</feedburner:origLink></entry><entry><title>Customizing an OpenShift Ansible Playbook Bundle</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/B7RQA9cGh7w/" /><category term="ansible" scheme="searchisko:content:tags" /><category term="apb" scheme="searchisko:content:tags" /><category term="asb" scheme="searchisko:content:tags" /><category term="broker" scheme="searchisko:content:tags" /><category term="bundle" scheme="searchisko:content:tags" /><category term="container" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="Deployment" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="OpenShift Enterprise by Red Hat" scheme="searchisko:content:tags" /><category term="paas" scheme="searchisko:content:tags" /><category term="platform" scheme="searchisko:content:tags" /><category term="playbook" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="service" scheme="searchisko:content:tags" /><author><name>Alessandro Arrichiello</name></author><id>searchisko:content:id:jbossorg_blog-customizing_an_openshift_ansible_playbook_bundle</id><updated>2018-05-23T13:30:11Z</updated><published>2018-05-23T13:30:11Z</published><content type="html">&lt;p&gt;Today I want to talk about Ansible Service Broker and Ansible Playbook Bundle. These components are relatively new in the Red Hat OpenShift ecosystem, but they are now fully supported features available in the Service Catalog component of OpenShift 3.9.&lt;/p&gt; &lt;p&gt;Before getting deep into the technology, I want to give you some basic information (quoted below from the product documentation) about all the components and their features:&lt;/p&gt; &lt;blockquote&gt; &lt;ul&gt; &lt;li&gt;Ansible Service Broker is an implementation of the &lt;a href="https://github.com/openservicebrokerapi/servicebroker"&gt;Open Service Broker API&lt;/a&gt; that manages applications defined in &lt;a href="https://github.com/ansibleplaybookbundle/ansible-playbook-bundle"&gt;Ansible Playbook Bundles&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Ansible Playbook Bundles (APB) are a method of defining applications via a collection of Ansible Playbooks built into a container with an Ansible runtime with the playbooks corresponding to a type of request specified in the &lt;a href="https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md#api-overview"&gt;Open Service Broker API specification&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Playbooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.&lt;/li&gt; &lt;/ul&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;span id="more-495887"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;So the ASB (Ansible Service Broker) is the man-in-the-middle between the APB (Ansible Playbook Bundle) and a third-party user that would like to consume the service offered through the &lt;a href="http://docs.ansible.com/ansible/latest/user_guide/playbooks.html"&gt;Ansible Playbook&lt;/a&gt; on OpenShift.&lt;/p&gt; &lt;p&gt;Linking up these two components, OpenShift Service Catalog is able to offer—through OpenShift Web Portal and its API—access to these pieces of deployment/configuration to OpenShift users. This enables an entire world of possibilities from an OpenShift perspective:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Easily define, distribute, and provision microservice(s), such as &lt;a href="https://github.com/ansibleplaybookbundle/rocketchat-apb"&gt;RocketChat&lt;/a&gt; and &lt;a href="https://github.com/ansibleplaybookbundle/postgresql-apb"&gt;PostgreSQL&lt;/a&gt;, via Ansible Playbooks packaged in Ansible Playbook Bundles.&lt;/li&gt; &lt;li&gt;Easily bind microservice(s) provisioned through Ansible Playbook Bundles, for example, as shown in this video: &lt;a href="https://www.youtube.com/watch?v=xmd52NhEjCk" rel="nofollow"&gt;Using the Service Catalog to Bind a PostgreSQL APB to a Python Web App&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Getting deeper into the technology, we&amp;#8217;ll see how to use the following steps to create a MariaDB APB that will set up MariaDB on an external remote host:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Get started with the technology by creating your first APB.&lt;/li&gt; &lt;li&gt;Customize your first APB and let it configure an external remote host (through SSH).&lt;/li&gt; &lt;li&gt;Build and push your first APB.&lt;/li&gt; &lt;li&gt;Understand the ASB and APB operations under the hood.&lt;/li&gt; &lt;li&gt;Troubleshoot an APB.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Are you ready? Let&amp;#8217;s get started!&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Please note:&lt;/strong&gt; You need to have a fully configured and functional OpenShift 3.9 cluster before continuing. Minishift and the CDK, at the moment, do not offer Service Catalog and Ansible Service Broker enabled. Please check the project documentation.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Getting Started with an APB&lt;/h2&gt; &lt;p&gt;Starting from OpenShift 3.9, you&amp;#8217;ll not need any additional configuration or deployment to get Ansible Service Broker and the Service Catalog working. They will be set up by the OpenShift installer at first installation/update.&lt;/p&gt; &lt;p&gt;So, the first thing you need to do is to let the Ansible Service Broker search in the OpenShift default registry for container images. To achieve this, edit the &lt;code&gt;configmap&lt;/code&gt; used by the Ansible Service Broker and edit the whitelist:&lt;/p&gt; &lt;pre&gt;$ oc edit configmap broker-config -n openshift-ansible-service-broker&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;configmap&lt;/code&gt;, add a whitelist rule for the OpenShift registry similar to the one already set up for the Docker Hub registry:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&amp;#8211; type: local_openshift&lt;br /&gt; name: localregistry&lt;br /&gt; namespaces:&lt;br /&gt; &amp;#8211; openshift&lt;br /&gt; white_list:&lt;br /&gt; &lt;strong&gt;&amp;#8211; &amp;#8220;.*-apb$&amp;#8221;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;With this rule in place, the Ansible Service Broker will search in the local OpenShift registry for container images  ending with the string &lt;code&gt;-apb&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You&amp;#8217;re now ready to initialize your first APB. First, you need the &lt;code&gt;apb&lt;/code&gt; binary. On a Red Hat Enterprise Linux (with OpenShift repos enabled), you just need to run this command:&lt;/p&gt; &lt;pre&gt;$ yum install -y apb&lt;/pre&gt; &lt;p&gt;Then you can initialize your first APB by running this command:&lt;/p&gt; &lt;pre&gt;$ apb init mariadb-deployment-apb&lt;/pre&gt; &lt;p&gt;The command will set up an initial directory tree (shown below), which is ready to be customized depending on your needs:&lt;/p&gt; &lt;pre&gt;$ ls -la mariadb-deployment-apb/ total 12 drwxrwxr-x. 4 ocpadmin ocpadmin 85 May 18 14:08 . drwx------. 9 ocpadmin ocpadmin 255 May 18 14:33 .. -rw-rw-r--. 1 ocpadmin ocpadmin 1082 May 18 13:57 apb.yml -rw-rw-r--. 1 ocpadmin ocpadmin 1731 May 18 14:34 Dockerfile -rw-rw-r--. 1 ocpadmin ocpadmin 769 May 16 12:16 Makefile drwxrwxr-x. 2 ocpadmin ocpadmin 50 May 18 14:33 playbooks drwxrwxr-x. 6 ocpadmin ocpadmin 145 May 16 12:30 roles&lt;/pre&gt; &lt;p&gt;As you can see, the command creates a description file called &lt;code&gt;apb.yml&lt;/code&gt; for metadata and parameters that need to be requested from users of the playbook bundle.&lt;b&gt; &lt;/b&gt;The metadata will be used for displaying the item in the ServiceCatalog, while the parameters will be used to prompt users of the bundle to supply necessary configuration details. We&amp;#8217;ll take a look and customize it in the next section.&lt;/p&gt; &lt;p&gt;Then it creates a Dockerfile for building up the final container and a Makefile, of course, that defines the method for building and pushing the container up to the OpenShift internal registry.&lt;/p&gt; &lt;p&gt;Finally, you&amp;#8217;ll find the two key directories containing—guess what?— Ansible &amp;#8220;playbooks&amp;#8221; and &amp;#8220;roles.&amp;#8221; These directories contain pre-built playbooks for provisioning and de-provisioning and a skeleton for a custom role you may want to build.&lt;/p&gt; &lt;p&gt;But let&amp;#8217;s take a look to the playbook it made:&lt;/p&gt; &lt;pre&gt;$ cat playbooks/provision.yml - name: mariadb-test-apb playbook to provision the application   hosts: localhost   gather_facts: false   connection: local   roles:   - role: ansible.kubernetes-modules     install_python_requirements: no   - role: ansibleplaybookbundle.asb-modules   - role: provision-mariadb-test-apb&lt;/pre&gt; &lt;p&gt;As you can see, the playbook is really simple. It runs against localhost (&lt;code&gt;connection: local&lt;/code&gt;), and it will execute two pre-defined roles: &lt;code&gt;ansible.kubernetes-modules&lt;/code&gt; and &lt;code&gt;ansibleplaybookbundle.asb-modules&lt;/code&gt;. These two roles will set up the basic actions for letting your container communicate with the current OpenShift platform and its underlying Kubernetes layer.&lt;/p&gt; &lt;p&gt;Finally, the playbook will execute a custom role, &lt;code&gt;provision-mariadb-test-apb&lt;/code&gt;. This role is basically empty; you should fill it with your code!&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Customizing Your First APB for Connecting to a Remote Host&lt;/h2&gt; &lt;p&gt;As mentioned in the introduction, you will not use the standard behavior for your APB. Instead, you&amp;#8217;ll make it connect to an external host for installing and configuring MariaDB.&lt;/p&gt; &lt;p&gt;First, you need to edit the &lt;code&gt;apb.yml&lt;/code&gt; file to add some metadata and variables that you&amp;#8217;ll use later in the playbooks:&lt;/p&gt; &lt;pre&gt;$ cat apb.yml version: 1.0 name: mariadb-deployment-apb description: This is a sample application generated by apb init bindable: False async: optional metadata:   displayName: MariaDB on vm (APB) plans:   - name: default     description: This default plan deploys mariadb-deployment-apb     free: True     metadata: {}     parameters:       - name: dbname         title: Database name to create on just created mariadb         type: string         default: myappdb         required: true       - name: rootpassword         title: Database root password to set         type: string         default: P4ssw0rd!         required: true         display_type: password       - name: target_host         title: Target Host for provisioning         type: string         default: 172.16.0.7         required: true       - name: remoteuser         title: SSH Remote User         type: string         default: user         required: true       - name: sshprivkey         title: SSH Private key for connecting to the remote machine         type: string         required: true         display_type: textarea&lt;/pre&gt; &lt;p&gt;As you can see, you set up five parameters that the user will be asked to provide through the OpenShift interface:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The database name&lt;/li&gt; &lt;li&gt;The root password for the database&lt;/li&gt; &lt;li&gt;The target host to connect to&lt;/li&gt; &lt;li&gt;The remote user to use during SSH connection&lt;/li&gt; &lt;li&gt;The SSH private key to use during SSH connection&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As you may suppose, you&amp;#8217;ll not write from scratch a role for installing and configuring MariaDB on your remote system. There are tons of roles available on the Ansible Galaxy network!&lt;/p&gt; &lt;p&gt;For this example, I chose these two (you&amp;#8217;ll need a role for configuring the firewall, too):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://galaxy.ansible.com/bertvv/mariadb"&gt;MariaDB Ansible role&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://galaxy.ansible.com/geerlingguy/firewall"&gt;Firewall Ansible role&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Download them and place them under &lt;code&gt;roles&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;After that, you need to edit the &lt;code&gt;provision.yml&lt;/code&gt; playbook. For connecting to an external host, you need to add the host to the inventory, dynamically.&lt;/p&gt; &lt;pre&gt;$ cat playbooks/provision.yml - name: mariadb-deployment-apb playbook to provision the application hosts: localhost gather_facts: false connection: local roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: provision-mariadb-deployment-apb playbook_debug: false tasks: - name: Adding the remote host to the inventory add_host: name: "{{ target_host }}" groups: target_group changed_when: false - name: Adding ssh private key shell: "mkdir -p /opt/apb/.ssh &amp;#38;&amp;#38; chmod 700 /opt/apb/.ssh &amp;#38;&amp;#38; echo -e \"{{ sshprivkey }}\" &amp;#62; /opt/apb/.ssh/id_rsa &amp;#38;&amp;#38; chmod 600 /opt/apb/.ssh/id_rsa" - name: Provision mariadb hosts: target_group remote_user: "{{ remoteuser }}" become: true vars: firewall_allowed_tcp_ports: - "22" - "3306" mariadb_bind_address: "0.0.0.0" mariadb_root_password: "{{ rootpassword }}" mariadb_databases: - name: "{{ dbname }}" roles: - role: ansible-role-firewall - role: ansible-role-mariadb&lt;/pre&gt; &lt;p&gt;Inspecting the playbook, you&amp;#8217;ll see that first you add the host (from the variable) to the inventory, and then you set up the SSH private key for connecting to the remote host. To accomplish this, I use a single shell command instead of taking the command apart using all the available Ansible modules.&lt;/p&gt; &lt;p&gt;Then in the second playbook, you connect to the remote host to configure the firewall and MariaDB.&lt;/p&gt; &lt;p&gt;So, you&amp;#8217;ll also need to edit the &lt;code&gt;deprovision.yml&lt;/code&gt; playbook:&lt;/p&gt; &lt;pre&gt;$ cat playbooks/deprovision.yml - name: mariadb-deployment-apb playbook to deprovision the application hosts: localhost gather_facts: false connection: local roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: deprovision-mariadb-deployment-apb playbook_debug: false tasks: - name: Adding the remote host to the inventory add_host: name: "{{ target_host }}" groups: target_group changed_when: false - name: Adding ssh private key shell: "mkdir -p /opt/apb/.ssh &amp;#38;&amp;#38; chmod 700 /opt/apb/.ssh &amp;#38;&amp;#38; echo -e \"{{ sshprivkey }}\" &amp;#62; /opt/apb/.ssh/id_rsa &amp;#38;&amp;#38; chmod 600 /opt/apb/.ssh/id_rsa" - name: Remove mariadb packages remote_user: "{{ remoteuser }}" become: yes hosts: target_group tasks: - name: Remove the package from the host package: name: mariadb state: absent&lt;/pre&gt; &lt;p&gt;The deprovisioning is just to remove the &lt;code&gt;mariadb&lt;/code&gt; package and nothing else.&lt;/p&gt; &lt;p&gt;Finally, for connecting smoothly to your remote host without SSH prompting us to add  the host to the known list, you can make a little addition to the Dockerfile to disable host_key_checking:&lt;/p&gt; &lt;pre&gt;$ cat ../mariadb-test-apb/Dockerfile FROM ansibleplaybookbundle/apb-base LABEL "com.redhat.apb.spec"=\ COPY playbooks /opt/apb/actions COPY roles /opt/ansible/roles &lt;strong&gt;RUN echo "host_key_checking = False" &amp;#62;&amp;#62; /opt/apb/ansible.cfg&lt;/strong&gt; RUN chmod -R g=u /opt/{ansible,apb} USER apb&lt;/pre&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Building and Pushing Your First APB&lt;/h2&gt; &lt;p&gt;First, you need to prepare the APB for the push to the registry:&lt;/p&gt; &lt;pre&gt;$ apb prepare Finished writing dockerfile.&lt;/pre&gt; &lt;p&gt;This command adds a signature to the Dockerfile so you can double-checking the build later.&lt;/p&gt; &lt;p&gt;After that, you can build the APB. Remember you need to be &lt;code&gt;root&lt;/code&gt; (or have proper rights for accessing the Docker daemon):&lt;/p&gt; &lt;pre&gt;$ sudo apb build Finished writing dockerfile. Building APB using tag: [mariadb-deployment-apb] Successfully built APB image: mariadb-deployment-apb&lt;/pre&gt; &lt;p&gt;You can double-check the build by checking the list of docker images:&lt;/p&gt; &lt;pre&gt;$ sudo docker images|grep mariadb mariadb-deployment-apb latest b4d6a95a79b7 2 days ago 604 MB&lt;/pre&gt; &lt;p&gt;And finally, you can push the APB into the registry. But before proceeding, you should be logged in to OpenShift as an admin user with a valid token. The user &lt;code&gt;system:admin&lt;/code&gt; doesn&amp;#8217;t have a token by default, so create an additional user and give it the &lt;code&gt;cluster-admin&lt;/code&gt;&amp;#8221; role.&lt;/p&gt; &lt;pre&gt;$ sudo oc whoami ocpadmin $ sudo apb push Didn't find OpenShift Ansible Broker route in namespace: ansible-service-broker. Trying openshift-ansible-service-broker version: 1.0 name: mariadb-deployment-apb description: This is a sample application generated by apb init bindable: False async: optional metadata: displayName: MariaDB on vm (APB) plans: - name: default description: This default plan deploys mariadb-deployment-apb free: True metadata: {} parameters: - name: dbname title: Database name to create on just created mariadb type: string default: myappdb required: true - name: rootpassword title: Database root password to set type: string default: R3dh4t1! required: true display_type: password - name: target_host title: Target Host for provisioning type: string default: 172.16.0.7 required: true - name: remoteuser title: SSH Remote User type: string default: user required: true - name: sshprivkey title: SSH Private key for connecting to the remote machine type: string required: true display_type: textarea Found registry IP at: 172.30.3.246:5000 Finished writing dockerfile. Building APB using tag: [172.30.3.246:5000/openshift/mariadb-deployment-apb] Successfully built APB image: 172.30.3.246:5000/openshift/mariadb-deployment-apb Found image: docker-registry.default.svc:5000/openshift/mariadb-deployment-apb Warning: Tagged image registry prefix doesn't match. Deleting anyway. Given: 172.30.3.246:5000; Found: docker-registry.default.svc:5000 Successfully deleted sha256:0bd78762bbf717333f1e017e3578bcd55a70877810fc7f859d04455e80df0a94 Pushing the image, this could take a minute... Successfully pushed image: 172.30.3.246:5000/openshift/mariadb-deployment-apb Contacting the ansible-service-broker at: https://asb-1338-openshift-ansible-service-broker.140.11.34.16.nip.io/ansible-service-broker/v2/bootstrap Successfully bootstrapped Ansible Service Broker Successfully relisted the Service Catalog&lt;/pre&gt; &lt;p&gt;You did it! You successfully loaded an APB into the OpenShift registry and Ansible Service Broker.&lt;/p&gt; &lt;p&gt;The next time you log in to OpenShift, you should find the APB in the Service Catalog:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-496167 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-1024x446.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-300x131.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-768x334.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console-1024x446.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/p&gt; &lt;p&gt;And moving forward, in the Configuration section, you&amp;#8217;ll see the required variables you configured earlier:&lt;/p&gt; &lt;p&gt;&lt;img class=" alignnone size-full wp-image-496177 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1.png 903w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1-300x224.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-5-20-OpenShift-Web-Console1-768x573.png 768w" sizes="(max-width: 903px) 100vw, 903px" /&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Understanding the ASB and APB Operations Under the Hood&lt;/h2&gt; &lt;p&gt;Requesting the element from the Service Catalog by clicking &lt;strong&gt;Create&lt;/strong&gt; in the previous screen, will start a very special action inside the running OpenShift cluster:&lt;/p&gt; &lt;pre&gt;$ oc get serviceinstance -n test NAME AGE localregistry-mariadb-deployment-apb-bd4cb 27s $ oc get pods --all-namespaces|grep apb localregistry-mariadb-deployment-apb-prov-6pntf apb-f34a346d-3b25-46a5-95c2-54d480ae6701 1/1 Running 0 28s&lt;/pre&gt; &lt;p&gt;As you can see, an element of type &lt;code&gt;ServiceInstance&lt;/code&gt; was created and linked to this, a new pod was scheduled: our Ansible playbook is just running in this container.&lt;/p&gt; &lt;p&gt;Looking through the logs, you can monitor the running activities:&lt;/p&gt; &lt;pre&gt;$ oc logs -n localregistry-mariadb-deployment-apb-prov-6pntf apb-f34a346d-3b25-46a5-95c2-54d480ae6701 -f PLAY [mariadb-deployment-apb playbook to provision the application] ************ TASK [ansible.kubernetes-modules : Install latest openshift client] ************ skipping: [localhost] TASK [ansibleplaybookbundle.asb-modules : debug] ******************************* skipping: [localhost] TASK [Adding the remote host to the inventory] ********************************* ok: [localhost] TASK [Adding ssh private key] ************************************************** [WARNING]: Consider using the file module with state=directory rather than running mkdir. If you need to use command because file is insufficient you can add warn=False to this command task or set command_warnings=False in ansible.cfg to get rid of this message. changed: [localhost] PLAY [Provision mariadb] ******************************************************* TASK [Gathering Facts] ********************************************************* ok: [10.1.0.11] TASK [ansible-role-firewall : Ensure iptables is present.] ********************* ok: [10.1.0.11] TASK [ansible-role-firewall : Flush iptables the first time playbook runs.] **** ok: [10.1.0.11] TASK [ansible-role-firewall : Copy firewall script into place.] **************** ok: [10.1.0.11] TASK [ansible-role-firewall : Copy firewall init script into place.] *********** skipping: [10.1.0.11] TASK [ansible-role-firewall : Copy firewall systemd unit file into place (for systemd systems).] *** ok: [10.1.0.11] TASK [ansible-role-firewall : Configure the firewall service.] ***************** ok: [10.1.0.11] ...&lt;/pre&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;Troubleshooting an APB&lt;/h2&gt; &lt;p&gt;What happens if your tests go wrong and the Ansible pod fails and produces an error?&lt;/p&gt; &lt;p&gt;Of course you can look through all the APB pods (as shown before) and run the usual &lt;code&gt;oc debug PODNAME&lt;/code&gt; command for creating a brand-new pod for troubleshooting.&lt;/p&gt; &lt;p&gt;If you experience some issue deleting a failed &lt;code&gt;ServiceInstance&lt;/code&gt;, you can edit the element to remove the Kubernetes finalizer, as shown below:&lt;/p&gt; &lt;pre&gt;$ oc get serviceinstance -n test -o yaml apiVersion: v1 items: - apiVersion: servicecatalog.k8s.io/v1beta1 kind: ServiceInstance metadata: creationTimestamp: 2018-05-20T21:14:23Z finalizers: - kubernetes-incubator/service-catalog generateName: localregistry-mariadb-deployment-apb- generation: 1 name: localregistry-mariadb-deployment-apb-bd4cb namespace: test ...&lt;/pre&gt; &lt;p&gt;Sometimes, some nodes can get a different version of your APB in the Docker cache, so you might experience different behaviors if you did multiple builds and pushes. You can manually log in to the various OpenShift nodes and then clean up the outdated Docker images (you may want use Ansible from bastion host for doing that).&lt;/p&gt; &lt;p&gt;That&amp;#8217;s all!&lt;/p&gt; &lt;p&gt;Feel free to ask if you have any questions!&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h2&gt;About Alessandro&lt;/h2&gt; &lt;p class="selectionShareable"&gt;&lt;img class="wp-image-496187 alignleft" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture.jpg" alt="" width="180" height="181" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture.jpg 957w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-150x150.jpg 150w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-300x300.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-768x770.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-32x32.jpg 32w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-50x50.jpg 50w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-64x64.jpg 64w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-96x96.jpg 96w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/profile_picture-128x128.jpg 128w" sizes="(max-width: 180px) 100vw, 180px" /&gt;Alessandro Arrichiello is a Solution Architect for Red Hat Inc. He has a passion for GNU/Linux systems, which began at age 14 and continues today. He works with tools for automating enterprise IT: configuration management and continuous integration through virtual platforms. He’s now working on a distributed cloud environment involving PaaS (OpenShift), IaaS (OpenStack) and processes management (CloudForms), container building, instance creation, HA services management, and workflow builds.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;linkname=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F23%2Fcustomizing-an-openshift-ansible-playbook-bundle%2F&amp;#38;title=Customizing%20an%20OpenShift%20Ansible%20Playbook%20Bundle" data-a2a-url="https://developers.redhat.com/blog/2018/05/23/customizing-an-openshift-ansible-playbook-bundle/" data-a2a-title="Customizing an OpenShift Ansible Playbook Bundle"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/23/customizing-an-openshift-ansible-playbook-bundle/"&gt;Customizing an OpenShift Ansible Playbook Bundle&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/B7RQA9cGh7w" height="1" width="1" alt=""/&gt;</content><summary>Today I want to talk about Ansible Service Broker and Ansible Playbook Bundle. These components are relatively new in the Red Hat OpenShift ecosystem, but they are now fully supported features available in the Service Catalog component of OpenShift 3.9. Before getting deep into the technology, I want to give you some basic information (quoted below from the product documentation) about all the com...</summary><dc:creator>Alessandro Arrichiello</dc:creator><dc:date>2018-05-23T13:30:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/23/customizing-an-openshift-ansible-playbook-bundle/</feedburner:origLink></entry><entry><title>Behind the scenes of Red Hat Summit 2018 scheduling</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0tVMjmyUNqw/BehindTheScenesOfRedHatSummitScheduling.html" /><category term="feed_group_name_optaplanner" scheme="searchisko:content:tags" /><category term="feed_name_optaplanner" scheme="searchisko:content:tags" /><category term="useCase" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-behind_the_scenes_of_red_hat_summit_2018_scheduling</id><updated>2018-05-23T16:33:00Z</updated><published>2018-05-23T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Earlier this month, Red Hat organized it’s annual Summit conference in San Francisco for more than 7000 attendees. As &lt;a href="https://youtu.be/r8e4bT0-zhU?t=1m47s"&gt;Jim Whitehurst explained in his opening keynote&lt;/a&gt;, OptaPlanner optimized attendee experience by scheduling all of the 325 non-keynote sessions. Let’s take a look behind the scenes.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_the_challenge_in_theory"&gt;The challenge (in theory)&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A few weeks after the CFP closed and the program group decided which talks to accepts, Arrie Brown (master coordinator of Summit, the business expert) and me (the technical expert) started the automatic scheduling. So for everyone wondering why your brilliant talk wasn’t accepted: &lt;em&gt;it’s not OptaPlanner’s fault!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Instead, it had to assign a timeslot and room to each talk. So for everyone wondering why your brilliant talk didn’t get a better time: &lt;em&gt;it is OptaPlanner’s fault!&lt;/em&gt; My own talk was on the last timeslot of the first day, from 16:30 until 17:15. Traitor. 12 years of dedication to its code and this how it repays me?&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Anyway, given this kind of input:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="summitConferenceScheduling_0.png" alt="summitConferenceScheduling 0"&gt; &lt;/img&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We generated this kind of output:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="summitConferenceScheduling_1.png" alt="summitConferenceScheduling 1"&gt; &lt;/img&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Seems easy, right?&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Of course, there’s a long list of constraints. For starters, there are 5 different talk types (Breakout, Panel, Birds of a Feather, Lab, Workshops, Mini Session) with different durations and different room requirements. A 2 hour Lab doesn’t fit in a 20 minute timeslot. But a Mini Session does. And that’s just the tip of the constraints iceberg:&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="summitConferenceScheduling_2.png" alt="summitConferenceScheduling 2"&gt; &lt;/img&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Arrie created a Google Docs spreadsheet, uploaded the data from the CFP application and ran it through the &lt;a href="https://www.optaplanner.org/learn/useCases/conferenceScheduling.html"&gt;OptaPlanner Conference Scheduling example&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_the_challenge_in_practice"&gt;The challenge (in practice)&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="sect2"&gt; &lt;h3 id="_the_pigeonhole_principle"&gt;The pigeonhole principle&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The first road bump we encountered was that OptaPlanner couldn’t find a feasible schedule. It scheduled dozens of talks in the same room at the same time. That’s a problem. &lt;a href="https://www.optaplanner.org/blog/2018/02/19/SchedulingVoxxedDaysZurich2018.html"&gt;Voxxed Zurich 2018&lt;/a&gt;, didn’t run into this problem, but it only had 1 talk type. It took us a while to figure out the cause, due to talk type complexity, the sheer size of the conference and especially the poor visualization at the time. I even got side-tracked on trying to fix the feasibility, instead of prioritizing the visualization first, for more insight into the result quality.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Once I improved the visualization in the output spreadsheet, the real problem surfaced immediately: &lt;a href="https://en.wikipedia.org/wiki/Pigeonhole_principle"&gt;not enough pigeon holes&lt;/a&gt;. There were 325 talks and only 300 slots to put them in.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We added the 2 missing rooms in the input data and got our first feasible solution. But we didn’t just want a workable schedule: we wanted a great schedule.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_garbage_in_garbage_out"&gt;Garbage in, garbage out&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The CFP application is pretty lenient. During the first import of that data into the OptaPlanner example, Arrie already fixed a bunch of data issues (such as duplicate speaker rows), to get the spreadsheet to import successfully.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;But not all data issues can be detected automatically. For example, it doesn’t automatically know which talks are popular and need a bigger room. Two rooms were already tagged as &lt;code&gt;Large&lt;/code&gt;. Based on experience of previous editions, she identified 7 talks that required such a &lt;code&gt;Large&lt;/code&gt; room. She configured their &lt;code&gt;Required room tags&lt;/code&gt; accordingly.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This kind of experienced, human input is vital to get a good schedule.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_tweaking_the_constraint_weights"&gt;Tweaking the constraint weights&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Attendees should be able to see all talks that they want to see. Ideally, we’d use the mobile app data to know which talks each attendee bookmarked, but that data is only available shortly before the conference, long after the schedule is published. That’s a catch 22. Instead, we avoid scheduling talks at the same time if they cover the same theme track, sector or content, so every attendee can attend all talks on a particular topic.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;These 3 tag types (theme track, sector, content) are not equally important. Some conflicts (2 talks at the same time) are worse than others. There are only a few theme tracks and each talk has multiple of those, so theme track conflicts are common. Therefore, a conflict of 2 talks with the &lt;em&gt;Containers&lt;/em&gt; theme track is less important than a conflict of 2 talks with the &lt;em&gt;Kubernetes&lt;/em&gt; content tag.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The spreadsheet defines the weight of each constraint, allowing to simulate the impact of different weights on the generated schedule. After some tweaking, avoiding content tag conflicts ended up 50 times as important as avoiding theme track conflicts.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some content tags were very common. For example, 46 talks included the content tag &lt;code&gt;OpenShift&lt;/code&gt;. Others only appeared 2 or 3 times. I’d now argue that a conflict with 2 (out of 46) &lt;code&gt;OpenShift&lt;/code&gt; talks is less important than a conflict with the &lt;em&gt;only&lt;/em&gt; 2 talks of another content tag. So for next year, we might want to normalize the impact of every content tag, based on the number of talks with that tag.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_pinning_as_a_workaround"&gt;Pinning as a workaround&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There was one talk that gave us grief, at least for a moment: 2 breakout sessions that were part 1 and part 2 of the same talk. Part 2 needed to start when part 1 ended, in the same room. To proceed quickly, we just &lt;em&gt;pinned&lt;/em&gt; those 2 session manually to a room and timeslots, before solving it. It worked. OptaPlanner scheduled all other talks while respecting those 2 pre-set assignments.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Of course, if we do see more of these 2-part session cases, I 'll add a new constraint to deal with it properly.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We generated a good, fair schedule with the &lt;a href="https://www.optaplanner.org/learn/useCases/conferenceScheduling.html"&gt;OptaPlanner Conference Scheduling example&lt;/a&gt;. Similar to Google Search, we only had to define what we want, not how to look for it.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Once we got the input data in a good state, we just had to press the &lt;em&gt;Solve&lt;/em&gt; button and give it some time. Next year, we’ll be able to reuse this code. If you’re organizing a conference, take a look at the video below to try it out yourself.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_related_video"&gt;Related video&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt; &lt;iframe width="853" height="480" src="https://www.youtube.com/embed/R0JizNdxEjU" frameborder="0" allowfullscreen="" /&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0tVMjmyUNqw" height="1" width="1" alt=""/&gt;</content><summary>Earlier this month, Red Hat organized it’s annual Summit conference in San Francisco for more than 7000 attendees. As Jim Whitehurst explained in his opening keynote, OptaPlanner optimized attendee experience by scheduling all of the 325 non-keynote sessions. Let’s take a look behind the scenes. The challenge (in theory) A few weeks after the CFP closed and the program group decided which talks to...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-23T00:00:00Z</dc:date><feedburner:origLink>https://www.optaplanner.org/blog/2018/05/23/BehindTheScenesOfRedHatSummitScheduling.html</feedburner:origLink></entry><entry><title>Using Ansible Galaxy Roles in Ansible Playbook Bundles</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KhvDAScZbGM/" /><category term="ansible" scheme="searchisko:content:tags" /><category term="apb" scheme="searchisko:content:tags" /><category term="Cloud Automation" scheme="searchisko:content:tags" /><category term="Cloud Services" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Open Service Broker" scheme="searchisko:content:tags" /><category term="OpenShift Enterprise by Red Hat" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Siamak Sadeghianfar</name></author><id>searchisko:content:id:jbossorg_blog-using_ansible_galaxy_roles_in_ansible_playbook_bundles</id><updated>2018-05-22T18:14:52Z</updated><published>2018-05-22T18:14:52Z</published><content type="html">&lt;p&gt;[In case you aren&amp;#8217;t following the &lt;a href="https://blog.openshift.com/"&gt;OpenShift blog&lt;/a&gt;, I&amp;#8217;m cross posting &lt;a href="https://blog.openshift.com/using-ansible-galaxy-roles-in-ansible-playbook-bundles"&gt;my article&lt;/a&gt; here because I think it will be of interest to the Red Hat Developer commnity.]&lt;/p&gt; &lt;p&gt;The Open Service Broker API standard aims to standardize how services (cloud, third-party, on-premise, legacy, etc) are delivered to applications running on cloud platforms like OpenShift. This allows applications to consume services the exact same way no matter on which cloud platform they are deployed. The service broker pluggable architecture enables admins to add third-party brokers to the platform in order to make third-party and cloud services available to the application developers directly from the OpenShift service catalog. As an example &lt;a href="https://aws.amazon.com/partners/servicebroker/"&gt;AWS Service Broker&lt;/a&gt; created jointly by Amazon and Red Hat, &lt;a href="https://github.com/azure/open-service-broker-azure"&gt;Azure Service Broker&lt;/a&gt; created by Microsoft and &lt;a href="https://github.com/google/helm-broker"&gt;Helm Service Broker&lt;/a&gt; created by Google to allow consumption of AWS services, Azure services and Helm charts on Kubernetes and OpenShift. Furthermore, admins can &lt;a href="https://github.com/openshift/open-service-broker-sdk"&gt;create their own brokers&lt;/a&gt; in order to make custom services like provisioning an Oracle database on their internal Oracle RAC available to the developers through the service catalog.&lt;span id="more-496237"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-496277 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1.png 940w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1-300x144.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/service-broker-1-768x368.png 768w" sizes="(max-width: 940px) 100vw, 940px" /&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://automationbroker.io/"&gt;OpenShift Automation Broker&lt;/a&gt; is a service broker that is included in OpenShift out-of-the-box and leverages a lightweight, container-based application definition called an Ansible Playbook Bundle (APB) to automate service provisioning using Ansible. The playbooks can perform actions on OpenShift platform, as well as off platform such as provisioning a virtual machine on VMware and installing a database on it.&lt;/p&gt; &lt;h3&gt;Ansible Galaxy Roles&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://docs.openshift.com/container-platform/3.9/apb_devel/writing/getting_started.html#apb-devel-writing-gs-creating"&gt;current structure of APBs&lt;/a&gt; requires all Ansible roles used in the playbooks to be available in the &lt;code&gt;roles&lt;/code&gt; directory, as Ansible normally demands it.&lt;/p&gt; &lt;pre&gt;myapb ├── apb.yml ├── Dockerfile ├── playbooks │ ├── deprovision.yml │ └── provision.yml └── roles ├── myrole1 └── myrole2 &lt;/pre&gt; &lt;p&gt;While that is useful, the power of Ansible lies in the large ecosystem of Ansible roles that are created by third-parties and the community which simplifies automating virtually anything. The expression “there is role for that!” is not far from reality in the Ansible world.&lt;/p&gt; &lt;p&gt;&lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt; is the hub for finding, reusing and sharing Ansible roles. Whenever one needs to create an Ansible role, the first step is usually to search on Ansible Galaxy for a role that already does the task needed.&lt;/p&gt; &lt;p&gt;Although on the roadmap, currently the Ansible Galaxy roles cannot be directly used in the APB playbooks unless they are already downloaded in the roles directory. Nevertheless, you can still use Ansible Galaxy roles in the APB through a few extra steps which are explained in the next section.&lt;/p&gt; &lt;h3&gt;Using Ansible Galaxy Roles in APBs&lt;/h3&gt; &lt;p&gt;The first step for using the Ansible Galaxy roles is to list those roles as dependencies in a &lt;code&gt;requirements.yml&lt;/code&gt; file in the root of the APB which is the standard way of describing the Ansible Galaxy roles in Ansible:&lt;/p&gt; &lt;pre&gt;- src: siamaksade.openshift_sonatype_nexus version: ocp-3.9 - src: siamaksade.openshift_gogs version: ocp-3.9 - src: siamaksade.openshift_eclipse_che version: ocp-3.9-1 &lt;/pre&gt; &lt;p&gt;The APB directory structure would look like the following:&lt;/p&gt; &lt;pre&gt;myapb/ ├── apb.yml ├── Dockerfile ├── requirements.yml ├── playbooks │ ├── deprovision.yml │ └── provision.yml └── roles ├── myrole1 └── myrole2 &lt;/pre&gt; &lt;p&gt;The above &lt;code&gt;requirements.yml&lt;/code&gt; file declares that this APB requires the &lt;code&gt;openshift_sonatype_nexus&lt;/code&gt;, &lt;code&gt;openshift_gogs&lt;/code&gt; and &lt;code&gt;openshift_eclipse_che&lt;/code&gt; roles from Ansible Galaxy.&lt;/p&gt; &lt;p&gt;Since Ansible Galaxy is not integrated into APBs yet, the next step is to add a few lines to the APB &lt;code&gt;Dockerfile&lt;/code&gt; to install the dependencies when the APB is being built and packaged:&lt;/p&gt; &lt;pre&gt;ADD requirements.yml /opt/apb/actions/requirements.yml RUN ansible-galaxy install -r /opt/apb/actions/requirements.yml &lt;/pre&gt; &lt;p&gt;Now you can use the declared roles in your APB action playbooks, for example in the &lt;code&gt;playbooks/provision.yml&lt;/code&gt; which is the playbook that runs when an APB is provisioned via the OpenShift service catalog:&lt;/p&gt; &lt;pre&gt; roles: - role: ansible.kubernetes-modules install_python_requirements: no - role: ansibleplaybookbundle.asb-modules - role: siamaksade.openshift_sonatype_nexus - role: siamaksade.openshift_gogs - role: siamaksade.openshift_eclipse_che &lt;/pre&gt; &lt;h3&gt;Further Integration With Ansible Galaxy&lt;/h3&gt; &lt;p&gt;In OpenShift 3.11, APBs will become a first class citizen of Ansible Galaxy and can be shared and reused conveniently the very same way that Ansible roles are shared and reused in Ansible Galaxy. The OpenShift Automation Broker will be able to discover APBs directly from Ansible Galaxy which means that a developer can publish their APB source code to Ansible Galaxy and the OpenShift Automation Broker will run the source directly on a special APB base image. This will allow developers to test their source code changes without the hassle of rebuilding the full APB container images each time they want to test it.&lt;/p&gt; &lt;p&gt;Furthermore, the integration with Ansible Galaxy would allow automatic resolution of the role dependencies based on the APB metadata and would allow using any role that is published to Ansible Galaxy and declared as a dependency in the metadata, directly in the playbooks.&lt;/p&gt; &lt;h3&gt;Summary&lt;/h3&gt; &lt;p&gt;APBs enable using Ansible playbooks for automating provisioning of services on OpenShift and also other platforms. Ansible Galaxy enriches the Ansible experience by providing a large set of pre-built roles that can be used to automate virtually anything. By creating a &lt;code&gt;requirements.yml&lt;/code&gt; file and modifying the &lt;code&gt;Dockerfile&lt;/code&gt; in an APB, one can take advantage of the Ansible Galaxy roles in authoring APBs and build complex automation flows.&lt;/p&gt; &lt;p&gt;Ansible Galaxy roles play an important role in APB authoring and therefore native Ansible Galaxy support in APBs is planned for future releases of OpenShift, possibly in OpenShift 3.11.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;linkname=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fusing-ansible-galaxy-roles-in-ansible-playbook-bundles%2F&amp;#38;title=Using%20Ansible%20Galaxy%20Roles%20in%20Ansible%20Playbook%20Bundles" data-a2a-url="https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/" data-a2a-title="Using Ansible Galaxy Roles in Ansible Playbook Bundles"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/"&gt;Using Ansible Galaxy Roles in Ansible Playbook Bundles&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KhvDAScZbGM" height="1" width="1" alt=""/&gt;</content><summary>[In case you aren’t following the OpenShift blog, I’m cross posting my article here because I think it will be of interest to the Red Hat Developer commnity.] The Open Service Broker API standard aims to standardize how services (cloud, third-party, on-premise, legacy, etc) are delivered to applications running on cloud platforms like OpenShift. This allows applications to consume services the exa...</summary><dc:creator>Siamak Sadeghianfar</dc:creator><dc:date>2018-05-22T18:14:52Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/22/using-ansible-galaxy-roles-in-ansible-playbook-bundles/</feedburner:origLink></entry><entry><title>Teiid 10.3.1 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kVXg8DUsmaw/teiid-1031-released.html" /><category term="feed_group_name_teiid" scheme="searchisko:content:tags" /><category term="feed_name_teiid" scheme="searchisko:content:tags" /><author><name>Steven Hawkins</name></author><id>searchisko:content:id:jbossorg_blog-teiid_10_3_1_released</id><updated>2018-05-22T14:27:01Z</updated><published>2018-05-22T14:27:00Z</published><content type="html">We are pleased to announce the release of &lt;a href="https://teiid.github.io/teiid.io/teiid_four_ways/teiid_wildfly/downloads/"&gt;Teiid 10.3.1&lt;/a&gt; (A regression with TEIID-5314 caused an immediate patch release).&amp;nbsp; See all &lt;a href="https://issues.jboss.org/projects/TEIID/versions/12337178"&gt;48 issues&lt;/a&gt; addressed.&amp;nbsp; The feature highlight are:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5293"&gt;TEIID-5293&lt;/a&gt; Added implicit partition wise joining in non-multisource scenarios as well.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5308"&gt;TEIID-5308&lt;/a&gt; Added ENV_VAR and SYS_PROP functions and clarified the usage of the ENV function.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-4745"&gt;TEIID-4745&lt;/a&gt; Added a polling query to trigger the reload of materialized views.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5317"&gt;TEIID-5317&lt;/a&gt; Added ODBC 3.0 functions current_time, current_date, current_timestamp.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5314"&gt;TEIID-5314&lt;/a&gt; Environment variables can now be used instead of or to override system properties.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-5307"&gt;TEIID-5307&lt;/a&gt; Added more information_schema tables for pg compatibility.&lt;/li&gt;&lt;li&gt;&lt;a href="https://issues.jboss.org/browse/TEIID-4864"&gt;TEIID-4864&lt;/a&gt; Added update support to the Excel translator.&lt;/li&gt;&lt;/ul&gt;Special thanks to the many community members who contributed to this release - in particular Andreas Krück, Divyesh Vallabh, Mathieu Rampant, Mike Higgins, Pedro Inácio, Yuming Zhu, Michael Echevarria, and Bram Gadeyne.&lt;br /&gt;&lt;br /&gt;We should also extend a warm welcome to&amp;nbsp;&lt;a href="https://github.com/rkorytkowski"&gt;Rafal Korytkowski&lt;/a&gt; to the Teiid Team. Rafal has begun by finishing off the work on the &lt;a href="https://issues.jboss.org/browse/TEIID-4520"&gt;Exosal translator&lt;/a&gt;&amp;nbsp;and will help with efforts related to cloud and OpenShift support.&lt;br /&gt;&lt;br /&gt;Other Important Stuff:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The new website is (mostly) live! See &lt;a href="https://teiid.io/"&gt;teiid.io&lt;/a&gt;. There are no under construction animated GIFs, but you can expect a few issues as we get the new Hugo/Netlify/Travis build worked out. Notice that we're effectively combining tooling and backend under a single site.&amp;nbsp; The content is being updated to be more relevant for cloud and OpenShift deployments. A lot of work is happening in creating a &lt;a href="https://github.com/teiid/beetle-studio"&gt;self-service data virtualization OpenShift based solution&lt;/a&gt;. If you have any interest in becoming involved, or would like to see more around the &lt;a href="https://github.com/teiid/teiid-komodo/"&gt;image build/configuration&lt;/a&gt; please let us know.&lt;/li&gt;&lt;li&gt;Teiid 10.2.2 and 10.1.4 will be available in the next couple of days. That will effectively end support for the 10.1.x release stream.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;Thank you,&lt;/div&gt;&lt;div&gt;The Teiid Team&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kVXg8DUsmaw" height="1" width="1" alt=""/&gt;</content><summary>We are pleased to announce the release of Teiid 10.3.1 (A regression with TEIID-5314 caused an immediate patch release).  See all 48 issues addressed.  The feature highlight are: TEIID-5293 Added implicit partition wise joining in non-multisource scenarios as well. TEIID-5308 Added ENV_VAR and SYS_PROP functions and clarified the usage of the ENV function. TEIID-4745 Added a polling query to trigg...</summary><dc:creator>Steven Hawkins</dc:creator><dc:date>2018-05-22T14:27:00Z</dc:date><feedburner:origLink>http://teiid.blogspot.com/2018/05/teiid-1031-released.html</feedburner:origLink></entry><entry><title>Teiid 10.1.4 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/bjKDSYxJ9kw/teiid-1014-released.html" /><category term="feed_group_name_teiid" scheme="searchisko:content:tags" /><category term="feed_name_teiid" scheme="searchisko:content:tags" /><author><name>Steven Hawkins</name></author><id>searchisko:content:id:jbossorg_blog-teiid_10_1_4_released</id><updated>2018-05-22T12:19:46Z</updated><published>2018-05-22T12:19:00Z</published><content type="html">Teiid &lt;a href="http://teiid.jboss.org/downloads_10x/"&gt;10.1.4&lt;/a&gt; has been released. It resolves 12 issues: &lt;ul&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5336'&gt;TEIID-5336&lt;/a&gt;] - Improve TEIID-5253 &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-4784'&gt;TEIID-4784&lt;/a&gt;] - Provide functionality to perform RENAME table in DDL scripts &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5319'&gt;TEIID-5319&lt;/a&gt;] - SAP IQ translator wrong pushdown of query with multiple JOINs &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5324'&gt;TEIID-5324&lt;/a&gt;] - MongoDB: SecurityType &amp;quot;None&amp;quot; is not working &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5326'&gt;TEIID-5326&lt;/a&gt;] - SAP IQ timestamp conversion to varchar wrong resulting format &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5328'&gt;TEIID-5328&lt;/a&gt;] - regression of org.teiid.padSpace does not affect to the &amp;quot;IN&amp;quot; operator behavior &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5329'&gt;TEIID-5329&lt;/a&gt;] - Problem with salesforce url &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5330'&gt;TEIID-5330&lt;/a&gt;] - FIRST_VALUE/LAST_VALUE/LEAD/LAG functions always try to return integer &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5331'&gt;TEIID-5331&lt;/a&gt;] - LEAD/LAG ignores ORDER BY in the OVER clause &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5333'&gt;TEIID-5333&lt;/a&gt;] - Complex foreign keys set the referenced key regardless of order &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5339'&gt;TEIID-5339&lt;/a&gt;] - Vertica join query fails due to unexpected ordering of intermediate results &lt;/li&gt;&lt;li&gt;[&lt;a href='https://issues.jboss.org/browse/TEIID-5342'&gt;TEIID-5342&lt;/a&gt;] - If excel FIRST_DATA_ROW_NUMBER is past all rows, the last row is still used &lt;/li&gt;&lt;/ul&gt; This concludes the community releases on 10.1.x. Please upgrade to 10.2 or 10.3. 10.2.2 will be released tomorrow. Thanks, Steve&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/bjKDSYxJ9kw" height="1" width="1" alt=""/&gt;</content><summary>Teiid 10.1.4 has been released. It resolves 12 issues: [TEIID-5336] - Improve TEIID-5253 [TEIID-4784] - Provide functionality to perform RENAME table in DDL scripts [TEIID-5319] - SAP IQ translator wrong pushdown of query with multiple JOINs [TEIID-5324] - MongoDB: SecurityType "None" is not working [TEIID-5326] - SAP IQ timestamp conversion to varchar wrong resulting format [TEIID-5328] - regress...</summary><dc:creator>Steven Hawkins</dc:creator><dc:date>2018-05-22T12:19:00Z</dc:date><feedburner:origLink>http://teiid.blogspot.com/2018/05/teiid-1014-released.html</feedburner:origLink></entry><entry><title>Container Testing in OpenShift with Meta Test Family</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jERJEcGQw38/" /><category term="ci/cd" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="meta test family" scheme="searchisko:content:tags" /><category term="MTF" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="test automation" scheme="searchisko:content:tags" /><category term="testing" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-container_testing_in_openshift_with_meta_test_family</id><updated>2018-05-22T11:00:54Z</updated><published>2018-05-22T11:00:54Z</published><content type="html">&lt;p&gt;Without proper testing, we should not ship any container. We should guarantee that a given service in a container works properly. &lt;a href="https://github.com/fedora-modularity/meta-test-family"&gt;Meta Test Family&lt;/a&gt; (MTF) was designed for this very purpose.&lt;/p&gt; &lt;p&gt;Containers can be tested as “standalone” containers and as “orchestrated” containers. Let’s look at how to test containers with the Red Hat &lt;a href="https://www.openshift.com/"&gt;OpenShift&lt;/a&gt; environment. This article describes how to do that and what actions are needed.&lt;/p&gt; &lt;p&gt;MTF is a &lt;b&gt;minimalistic library&lt;/b&gt; built on the existing &lt;a href="https://avocado-framework.github.io/"&gt;Avocado&lt;/a&gt; and &lt;a href="https://github.com/behave/behave"&gt;behave&lt;/a&gt; testing frameworks, assisting developers in quickly enabling test automation and requirements. MTF adds basic support and abstraction for testing various module artifact types: RPM-based, Docker images, and more. For detailed information about the framework and how to use it check out the &lt;a href="http://meta-test-family.readthedocs.io/en/latest/"&gt;MTF documentation&lt;/a&gt;.&lt;span id="more-495577"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h1&gt;Installing MTF&lt;/h1&gt; &lt;p&gt;Before you can start testing, install MTF from the official EPEL repository using &lt;code&gt;sudo&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;sudo yum install -y meta-test-family&lt;/pre&gt; &lt;p&gt;A COPR repository contains a development version of MTF that should not be used in a production environment.&lt;/p&gt; &lt;p&gt;However, you can install MTF with these commands:&lt;/p&gt; &lt;pre&gt;dnf copr enable phracek/meta-test-family dnf install -y meta-test-family&lt;/pre&gt; &lt;p&gt;To install MTF directly from GitHub, run these commands:&lt;/p&gt; &lt;pre&gt;git clone git@github.com:fedora-modularity/meta-test-family.git cd meta-test-family sudo python setup.py install&lt;/pre&gt; &lt;p&gt;Now, you can start testing containers in the OpenShift environment.&lt;/p&gt; &lt;h1&gt;Prepare a Test for OpenShift&lt;/h1&gt; &lt;p&gt;Running your containers locally is dead-simple: just use the &lt;code&gt;docker run&lt;/code&gt; command. But that’s not how you run your application in production—that’s OpenShift’s business. To make sure your containers are orchestrated well, you should test them in the same environment.&lt;/p&gt; &lt;p&gt;Bear in mind that standalone and orchestrated environments are different. Standalone containers can be executed easily with a single command. Managing such containers isn’t as easy: you need to figure out persistent storage, backups, updates, routing, and scaling—all the things you get for free with orchestrators.&lt;/p&gt; &lt;p&gt;The OpenShift environment has its own characteristics: security restrictions, differences in persistent storage logic, expectation of stateless pods, support for updates, a multi-node environment, native source-to-image support, and much much more.  Deploying an orchestrator here is not an easy task. This is the reason why MTF supports OpenShift: so you can easily test your containerized application in an orchestrated environment.&lt;/p&gt; &lt;p&gt;Before running and preparing the OpenShift environment, you have to create a test and a configuration file for MTF in YAML format. These two files have to be in the same directory, and tests will be executed from the directory.&lt;/p&gt; &lt;h1&gt;Structure of MTF Tests&lt;/h1&gt; &lt;p&gt;Create a directory which will contain the following files:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: The configuration file for MTF&lt;/li&gt; &lt;li&gt;&lt;code&gt;sanity1.py&lt;/code&gt;: The container test that is run by MTF&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Configuration File for MTF&lt;/h1&gt; &lt;p&gt;The configuration file loos like this:&lt;/p&gt; &lt;pre&gt;document: modularity-testing version: 1 name: memcached service:     port: 11211 module:     openshift:            start: VARIABLE_MEMCACHED=60         container: docker.io/modularitycontainers/memcached &lt;/pre&gt; &lt;p&gt;Here’s an explanation of each field in the YAML config file for MTF:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;service.port&lt;/code&gt;: Port where the service is available&lt;/li&gt; &lt;li&gt;&lt;code&gt;module.openshift&lt;/code&gt;: Configuration part relevant only for the OpenShift environment&lt;/li&gt; &lt;li&gt;&lt;code&gt;module.openshift.start&lt;/code&gt;:&lt;b&gt; &lt;/b&gt;Parameters that will be used for testing in OpenShift&lt;/li&gt; &lt;li&gt;&lt;code&gt;module.openshift.container&lt;/code&gt;: Reference to the container, which will be used for testing in OpenShift&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Test for memcached Container&lt;/h1&gt; &lt;p&gt;Here’s an example of a &lt;code&gt;memcached&lt;/code&gt; test for a container:&lt;/p&gt; &lt;pre&gt;$ cat memcached_sanity.py import pexpect from avocado import main from avocado.core import exceptions from moduleframework import module_framework from moduleframework import common class MemcachedSanityCheck(module_framework.AvocadoTest): """ :avocado: enable """ def test_smoke(self):    self.start()    session = pexpect.spawn("telnet %s %s " % (self.ip_address, self.getConfig()['service']['port']))    session.sendline('set Test 0 100 4\r\n\n')    session.sendline('JournalDev\r\n\n')    common.print_info("Expecting STORED")    session.expect('STORED')    common.print_info("STORED was catched")    session.close() if __name__ == '__main__':    main() &lt;/pre&gt; &lt;p&gt;This test connects to &lt;code&gt;memcached&lt;/code&gt; via telnet on the given IP address and port. The port is specified in the MTF configuration file. The following sections speak more about the IP address.&lt;/p&gt; &lt;h1&gt;Prepare OpenShift for Container Testing&lt;/h1&gt; &lt;p&gt;MTF can install the OpenShift environment on your local system with the &lt;code&gt;mtf-env-set&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt;$ &lt;b&gt;sudo MODULE=openshift OPENSHIFT_LOCAL=yes mtf-env-set&lt;/b&gt; Setting environment for module: openshift Preparing environment ... Loaded config for name: memcached Starting OpenShift Starting OpenShift using openshift/origin:v3.6.0 ... OpenShift server started. The server is accessible via web console at: https://127.0.0.1:8443 You are logged in as: User: developer Password: &amp;#60;any value&amp;#62; To login as administrator: oc login -u system:admin &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;mtf-env-set&lt;/code&gt; command checks for a shell variable called &lt;code&gt;OPENSHIFT_LOCAL&lt;/code&gt;. If it is specified, the command checks if the &lt;code&gt;origin&lt;/code&gt; and &lt;code&gt;origin-clients&lt;/code&gt; packages are installed. If they are not, then it installs them.&lt;/p&gt; &lt;p&gt;In this case, a local machine performs the container testing. If you test containers on a remote OpenShift instance, you can ignore this step. If the &lt;code&gt;OPENSHIFT_LOCAL&lt;/code&gt; variable is missing, tests are executed on the remote OpenShift instance specified by the &lt;code&gt;OPENSHIFT_IP&lt;/code&gt; parameter (see below).&lt;/p&gt; &lt;h1&gt;Container Testing&lt;/h1&gt; &lt;p&gt;Now you can test your container either on a local or remote OpenShift instance by using &lt;code&gt;mtf&lt;/code&gt; command. The only difference between the following commands and the previous command is the command parameters.&lt;/p&gt; &lt;p&gt;In the following local testing case, &lt;code&gt;sanity1.py&lt;/code&gt; uses 127.0.0.1 as the value for &lt;code&gt;self.ip_address&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ &lt;strong&gt;sudo MODULE=openshift OPENSHIFT_USER=developer OPENSHIFT_PASSWORD=developer mtf memcached_sanity.py&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;In the following remote testing case, &lt;code&gt;sanity1.py&lt;/code&gt; uses &lt;code&gt;OPENSHIFT_IP&lt;/code&gt; as the value for &lt;code&gt;self.ip_address&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ &lt;strong&gt;sudo OPENSHIFT_IP=&amp;#60;ip_address&amp;#62; OPENSHIFT_USER=&amp;#60;username&amp;#62; OPENSHIFT_PASSWD=&amp;#60;passwd&amp;#62; mtf memcached_sanity.py&lt;/strong&gt;&lt;/pre&gt; &lt;p&gt;Tests are then executed from the environment where you store the configuration file and tests for the given OpenShift instance. The output looks like this:&lt;/p&gt; &lt;pre&gt;JOB ID : c2b0877ca52a14c6c740582c76f60d4f19eb2d4d JOB LOG : &lt;b&gt;/root/avocado/job-results/job-2017-12-18T12.32-c2b0877/job.log&lt;/b&gt; (1/1) memcached_sanity.py:SanityCheck1.test_smoke: &lt;b&gt;PASS&lt;/b&gt; (13.19 s) RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 | INTERRUPT 0 | CANCEL 0 JOB TIME : 13.74 s JOB HTML : /root/avocado/job-results/job-2017-12-18T12.32-c2b0877/results.html $ &lt;/pre&gt; &lt;p&gt;If you open the &lt;code&gt;&lt;b&gt;/&lt;/b&gt;root/avocado/job-results/job-2017-12-18T12.32-c2b0877/job.log&lt;/code&gt;&lt;b&gt; &lt;/b&gt;file, you’ll see contents similar to the example below.&lt;/p&gt; &lt;pre&gt;[...snip...] ['/var/log/messages', '/var/log/syslog', '/var/log/system.log']) 2017-12-18 14:29:36,208 job L0321 INFO | Command line: /bin/avocado run --json /tmp/tmppfZpNe sanity1.py 2017-12-18 14:29:36,208 job L0322 INFO | 2017-12-18 14:29:36,208 job L0326 INFO | Avocado version: 55.0 2017-12-18 14:29:36,208 job L0342 INFO | 2017-12-18 14:29:36,208 job L0346 INFO | Config files read (in order): 2017-12-18 14:29:36,208 job L0348 INFO | /etc/avocado/avocado.conf 2017-12-18 14:29:36,208 job L0348 INFO | /etc/avocado/conf.d/gdb.conf 2017-12-18 14:29:36,208 job L0348 INFO | /root/.config/avocado/avocado.conf 2017-12-18 14:29:36,208 job L0353 INFO | 2017-12-18 14:29:36,208 job L0355 INFO | Avocado config: 2017-12-18 14:29:36,209 job L0364 INFO | Section.Key [...snip...] :::::::::::::::::::::::: SETUP :::::::::::::::::::::::: 2017-12-18 14:29:36,629 avocado_test L0069 DEBUG| :::::::::::::::::::::::: START MODULE :::::::::::::::::::::::: &lt;/pre&gt; &lt;p&gt;MTF verifies whether the application exists in the OpenShift environment:.&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:36,629 process L0389 INFO | Running '&lt;b&gt;oc get dc memcached -o json&lt;/b&gt;' 2017-12-18 14:29:36,842 process L0479 DEBUG| [stderr] Error from server (NotFound): deploymentconfigs.apps.openshift.io "memcached" not found 2017-12-18 14:29:36,846 process L0499 INFO | Command 'oc get dc memcached -o json' finished with 1 after 0.213222980499s &lt;/pre&gt; &lt;p&gt;In the next step, MTF verifies whether the pod exists in OpenShift:&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:36,847 process L0389 INFO | Running '&lt;b&gt;oc get pods -o json&lt;/b&gt;' 2017-12-18 14:29:37,058 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "items": [], 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "kind": "List", 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "metadata": {}, 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "resourceVersion": "", 2017-12-18 14:29:37,059 process L0479 DEBUG| [stdout] "selfLink": "" 2017-12-18 14:29:37,060 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:37,064 process L0499 INFO | Command 'oc get pods -o json' finished with 0 after 0.211796045303s &lt;/pre&gt; &lt;p&gt;The next step creates an application with the given label &lt;code&gt;mtf_testing&lt;/code&gt; and with the name taken from the &lt;code&gt;config.yaml&lt;/code&gt; file in the &lt;code&gt;container&lt;/code&gt; tag.&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:37,064 process L0389 INFO | Running '&lt;b&gt;oc new-app -l mtf_testing=true docker.io/modularitycontainers/memcached --name=memcached&lt;/b&gt;' 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] --&amp;#62; Found Docker image bbc8bba (5 weeks old) from docker.io for "docker.io/modularitycontainers/memcached" 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] memcached is a high-performance, distributed memory object caching system, generic in nature, but intended for use in speeding up dynamic web applications by alleviating database load. 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,022 process L0479 DEBUG| [stdout] Tags: memcached 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * An image stream will be created as "memcached:latest" that will track this image 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * This image will be deployed in deployment config "memcached" 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * Port 11211/tcp will be load balanced by service "memcached" 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] * Other containers can access this service through the hostname "memcached" 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] 2017-12-18 14:29:39,023 process L0479 DEBUG| [stdout] --&amp;#62; Creating resources with label mtf_testing=true ... 2017-12-18 14:29:39,032 process L0479 DEBUG| [stdout] imagestream "memcached" created 2017-12-18 14:29:39,043 process L0479 DEBUG| [stdout] deploymentconfig "memcached" created 2017-12-18 14:29:39,063 process L0479 DEBUG| [stdout] service "memcached" created 2017-12-18 14:29:39,064 process L0479 DEBUG| [stdout] --&amp;#62; Success 2017-12-18 14:29:39,064 process L0479 DEBUG| [stdout] Run 'oc status' to view your app. 2017-12-18 14:29:39,069 process L0499 INFO | Command 'oc new-app -l mtf_testing=true docker.io/modularitycontainers/memcached --name=memcached' finished with 0 after 2.00025391579s &lt;/pre&gt; &lt;p&gt;The next step verifies whether the application is really running and on which IP address it’s reachable:&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:46,201 process L0389 INFO | Running '&lt;b&gt;oc get service -o json&lt;/b&gt;' 2017-12-18 14:29:46,416 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "items": [ 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "kind": "Service", 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "metadata": { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "annotations": { 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] "openshift.io/generated-by": "OpenShiftNewApp" 2017-12-18 14:29:46,417 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "creationTimestamp": "2017-12-18T13:29:39Z", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "labels": { 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "app": "memcached", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "mtf_testing": "true" 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "name": "memcached", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "namespace": "myproject", 2017-12-18 14:29:46,418 process L0479 DEBUG| [stdout] "resourceVersion": "2121", 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "selfLink": "/api/v1/namespaces/myproject/services/memcached", 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "uid": "7f50823d-e3f7-11e7-be28-507b9d4150cb" 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "spec": { 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] &lt;b&gt;"clusterIP": "172.30.255.42"&lt;/b&gt;, 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "ports": [ 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "name": "11211-tcp", 2017-12-18 14:29:46,419 process L0479 DEBUG| [stdout] "port": 11211, 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "protocol": "TCP", 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "targetPort": 11211 2017-12-18 14:29:46,420 process L0499 INFO | Command 'oc get service -o json' finished with 0 after 0.213701963425s 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] ], 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "selector": { 2017-12-18 14:29:46,420 process L0479 DEBUG| [stdout] "app": "memcached", 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "deploymentconfig": "memcached", 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "mtf_testing": "true" 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "sessionAffinity": "None", 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "type": "ClusterIP" 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] }, 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "status": { 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] "loadBalancer": {} 2017-12-18 14:29:46,421 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] } 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] ], 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "kind": "List", 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "metadata": {}, 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "resourceVersion": "", 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] "selfLink": "" 2017-12-18 14:29:46,422 process L0479 DEBUG| [stdout] } &lt;/pre&gt; &lt;p&gt;In the last phase, tests are executed.&lt;/p&gt; &lt;pre&gt;2017-12-18 14:29:46,530 output &lt;b&gt;L0655 DEBUG| Expecting STORED&lt;/b&gt; &lt;b&gt;2017-12-18 14:29:46,531 output L0655 DEBUG| STORED was catched&lt;/b&gt; 2017-12-18 14:29:46,632 avocado_test L0069 DEBUG| :::::::::::::::::::::::: TEARDOWN :::::::::::::::::::::::: 2017-12-18 14:29:46,632 process L0389 INFO | Running 'oc get dc memcached -o json' 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] { 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] "apiVersion": "v1", 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] "kind": "DeploymentConfig", 2017-12-18 14:29:46,841 process L0479 DEBUG| [stdout] "metadata": { &lt;/pre&gt; &lt;p&gt;At the end of the tests, you can verify whether the service is running in the OpenShift environment by using the command &lt;code&gt;oc status&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ &lt;b&gt;sudo oc status&lt;/b&gt; In project My Project (myproject) on server https://127.0.0.1:8443 You have no services, deployment configs, or build configs. Run 'oc new-app' to create an application. &lt;/pre&gt; &lt;p&gt;From this output, you can see that you can test an arbitrary container and afterward, the OpenShift environment is cleared.&lt;/p&gt; &lt;h1&gt;&lt;b&gt;Summary&lt;/b&gt;&lt;/h1&gt; &lt;p&gt;As you have seen in this article, writing tests for containers is really easy. Testing helps you guarantee that a container is working properly just as an RPM package would. In the near future, there are plans to extend MTF capabilities with&lt;a href="https://github.com/openshift/source-to-image"&gt; S2I&lt;/a&gt; testing and testing containers with OpenShift templates. You can read more in the &lt;a href="http://meta-test-family.readthedocs.io/en/latest/"&gt;MTF documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;linkname=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F22%2Fcontainer-testing-in-openshift-with-meta-test-family%2F&amp;#38;title=Container%20Testing%20in%20OpenShift%20with%20Meta%20Test%20Family" data-a2a-url="https://developers.redhat.com/blog/2018/05/22/container-testing-in-openshift-with-meta-test-family/" data-a2a-title="Container Testing in OpenShift with Meta Test Family"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/22/container-testing-in-openshift-with-meta-test-family/"&gt;Container Testing in OpenShift with Meta Test Family&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jERJEcGQw38" height="1" width="1" alt=""/&gt;</content><summary>Without proper testing, we should not ship any container. We should guarantee that a given service in a container works properly. Meta Test Family (MTF) was designed for this very purpose. Containers can be tested as “standalone” containers and as “orchestrated” containers. Let’s look at how to test containers with the Red Hat OpenShift environment. This article describes how to do that and what a...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-22T11:00:54Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/22/container-testing-in-openshift-with-meta-test-family/</feedburner:origLink></entry><entry><title>Narayana JDBC integration for Tomcat</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cov_oZNFoDg/narayana-jdbc-integration-for-tomcat.html" /><category term="feed_group_name_jbosstransactions" scheme="searchisko:content:tags" /><category term="feed_name_transactions" scheme="searchisko:content:tags" /><author><name>Ondřej Chaloupka</name></author><id>searchisko:content:id:jbossorg_blog-narayana_jdbc_integration_for_tomcat</id><updated>2018-05-22T06:24:33Z</updated><published>2018-05-21T22:16:00Z</published><content type="html">&lt;p&gt; Narayana implements JTA specification in Java. It's flexible and easy to be integrated to any system which desires transaction capabilities. As proof of the Narayana extensibility check our quickstarts like &lt;a href="https://github.com/jbosstm/quickstart/tree/master/spring/narayana-spring-boot"&gt;Spring Boot one&lt;/a&gt; or &lt;a href="https://github.com/jbosstm/quickstart/tree/master/spring/camel-with-narayana-spring-boot"&gt;Camel one&lt;/a&gt;. &lt;br/&gt; But this blogpost is different integration effort. It talks in details about Narayana integration with Apache Tomcat server. &lt;/p&gt; &lt;p&gt; If you do not care about details then just jump directly to the Narayana quickstarts in this area and use the code there for yourself. &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/tree/5.8.1.Final/dbcp2-and-tomcat"&gt;dbcp2-and-tomcat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/tree/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat"&gt;transactionaldriver-and-tomcat&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/tree/5.8.1.Final/jca-and-tomcat"&gt;jca-and-tomcat&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; If you want more detailed understanding read further.&lt;br/&gt; All the discussed abilities are considered as the state of Narayana 5.8.1.Final or later. &lt;/p&gt; &lt;h2&gt;Narayana, database resources and JDBC interface&lt;/h2&gt; &lt;p&gt; All the proclaimed Narayana capabilities to integrate with other systems come from requirements for the system to conform with the &lt;a href="https://github.com/javaee/jta-spec"&gt;JTA specification&lt;/a&gt;. JTA expects manageable resources which follows &lt;a href="pubs.opengroup.org/onlinepubs/009680699/toc.pdf"&gt;XA specification&lt;/a&gt; in particular. For case of the database resources the underlaying API is defined by &lt;a href="https://docs.oracle.com/javase/8/docs/technotes/guides/jdbc/"&gt;JDBC specification&lt;/a&gt;. JDBC assembled resources manageable by transaction manager under package &lt;code&gt;javax.sql&lt;/code&gt; &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/sql/package-summary.html"&gt;It defines interfaces&lt;/a&gt; used for managing XA capabilities. The probably most noticable is &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/sql/XADataSource.html"&gt;XADataSource&lt;/a&gt; which serves as factory for &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/sql/XAConnection.html"&gt;XAConnection&lt;/a&gt;. From there we can otain &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/transaction/xa/XAResource.html"&gt;XAResource&lt;/a&gt;. The &lt;code&gt;XAResource&lt;/code&gt; is interface that the transaction manager works with. The instance of it participates in &lt;a href="https://developer.jboss.org/wiki/TwoPhaseCommit2PC"&gt;the two phase commit&lt;/a&gt;. &lt;/p&gt;&lt;p&gt; The workflow is to get or create the &lt;code&gt;XADataSource&lt;/code&gt;, obtains &lt;code&gt;XAConnection&lt;/code&gt; and as next the &lt;code&gt;XAResource&lt;/code&gt; which is enlisted to the global transaction (managed by a transaction manager). Now we can call queries or statements through the &lt;code&gt;XAConnection&lt;/code&gt;. When all the business work is finished the global transaction is commanded to commit which is propagated to call the commit on each enlisted &lt;code&gt;XAResource&lt;/code&gt;s. &lt;/p&gt; &lt;p&gt; It's important to mention that developer is not expected to do all this (getting xa resources, enlisting them to transaction manager&amp;hellip;) All this handling is responsibility of the "container" which could be &lt;a href="http://wildfly.org"&gt;WildFly&lt;/a&gt;, &lt;a href="https://spring.io/guides/gs/managing-transactions"&gt;Spring&lt;/a&gt; or &lt;a href="http://tomcat.apache.org/"&gt;Apache Tomcat&lt;/a&gt; in our case. &lt;br/&gt; Normally the integration which ensures the database &lt;code&gt;XAResource&lt;/code&gt; is enlisted to the transaction is provided by some &lt;i&gt;pooling library&lt;/i&gt;. By the term &lt;i&gt;pooling library&lt;/i&gt; we means code that manages a connection pool with capability enlisting database resource to the transaction. &lt;/p&gt; &lt;p&gt; We can say at the high level that integration parts are &lt;ul&gt; &lt;li&gt;the Apache Tomcat container&lt;/li&gt; &lt;li&gt;Narayana library&lt;/li&gt; &lt;li&gt;jdbc pooling library&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; In this article we will talk about &lt;a href="https://github.com/jbosstm/narayana/tree/5.8.1.Final/ArjunaJTA/jdbc"&gt;Narayana JDBC transactional driver&lt;/a&gt;, &lt;a href="https://github.com/apache/commons-dbcp"&gt;Apache Commons DBCP&lt;/a&gt; and &lt;a href="https://github.com/ironjacamar/ironjacamar"&gt;IronJacamar&lt;/a&gt;. &lt;/p&gt; &lt;h2&gt;Narayana configuration with Tomcat&lt;/h2&gt; &lt;p&gt; After the brief overview of integration requirements, let's elaborate on common settings needed for any integration approach you choose.&lt;br/&gt; Be aware that each library needs a little bit different configuration and especially IronJacamar is specific. &lt;/p&gt; &lt;h3&gt;JDBC pooling libraries integration&lt;/h3&gt; &lt;p&gt; Narayana provides integration code in maven module &lt;a href="https://github.com/jbosstm/narayana/tree/5.8.1.Final/tomcat/tomcat-jta"&gt;&lt;code&gt;tomcat-jta&lt;/code&gt;&lt;/a&gt;. That contains the glue code which integrates Narayana to the world of the Tomcat. If you write an application you will need the following: &lt;ul&gt; &lt;li&gt;providing Narayana itself to the application classpath&lt;/li&gt; &lt;li&gt;providing Narayana &lt;code&gt;tomcat-jta&lt;/code&gt; module to the application classpath&lt;/li&gt; &lt;li&gt;configure &lt;code&gt;WEB-INF/web.xml&lt;/code&gt; with &lt;code&gt;NarayanaJtaServletContextListener&lt;/code&gt; which ensures the intialization of Narayana transaction manager&lt;/li&gt; &lt;li&gt;add &lt;code&gt;META-INF/context.xml&lt;/code&gt; which setup Tomcat to start using implementation of JTA interfaces provided by Narayana&lt;/li&gt; &lt;li&gt;configure database resources to be XA aware and cooperate with Narayana by setting them up in the &lt;code&gt;META-INF/context.xml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; &lt;i&gt;NOTE:&lt;/i&gt; if you expect to use the IronJacamar this requirements differs a bit! &lt;/p&gt; &lt;p&gt; If we take a look at the structure of the jar to be deployed we would get the picture possibly similar to this one: &lt;pre&gt;&lt;br /&gt; ├── META-INF&lt;br /&gt; │   └── context.xml&lt;br /&gt; └── WEB-INF&lt;br /&gt; ├── classes&lt;br /&gt; │   ├── application&amp;hellip;&lt;br /&gt; │   └── jbossts-properties.xml&lt;br /&gt; ├── lib&lt;br /&gt; │   ├── arjuna-5.8.1.Final.jar&lt;br /&gt; │   ├── jboss-logging-3.2.1.Final.jar&lt;br /&gt; │   ├── jboss-transaction-spi-7.6.0.Final.jar&lt;br /&gt; │   ├── jta-5.8.1.Final.jar&lt;br /&gt; │   ├── postgresql-9.0-801.jdbc4.jar&lt;br /&gt; │   └── tomcat-jta-5.8.1.Final.jar&lt;br /&gt; └── web.xml&lt;br /&gt; &lt;/pre&gt;&lt;/p&gt; &lt;p&gt; From this summary let's overview the configuration files one by one to see what's needed to be defined there. &lt;/p&gt; &lt;h3&gt;Configuration files to be setup for the integration&lt;/h3&gt; &lt;h4&gt;WEB-INF/web.xml&lt;/h4&gt;&lt;p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&lt;br /&gt; &amp;lt;web-app&amp;gt;&lt;br /&gt; &amp;lt;listener&amp;gt;&lt;br /&gt; &amp;lt;listener-class&amp;gt;org.jboss.narayana.tomcat.jta.NarayanaJtaServletContextListener&amp;lt;/listener-class&amp;gt;&lt;br /&gt; &amp;lt;/listener&amp;gt;&lt;br /&gt; &amp;lt;/web-app&amp;gt;&lt;br /&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;&lt;p&gt; The &lt;code&gt;web.xml&lt;/code&gt; needs to define the &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.1.Final/tomcat/tomcat-jta/src/main/java/org/jboss/narayana/tomcat/jta/NarayanaJtaServletContextListener.java"&gt;NarayanaJtaServletContextListener&lt;/a&gt; to be loaded during context initialization to initialize the Narayana itself. Narayana needs to get running, for example, reaper thread that ensures transaction timeouts checking or thread of recovery manager. &lt;/p&gt; &lt;h4&gt;WEB-INF/clases/jbossts-properties.xml&lt;/h4&gt;&lt;p&gt; This file is not compulsory. The purpose is to configure the Narayana itself.&lt;br/&gt; If you don't use your own configuration file then the default is in charge. See more at blogpost &lt;a href="https://jbossts.blogspot.cz/2018/01/narayana-periodic-recovery-of-xa.html#configuration"&gt; Narayana periodic recovery of XA transactions &lt;/a&gt; or consider settings done by the default descriptor &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.1.Final/ArjunaJTS/narayana-jts-idlj/src/main/resources/jbossts-properties.xml"&gt;jbossts-properties.xml at narayana-jts-idlj&lt;/a&gt;. &lt;/p&gt; &lt;h4&gt;META-INF/context.xml&lt;/h4&gt; &lt;p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&lt;br /&gt; &amp;lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&amp;gt;&lt;br /&gt; &amp;lt;Context antiJarLocking="true" antiResourceLocking="true"&amp;gt;&lt;br /&gt; &amp;lt;!-- Narayana resources --&amp;gt;&lt;br /&gt; &amp;lt;Transaction factory="org.jboss.narayana.tomcat.jta.UserTransactionFactory"/&amp;gt;&lt;br /&gt; &amp;lt;Resource factory="org.jboss.narayana.tomcat.jta.TransactionManagerFactory"&lt;br /&gt; name="TransactionManager" type="javax.transaction.TransactionManager"/&amp;gt;&lt;br /&gt; &amp;lt;Resource factory="org.jboss.narayana.tomcat.jta.TransactionSynchronizationRegistryFactory"&lt;br /&gt; name="TransactionSynchronizationRegistry" type="javax.transaction.TransactionSynchronizationRegistry"/&amp;gt;&lt;br /&gt;&lt;br /&gt; &amp;lt;Resource auth="Container" databaseName="test" description="Data Source"&lt;br /&gt; factory="org.postgresql.xa.PGXADataSourceFactory" loginTimeout="0"&lt;br /&gt; name="myDataSource" password="test" portNumber="5432" serverName="localhost"&lt;br /&gt; type="org.postgresql.xa.PGXADataSource" user="test" username="test"&lt;br /&gt; uniqueName="myDataSource" url="jdbc:postgresql://localhost:5432/test"/&amp;gt;&lt;br /&gt; &amp;lt;Resource auth="Container" description="Transactional Data Source"&lt;br /&gt; factory="org.jboss.narayana.tomcat.jta.TransactionalDataSourceFactory"&lt;br /&gt; initialSize="10" jmxEnabled="true" logAbandoned="true" maxAge="30000"&lt;br /&gt; maxIdle="16" maxTotal="4" maxWaitMillis="10000" minIdle="8"&lt;br /&gt; name="transactionalDataSource" password="test" removeAbandoned="true"&lt;br /&gt; removeAbandonedTimeout="60" testOnBorrow="true" transactionManager="TransactionManager"&lt;br /&gt; type="javax.sql.XADataSource" uniqueName="transactionalDataSource"&lt;br /&gt; username="test" validationQuery="select 1" xaDataSource="myDataSource"/&amp;gt;&lt;br /&gt; &amp;lt;/Context&amp;gt;&lt;br /&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/p&gt; &lt;p&gt; I divide explanation this file into two parts. First are the generic settings - those needed for transaction manager integration (top part of the &lt;code&gt;context.xml&lt;/code&gt;). The second part is on resource declaration that defines linking to the JDBC pooling library. &lt;/p&gt; &lt;h5&gt;Transaction manager integration settings&lt;/h5&gt; &lt;p&gt; We define implementation classes for the &lt;a href="https://github.com/eclipse-ee4j/jta-api"&gt;JTA api&lt;/a&gt; here. The implementation is provided by Narayana transaction manager. Those are lines of &lt;code&gt;UserTransactionFactory&lt;/code&gt; and resources of &lt;code&gt;TransactionManager&lt;/code&gt; and &lt;code&gt;TransactionSynchronizationRegistry&lt;/code&gt; in the &lt;code&gt;context.xml&lt;/code&gt; file.&lt;br/&gt;&lt;/p&gt; &lt;h5&gt;JDBC pooling library settings&lt;/h5&gt; &lt;p&gt; We aim to define database resources that can be used in the application. That's how you &lt;a href="https://github.com/jbosstm/quickstart/blob/master/dbcp2-and-tomcat/src/main/java/io/narayana/StringDao.java#L119"&gt;get the connection&lt;/a&gt; typically with code &lt;code class="java"&gt;DataSource ds = InitialContext.doLookup("java:comp/env/transactionalDataSource")&lt;/code&gt;, and eventually &lt;a href="https://github.com/jbosstm/quickstart/blob/master/dbcp2-and-tomcat/src/main/java/io/narayana/StringDao.java#L81"&gt;execute a sql statement&lt;/a&gt;. &lt;br/&gt; We define a PostgreSQL datasource with information how to create a new XA connection (we provide the host and port, credentials etc.) in the example.&lt;br/&gt; The second resource is definition of jdbc pooling library to utilize the PostgreSQL one and to provide the XA capabilities. It roughtly means putting the PostgreSQL connection to the managed pool and enlisting the work under an active transaction. &lt;br/&gt; Thus we have got two resources defined here. One is non-managed (the PosgreSQL one) and the second manages the first one to provide the ease work with the resources. For the developer is the most important to know he needs to use the managed one in his application, namely the &lt;code&gt;transactionalDataSource&lt;/code&gt; from our example. &lt;/p&gt; &lt;h5&gt;A bit about datasource configuration of Apache Tomcat context.xml&lt;/h5&gt; &lt;p&gt; Let's take a side step at this place. Before we will talk in details about supported pooling libraries let's check a little bit more about the configuration of the &lt;code&gt;Resource&lt;/code&gt; from perspective of XA connection in the &lt;code&gt;context.xml&lt;/code&gt;. &lt;/p&gt;&lt;p&gt; Looking at the &lt;code&gt;Resource&lt;/code&gt; definition there are highlighted parts which are interesting for us &lt;pre&gt;&lt;br /&gt; &amp;lt;Resource auth="Container" &lt;span style="color:rgb(255, 150, 200)"&gt;databaseName="test"&lt;/span&gt; description="Data Source"&lt;br /&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;factory="org.postgresql.xa.PGXADataSourceFactory"&lt;/span&gt;&lt;br /&gt; loginTimeout="0" &lt;span style="color:rgb(255, 150, 200)"&gt;name="myDataSource"&lt;/span&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;password="test"&lt;/span&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;portNumber="5432"&lt;/span&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;serverName="localhost"&lt;/span&gt;&lt;br /&gt; &lt;span style="color:rgb(255, 150, 200)"&gt;type="org.postgresql.xa.PGXADataSource"&lt;/span&gt; uniqueName="myDataSource"&lt;br /&gt; url="jdbc:postgresql://localhost:5432/test" &lt;span style="color:rgb(255, 150, 200)"&gt;user="test"&lt;/span&gt; username="test"/&amp;gt;&lt;br /&gt; &lt;/pre&gt;&lt;/p&gt;&lt;p&gt; &lt;ul&gt; &lt;dl&gt; &lt;dt&gt;&lt;code&gt;name&lt;/code&gt;&lt;/dt&gt; &lt;dd&gt;defines the name the resource is bound at the container and we can use the jndi lookup to find it by that name in application&lt;/dd&gt; &lt;dt&gt;&lt;code&gt;factory&lt;/code&gt;&lt;/dt&gt; &lt;dd&gt;defines what type we will get as the final created &lt;code&gt;Object&lt;/code&gt;. The factory which we declares here is class which implements interface &lt;a href="https://docs.oracle.com/javase/8/docs/api/javax/naming/spi/ObjectFactory.html"&gt;ObjectFactory&lt;/a&gt; and from the provided properties it construct an object.&lt;br/&gt; If we would not define any &lt;code&gt;factory&lt;/code&gt; element in the definition then the Tomcat class &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/naming/factory/ResourceFactory.java#L42"&gt;ResourceFactory&lt;/a&gt; is used (see &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/naming/factory/Constants.java#L26"&gt;default factory constants&lt;/a&gt;). The &lt;code&gt;ResourceFactory&lt;/code&gt; will pass the call to the &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/tomcat/dbcp/dbcp2/BasicDataSourceFactory.java"&gt;BasicDataSourceFactory&lt;/a&gt; of the dbcp2 library. Here we can see the importantce of the &lt;code&gt;type&lt;/code&gt; xml parameter which defines what is the object type we want to obtain and the factory normally checks if it's able to provide such (by &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/tomcat/dbcp/dbcp2/BasicDataSourceFactory.java#L251"&gt;string equals check&lt;/a&gt; usually).&lt;br/&gt; The next step is generation of the object itself where the factory takes each of the &lt;a href="https://github.com/apache/tomcat/blob/trunk/java/org/apache/tomcat/dbcp/dbcp2/BasicDataSourceFactory.java#L365"&gt;properties and tries to applied&lt;/a&gt; them. &lt;br/&gt; In our case we use the &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/xa/PGXADataSourceFactory.java"&gt;PGXADataSourceFactory&lt;/a&gt; which utilizes &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/ds/common/BaseDataSource.java#L1212"&gt;some of the properties to create the &lt;code&gt;XADataSource&lt;/code&gt;&lt;/a&gt;. &lt;/dd&gt; &lt;dt&gt;&lt;code&gt;serverName&lt;/code&gt;, &lt;code&gt;portNumber&lt;/code&gt;, &lt;code&gt;databaseName&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, &lt;code&gt;password&lt;/code&gt;&lt;/dt&gt; &lt;dd&gt;are properties used by the &lt;code&gt;object factory&lt;/code&gt; class to get connection from the database &lt;br/&gt; Knowing the name of the properties for the particular &lt;code&gt;ObjectFactory&lt;/code&gt; is possibly the most important when you need to configure your datasource. Here you need to check &lt;code&gt;setters&lt;/code&gt; of the factory implementation. &lt;br/&gt; In case of the &lt;code&gt;PGXADataSourceFactory&lt;/code&gt; we need to go through the inheritance hierarchy to find the properties are saved at &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/ds/common/BaseDataSource.java"&gt;BaseDataSource&lt;/a&gt;. For our case for the relevant properties are &lt;code&gt;user name&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt;. From the &lt;code&gt;BaseDataSource&lt;/code&gt; we can see the setter for the user name is &lt;a href="https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/ds/common/BaseDataSource.java#L190"&gt;setUser&lt;/a&gt; thus the property name we look for is &lt;code&gt;user&lt;/code&gt;. &lt;/dd&gt; &lt;/dl&gt; &lt;/ul&gt;&lt;/p&gt; &lt;p&gt; After this side step let's take a look at the setup of the &lt;code&gt;Resource&lt;/code&gt;s in respect of the used pooling library. &lt;/p&gt; &lt;h3&gt;Apache Commons DBCP2 library&lt;/h3&gt; &lt;p&gt;&lt;b&gt;Quickstart:&lt;/b&gt; &lt;a href="https://github.com/jbosstm/quickstart/tree/master/dbcp2-and-tomcat"&gt;https://github.com/jbosstm/quickstart/tree/master/dbcp2-and-tomcat&lt;/a&gt;&lt;/p&gt;&lt;p&gt; The best integration comes probably with Apache Common DBCP2 as the library itself is part of the Tomcat distribution (the Tomcat code base uses &lt;a href="https://github.com/apache/tomcat/tree/trunk/java/org/apache/tomcat/dbcp/dbcp2"&gt;fork of the project&lt;/a&gt;). The XA integration is provided in Apache Tomcat version 9.0.7 and later. There is added dbcp2 package &lt;a href="https://github.com/apache/tomcat/tree/trunk/java/org/apache/tomcat/dbcp/dbcp2/managed"&gt;managed&lt;/a&gt; which knows how to enlist a resource to XA transaction. &lt;/p&gt; &lt;p&gt; The integration is similar to what we discussed in case of the JDBC transactional driver. You need to have configured two resources in &lt;code&gt;context.xml&lt;/code&gt;. One is the database datasource (&lt;a href="#contextxmldatasource"&gt;see above&lt;/a&gt;) and other is wrapper providing XA capabilities. &lt;/p&gt; &lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;Resource name="transactionalDataSource" uniqueName="transactionalDataSource"&lt;br /&gt; auth="Container" type="javax.sql.XADataSource"&lt;br /&gt; transactionManager="TransactionManager" xaDataSource="h2DataSource"&lt;br /&gt; factory="org.jboss.narayana.tomcat.jta.TransactionalDataSourceFactory"/&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; The integration is here done over the use of the specific &lt;code&gt;factory&lt;/code&gt; which directly depends on classes from Apache Tomcat &lt;code&gt;org.apache.tomcat.dbcp.dbcp2&lt;/code&gt; package. The factory ensures the resource being enlisted to the recovery manager as well.&lt;br/&gt; The nice feature is that you can use all the DBCP2 configuration parameters for pooling as you would used when &lt;code&gt;BasicDataSource&lt;/code&gt; is configured. See the configuration options and the their meaning at the &lt;a href="https://commons.apache.org/proper/commons-dbcp/configuration.html"&gt;Apache Commons documentation&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; &lt;b&gt;Summary:&lt;/b&gt; &lt;ul&gt; &lt;li&gt;Already packed in the &lt;code&gt;Apache Tomcat&lt;/code&gt; distribution from version 9.0.7&lt;/li&gt; &lt;li&gt;Configure two resources in &lt;code&gt;context.xml&lt;/code&gt;. One is the database datasource, the second is wrapper providing XA capabilities with use of the &lt;code&gt;dbcp2&lt;/code&gt; pooling capabilities integrated with &lt;code&gt;TransactionalDataSourceFactory&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/p&gt; &lt;h3&gt;Narayana jdbc transactional driver&lt;/h3&gt; &lt;p&gt;&lt;b&gt;Quickstart:&lt;/b&gt; &lt;a href="https://github.com/jbosstm/quickstart/tree/master/transactionaldriver/transactionaldriver-and-tomcat"&gt;https://github.com/jbosstm/quickstart/tree/master/transactionaldriver/transactionaldriver-and-tomcat&lt;/a&gt;&lt;/p&gt;&lt;p&gt;With this we will get back to other two recent articles about &lt;a href="https://jbossts.blogspot.cz/2017/12/narayana-jdbc-transactional-driver.html"&gt;jdbc transactional driver&lt;/a&gt; and &lt;a href="https://jbossts.blogspot.cz/2018/01/recovery-of-narayana-jdbc-transactional.html"&gt;recovery of the transactional driver&lt;/a&gt;.&lt;br/&gt;The big advantage of jdbc transactional driver is its tight integration with Narayana. It's the dependecy of the &lt;a href="https://github.com/jbosstm/narayana/tree/master/tomcat/tomcat-jta"&gt;Narayana &lt;code&gt;tomcat-jta&lt;/code&gt;&lt;/a&gt;module which contains all the integration code needed for Narayana working in Tomcat. So if you take the &lt;code&gt;tomcat-jta-5.8.1.Final&lt;/code&gt;you have packed the Narayna integration code and jdbc driver in out-of-the-box working bundle. &lt;/p&gt; &lt;h4&gt;Configuration actions&lt;/h4&gt;&lt;p&gt; Here we will define two resources in the &lt;code&gt;context.xml&lt;/code&gt; file. The first one is &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/src/main/webapp/META-INF/context.xml#L11"&gt;the database one&lt;/a&gt;. &lt;/p&gt; &lt;a id="contextxmldatasource" name="contextxmldatasource"&gt;&lt;/a&gt;&lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;Resource name="h2DataSource" uniqueName="h2Datasource" auth="Container"&lt;br /&gt; type="org.h2.jdbcx.JdbcDataSource" username="sa" user="sa" password="sa"&lt;br /&gt; url="jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1" description="H2 Data Source"&lt;br /&gt; loginTimeout="0" factory="org.h2.jdbcx.JdbcDataSourceFactory"/&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; The database one defines data needed for preparation of datasource and creation of the connection. The datasource is not XA aware. We need to add one more layer on top which is transactional JDBC driver here. &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/src/main/webapp/META-INF/context.xml#L14"&gt;It wraps the datasource connection&lt;/a&gt; within XA capabilities. &lt;/p&gt; &lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;Resource name="transactionalDataSource" uniqueName="transactionalDataSource"&lt;br /&gt; auth="Container" type="javax.sql.DataSource" username="sa" password="sa"&lt;br /&gt; driverClassName="com.arjuna.ats.jdbc.TransactionalDriver"&lt;br /&gt; url="jdbc:arjuna:java:comp/env/h2DataSource" description="Transactional Driver Datasource"&lt;br /&gt; connectionProperties="POOL_CONNECTIONS=false"/&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; As we do not define the element &lt;code&gt;factory&lt;/code&gt; then the default one is used which is &lt;code&gt;org.apache.tomcat.dbcp.dbcp2.BasicDataSourceFactory&lt;/code&gt;. Unfortunately, this is fine up to the time you need to process some more sophisticated pooling strategies. In this aspect the transactional driver does not play well with the default factory and some further integration work would be needed. &lt;/p&gt; &lt;p&gt; This configuration is nice for having &lt;code&gt;transactionalDataSource&lt;/code&gt; available for the transactional work. Unfortunately, it's not all that you need to do. You miss here configuration of recovery. You need to tell the recovery manager what is the resource to care of. You can setup this in &lt;code&gt;jbossts-properties.xml&lt;/code&gt; or maybe easier way to add it to &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/run.sh#L10"&gt;environment variables of the starting Tomcat&lt;/a&gt;, for example by adding the setup under script &lt;code&gt;$CATALINA_HOME/bin/setenv.sh&lt;/code&gt;&lt;br/&gt; You define it with property &lt;a href="https://github.com/jbosstm/narayana/blob/master/ArjunaJTA/jta/classes/com/arjuna/ats/jta/common/JTAEnvironmentBean.java#L66"&gt;com.arjuna.ats.jta.recovery.XAResourceRecovery&lt;/a&gt;. &lt;/p&gt; &lt;pre&gt;&lt;code style="bash"&gt;&lt;br /&gt;-Dcom.arjuna.ats.jta.recovery.XAResourceRecovery1=com.arjuna.ats.internal.jdbc.recovery.BasicXARecovery;abs://$(pwd)/src/main/resources/h2recoveryproperties.xml&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; You can define whatever number of the resources you need the recovery is aware of. It's done by adding more &lt;code&gt;numbers&lt;/code&gt; at the end of the property name (we use &lt;code&gt;1&lt;/code&gt; in the example above). The value of the property is the class implementing &lt;code&gt;com.arjuna.ats.jta.recovery.XAResourceRecovery&lt;/code&gt;. All the properties provided to the particular implementation is concatenated after the &lt;code&gt;;&lt;/code&gt; character. In our example it's path to the &lt;code&gt;xml descriptor h2recoveryproperties.xml&lt;/code&gt;.&lt;br/&gt; When transactional driver is used then you need to declare&lt;code&gt;BasicXARecovery&lt;/code&gt; as recovery implementation class and this class needs &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/transactionaldriver/transactionaldriver-and-tomcat/src/main/resources/h2recoveryproperties.xml"&gt;connection properties&lt;/a&gt; to be declared in the xml descriptor. &lt;/p&gt; &lt;pre&gt;&lt;code style="xml"&gt;&lt;br /&gt; &amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;&lt;br /&gt;&amp;lt;!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd"&amp;gt;&lt;br /&gt;&amp;lt;properties&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabaseUser"&amp;gt;sa&amp;lt;/entry&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabasePassword"&amp;gt;sa&amp;lt;/entry&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabaseDynamicClass"&amp;gt;&amp;lt;/entry&amp;gt;&lt;br /&gt; &amp;lt;entry key="DB_1_DatabaseURL"&amp;gt;java:comp/env/h2DataSource&amp;lt;/entry&amp;gt;&lt;br /&gt;&amp;lt;/properties&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; Note: there is option not defining the two resources under &lt;code&gt;context.xml&lt;/code&gt; and use the env property for recovery enlistment. All the configuration properties are then involved in one &lt;code&gt;properties&lt;/code&gt; file and &lt;a href="https://jbossts.blogspot.cz/2017/12/narayana-jdbc-transactional-driver.html"&gt;transactional driver dynamic class&lt;/a&gt; is used. If interested the working example is at &lt;a href="https://github.com/ochaloup/quickstart-jbosstm/tree/transactional-driver-and-tomcat-dynamic-class/transactionaldriver/transactionaldriver-and-tomcat"&gt;ochaloup/quickstart-jbosstm/tree/transactional-driver-and-tomcat-dynamic-class&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; &lt;b&gt;Summary:&lt;/b&gt; &lt;ul&gt; &lt;li&gt;Already packed in the &lt;code&gt;tomcat-jta&lt;/code&gt; artifact&lt;/li&gt; &lt;li&gt;Configure two resources in &lt;code&gt;context.xml&lt;/code&gt;. One is database datasource, the second is transactional datasource wrapped by transactional driver.&lt;/li&gt; &lt;li&gt;Need to configure recovery with env variable setup &lt;code&gt;com.arjuna.ats.jta.recovery.XAResourceRecovery&lt;/code&gt; while providing xml descriptor with connection parameters&lt;/li&gt; &lt;/ul&gt;&lt;/p&gt; &lt;h3&gt;IronJacamar&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Quickstart:&lt;/b&gt; &lt;a href="https://github.com/jbosstm/quickstart/tree/master/jca-and-tomcat"&gt;https://github.com/jbosstm/quickstart/tree/master/jca-and-tomcat&lt;/a&gt;&lt;/p&gt; &lt;p&gt; The settings of IronJacamar integration differs pretty much from what we've seen so far. The IronJacamar implements whole JCA specification and it's pretty different beast (not &lt;i&gt;just&lt;/i&gt; a jdbc pooling library). &lt;/p&gt; &lt;p&gt; The whole handling and integration is passed to IronJacamar itself.&lt;br/&gt; You don't use &lt;code&gt;tomcat-jta&lt;/code&gt; module at all.&lt;br/&gt; You need to configure all aspects in the IronJacamar &lt;code&gt;xml descriptors&lt;/code&gt;. Aspects like datasource definition, transaction configuration, pooling definition, up to the jndi binding. &lt;/p&gt; &lt;p&gt; The standalone IronJacamar is needed to be started with command &lt;code&gt;org.jboss.jca.embedded.EmbeddedFactory.create().startup()&lt;/code&gt; where you &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/test/java/org/jboss/narayana/quickstart/jca/common/AbstractTest.java#L51"&gt;defines the descriptors to be used&lt;/a&gt;. You can configure it in &lt;code&gt;web.xml&lt;/code&gt; as &lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/java/org/jboss/narayana/quickstart/jca/listener/ServletContextListenerImpl.java"&gt;&lt;code&gt;ServletContextListener&lt;/code&gt;&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; What are descriptors to be defined: &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/resources/jdbc-xa.rar"&gt;jdbc-xa.rar&lt;/a&gt; which is resource adapter provided by IronJacamar itself. It needs to be part of the deployment. It's capable to process &lt;code&gt;ds&lt;/code&gt; files. &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/resources/postgres-xa-ds.xml"&gt;ds.xml&lt;/a&gt; which defines connecion properties and jndi name binding &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jbosstm/quickstart/blob/5.8.1.Final/jca-and-tomcat/src/main/resources/transaction.xml"&gt;transaction.xml&lt;/a&gt; which configures transaction manager instead of use of the &lt;code&gt;jbossts-properties.xml&lt;/code&gt;. &lt;/li&gt; &lt;/ul&gt; Check more configuration in &lt;a href="http://www.ironjacamar.org/doc/userguide/1.2/en-US/html_single/index.html"&gt;IronJacamar documentation&lt;/a&gt;. &lt;/p&gt; &lt;p&gt; &lt;b&gt;Summary:&lt;/b&gt; IronJacamar is started as embedded system and process all the handling on its own. Developer needs to provide xml descriptor to set up. &lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt; This article provides the details about configuration of the Narayana when used in Apache Tomcat container. We've seen the three possible libraries to get the integration working - the Narayana JDBC transactional driver, Apache DBCP2 library and IronJacamar JCA implementation.&lt;br/&gt; On top of it, the article contains many details about Narayana and Tomcat resource configuration. &lt;/p&gt; &lt;p&gt;If you hesitate what alternative is the best fit for your project then this table can help you &lt;table border="1" style="border-collapse: collapse; width: 100%; border: 1px solid #dddddd; text-align: left; padding: 8px;"&gt; &lt;tr style="background-color: #dddddd;"&gt; &lt;th width="180"&gt;JDBC integration library&lt;/th&gt; &lt;th&gt;When to use&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Apache DBCP2&lt;/td&gt; &lt;td&gt;It's the recommended option when you want to obtain Narayana transaction handling in the Apache Tomcat Integration is done in the Narayana resource factory which ensures easily setting up the datasource and recovery in the one step. &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Narayana transactional jdbc driver&lt;/td&gt; &lt;td&gt;Is good fit when you want to have all parts integrated and covered by Narayana project. It provides lightweight JDBC pooling layer that could be nice for small projects. Integration requires a little bit more hand working. &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IronJacamar&lt;/td&gt; &lt;td&gt;To be used when you need whole JCA functionality running in Apache Tomcat. The benefit of this solution is the battle tested integration of Narayana and IronJacamar as they are delivered as one pack in the WildFly application server. &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cov_oZNFoDg" height="1" width="1" alt=""/&gt;</content><summary>Narayana implements JTA specification in Java. It's flexible and easy to be integrated to any system which desires transaction capabilities. As proof of the Narayana extensibility check our quickstarts like Spring Boot one or Camel one. But this blogpost is different integration effort. It talks in details about Narayana integration with Apache Tomcat server. If you do not care about details then ...</summary><dc:creator>Ondřej Chaloupka</dc:creator><dc:date>2018-05-21T22:16:00Z</dc:date><feedburner:origLink>http://jbossts.blogspot.com/2018/05/narayana-jdbc-integration-for-tomcat.html</feedburner:origLink></entry><entry><title>Red Hat Summit: Lowering the risk of monolith to microservices</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fz4rA-JfAIc/" /><category term="enterprise architecture" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="modernization" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="Red Hat Summit 2018" scheme="searchisko:content:tags" /><author><name>unknown</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_summit_lowering_the_risk_of_monolith_to_microservices</id><updated>2018-05-21T17:57:33Z</updated><published>2018-05-21T17:57:33Z</published><content type="html">&lt;p&gt;Christian Posta, Chief Architect at Red Hat, presented the story of a fictitious company&lt;sup&gt;1&lt;/sup&gt; moving a monolithic application to microservices.&lt;/p&gt; &lt;p&gt;When considering risk, we think we know the bad things that can happen and the probabilities of those bad things actually happening. Christian defines a &lt;em&gt;monolith&lt;/em&gt; as a large application developed over many years by different teams that delivers proven business value while being very difficult to update and maintain. Its architecture, elegant at one point, has eroded over time. That makes it difficult to assess the risk of migrating a monolith.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s hear from the new school:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Microservice&lt;/em&gt; is a highly distracting word that serves to confuse developers, architects, and IT leaders into believing that we can actually have a utopian application architecture.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Um, no. While that&amp;#8217;s cynical but true, the correct definition is:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Microservices&lt;/em&gt; are an &lt;em&gt;architectural optimization&lt;/em&gt; that treats the modules of an application as independently owned and deployed services for the purpose of increasing an organization&amp;#8217;s velocity and eliminating &lt;a href="https://en.wikipedia.org/wiki/Technical_debt"&gt;technical debt&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;The performance of an IT organization has a strong correlation to business performance, boosting productivity, profitability, and market share&lt;sup&gt;2&lt;/sup&gt;. Statistics over the last few years for high performers versus low performers include moving code from commit to production &lt;em&gt;200x faster&lt;/em&gt;&lt;sup&gt;3&lt;/sup&gt;, deployments being &lt;em&gt;46x more frequent&lt;/em&gt;, and the lead time for changes being &lt;em&gt;440x faster&lt;/em&gt;&lt;sup&gt;4&lt;/sup&gt;. Technologies and techniques such as containers, automated testing, and deployment pipelines enable teams to go faster. In addition, high-performing teams are introducing errors at a much lower rate and are recovering from the errors they do create at a much higher rate.&lt;/p&gt; &lt;p&gt;When considering microservices, the goal is not to pursue a microservice architecture mindlessly. The goal is to use microservices where they make sense. Specifically, to use microservices to speed up development, to lower the risk of bad things happening, and to make it simpler to understand and recover from bad things when they happen. (Which, of course, they probably will.)&lt;/p&gt; &lt;p&gt;The case study Christian covered is the &lt;a href="https://developers.redhat.com/ticket-monster/"&gt;TicketMonster demo&lt;/a&gt;. Originally written for JBoss, the code has been around for at least a decade and has become, in Christian&amp;#8217;s words, &amp;#8220;a morass of stuff.&amp;#8221; Maintaining this or any other monolith is a major pain for a long list of reasons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Making changes in one place negatively affects unrelated areas&lt;/li&gt; &lt;li&gt;We have low confidence that reasonable changes won&amp;#8217;t break something somewhere else&lt;/li&gt; &lt;li&gt;We spend lots of time coordinating work among team members&lt;/li&gt; &lt;li&gt;The structure of the application has eroded or is non-existent&lt;/li&gt; &lt;li&gt;We have no way to quantify how long code merges will take&lt;/li&gt; &lt;li&gt;Development is tedious because the project is so big (the IDE bogs down, running tests take forever, bootstrap times are long, etc.)&lt;/li&gt; &lt;li&gt;Changes to one module force changes across other modules&lt;/li&gt; &lt;li&gt;Sunsetting outdated technology is difficult&lt;/li&gt; &lt;li&gt;We may have to base new applications on old approaches like batch processing&lt;/li&gt; &lt;li&gt;The monolith gets in its own way when managing resources, allocations, and computations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of these factors make microservices attractive. But it&amp;#8217;s important to remember that &lt;em&gt;microservices are about optimizing for speed&lt;/em&gt;. You need to ask yourself the question, &amp;#8220;Is the architecture of our application the bottleneck that keeps us from moving faster?&amp;#8221; If it&amp;#8217;s not, Christian prescribes a three-step process:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Figure out what &lt;em&gt;is&lt;/em&gt; keeping you from moving faster.&lt;/li&gt; &lt;li&gt;Fix that.&lt;/li&gt; &lt;li&gt;Come back and look at microservices again.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If microservices are a good fit for you, the question is how to break a monolith into microservices. Some common approaches include, &amp;#8220;Do one thing and do it well,&amp;#8221; &amp;#8220;Organize around verbs,&amp;#8221; &amp;#8220;Organize around nouns,&amp;#8221; and &amp;#8220;Focus on products not projects.&amp;#8221; We said similar things about SOA back in the day, however. &amp;#8220;Services are autonomous,&amp;#8221; &amp;#8220;Loose coupling is vital,&amp;#8221; &amp;#8220;Boundaries are explicit,&amp;#8221; were equally high-minded goals that often eluded us. So we have platitudes on what the system should be, but no real guidelines on how to make it happen. A more sophisticated approach:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Identify modules and boundaries (&lt;em&gt;aka&lt;/em&gt; use &lt;a href="https://en.wikipedia.org/wiki/Domain-driven_design"&gt;domain-driven design&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Align to business capabilities&lt;/li&gt; &lt;li&gt;Identify data entities responsible for features and modules&lt;/li&gt; &lt;li&gt;Break out those entities and wrap them with APIs or services&lt;/li&gt; &lt;li&gt;Update old code to call the new APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That&amp;#8217;s all well and good, but it misses a lot of detail&lt;sup&gt;5&lt;/sup&gt;. As usual, reality rears its ugly, pointy little head, presenting difficult problems that can&amp;#8217;t be waved away:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It&amp;#8217;s not easy to modularize a monolith. If it were, the teams that maintained the monolith over the years would have kept it modular.&lt;/li&gt; &lt;li&gt;There are often tight couplings and integrity constraints with an SQL database. This is often overlooked. The database backing the monolith probably has normalized tables and referential integrity in place. Disregarding that can be disastrous. (In fact, it almost certainly will be disastrous.)&lt;/li&gt; &lt;li&gt;It&amp;#8217;s difficult to understand which modules use which tables in the database.&lt;/li&gt; &lt;li&gt;As appealing as it might be, we can&amp;#8217;t shut down the enterprise to do migrations.&lt;/li&gt; &lt;li&gt;There will be some ugly migration steps that can&amp;#8217;t be wished away. You&amp;#8217;ll have to undo the existing technical debt accrued over the years.&lt;/li&gt; &lt;li&gt;Finally, there is probably a point of diminishing returns at which it simply doesn&amp;#8217;t make sense to break any more things out of the monolith&lt;sup&gt;6&lt;/sup&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Yet another list—you need to make sure that these things are in place:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test coverage for the existing project&lt;sup&gt;7&lt;/sup&gt; (consider &lt;a href="http://arquillian.org"&gt;Arquillian&lt;/a&gt; for integration testing. Also consider Michael Feathers&amp;#8217; book &lt;a href="https://www.amazon.com/Working-Effectively-Legacy-Michael-Feathers/dp/0131177052"&gt;Working Effectively with Legacy Code&lt;/a&gt;.)&lt;/li&gt; &lt;li&gt;Some level of monitoring to detect issues, exceptions, etc.&lt;/li&gt; &lt;li&gt;Some level of black-box system tests and load testing in place (&lt;a href="http://jmeter.apache.org"&gt;JMeter&lt;/a&gt; and &lt;a href="https://gatling.io"&gt;Gatling&lt;/a&gt; can help here)&lt;/li&gt; &lt;li&gt;The ability to deploy to an environment reliably (OpenShift or Kubernetes)&lt;/li&gt; &lt;li&gt;Some kind of CI/CD pipeline to make changes economical&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Moving on to the sample monolith&amp;#8230;the Ticket Monster code has the UI baked in to it. If possible, a good first step is to break the UI out of the monolith. That gives you the freedom to bring the UI into the modern era, and it also sets the stage for the separation of concerns that is promised by a microservices architecture. Start by making a copy of the UI code separate from the monolith by taking out all the JavaScript, CSS, etc. The new UI simply calls the back-end systems of the monolith. You can then do a dark launch or a canary deployment of the new UI, sending only some production traffic to it as you verify that the new UI is functionally equivalent to the original.&lt;/p&gt; &lt;p&gt;With that in mind, Christian stressed the difference between deployments and releases. A &lt;em&gt;deployment&lt;/em&gt; is simply code that lives on a production server but doesn&amp;#8217;t get any traffic. It isn&amp;#8217;t considered a &lt;em&gt;release&lt;/em&gt; until it&amp;#8217;s using live data in a production environment. A deployment should be a non-event to the business. Developers should be able to deploy code without approvals or intervention by administrators or the ops staff. Deciding to route production traffic to the deployment, on the other hand, is a business decision made once the deployment has been tested thoroughly.&lt;/p&gt; &lt;p&gt;The &lt;a href="http://istio.io"&gt;Istio service mesh&lt;/a&gt; makes this easy in an OpenShift / Kubernetes environment. It is an infrastructure that allows you to separate deployments and releases. (Read Don Schenck’s excellent &lt;a href="https://developers.redhat.com/blog/2018/03/06/introduction-istio-makes-mesh-things/"&gt;Introduction to Istio blog series&lt;/a&gt; to fully understand this technology.) Istio lets you say that only 1% of the traffic should go to the new deployment or that only users who have certain characteristics should see the new deployment. Another definition:&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;A &lt;em&gt;service mesh&lt;/em&gt; is a decentralized application networking infrastructure among your services that provides resiliency, security, observability, and routing control. A service mesh is comprised of a control plane and a data plane.&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;Istio works via &lt;em&gt;sidecar proxies&lt;/em&gt;. Every service in the service mesh has a proxy beside it. Any traffic meant for a service first goes to the sidecar proxy. Istio controls the sidecar proxy to determine how or when or if that traffic is actually delivered to the service. This approach allows you to manage all the microservices in your service mesh without changing any code..&lt;/p&gt; &lt;p&gt;Continuing the refactoring of the monolith, we can take a particular function of the monolith and move that code into a service. The starting point, once again, is to copy the code from the monolith and put it into a separate module. Then we can use Istio to route only some of the traffic to the new service. To maximize the value of our investment, the service we choose should be something that provides significant business value. In other words, we look at the monolith and determine that some part of the system could give us a substantial ROI if we made it more agile and flexible. In the Ticket Monster example, the order processing functions could be far more valuable to the business if they were in a separate module that could be enhanced independently of the monolith. For example, if that code was written a decade ago, it doesn&amp;#8217;t support things like ApplePay or Venmo. A low-risk architecture for adding new methods of payment would clearly help the business stay current.&lt;/p&gt; &lt;p&gt;To sum up:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Write lots of tests. For the monolith if you can, but definitely for the new services.&lt;/li&gt; &lt;li&gt;Use advanced techniques such as canary deployments and other fine-grained traffic control to manage the transition from deployments to releases.&lt;/li&gt; &lt;li&gt;Reduce boilerplate code for data integration in the initial service implementation.&lt;/li&gt; &lt;li&gt;Use technical debt to your advantage.&lt;/li&gt; &lt;li&gt;Have lots of monitoring in place.&lt;/li&gt; &lt;li&gt;Leverage your deployment and release infrastructure to experiment and learn about your system as you go forward.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was a great session with lots of practical advice based on real-world experience. If you&amp;#8217;d like to get it from the source, the &lt;a href="https://youtu.be/YP6wJXblyWM"&gt;video of Christian&amp;#8217;s presentation&lt;/a&gt; is one of the &lt;a href="https://developers.redhat.com/blog/2018/05/15/100-red-hat-summit-2018-session-videos-online/"&gt;100+ Red Hat Summit 2018 breakout sessions&lt;/a&gt; you can view online for free.&lt;/p&gt; &lt;div align="center"&gt;&lt;iframe src="https://www.youtube.com/embed/YP6wJXblyWM" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start"&gt;﻿&lt;/span&gt;&lt;/iframe&gt;&lt;/div&gt; &lt;p&gt;Good luck with your enterprise modernization!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.slideshare.net/ceposta/lowering-the-risk-of-monolith-to-microservices"&gt;Christian&amp;#8217;s slides for this presentation on SlideShare&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Christian and Burr Sutter&amp;#8217;s new book: &lt;a href="https://developers.redhat.com/books/introducing-istio-service-mesh-microservices/"&gt;Introducing Istio Service Mesh for Microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Christian&amp;#8217;s book: &lt;a href="https://developers.redhat.com/books/microservices-java-developers-hands-introduction-frameworks-and-containers/"&gt;Microservices for Java Developers: A Hands-on Introduction to Frameworks and Containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/books/microservices-java-developers-hands-introduction-frameworks-and-containers/"&gt;Christian&amp;#8217;s blog articles&lt;/a&gt; on &lt;a href="https://developers.redhat.com/blog/"&gt;developers.redhat.com/blog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Footnotes:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Fictitious, but based on a true story. Many of them, in fact.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; See &lt;a href="https://puppet.com/resources/whitepaper/2014-state-devops-report"&gt;https://puppet.com/resources/whitepaper/2014-state-devops-report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; See &lt;a href="https://puppet.com/resources/whitepaper/2015-state-devops-report"&gt;https://puppet.com/resources/whitepaper/2015-state-devops-report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;4&lt;/sup&gt; See &lt;a href="https://puppet.com/resources/whitepaper/2017-state-devops-report"&gt;https://puppet.com/resources/whitepaper/2017-state-of-devops-report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; What, you were expecting a full, prescriptive recommendation from a blog post? Sorry, but the real world is more complicated than that.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;6&lt;/sup&gt; You can stab it with your steely knives, but you just can&amp;#8217;t kill the beast.&lt;/p&gt; &lt;p&gt;&lt;sup&gt;7&lt;/sup&gt; One definition of legacy code is &amp;#8220;code that doesn&amp;#8217;t have any tests.&amp;#8221;’&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;linkname=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fred-hat-summit-lowering-the-risk-of-monolith-to-microservices%2F&amp;#38;title=Red%20Hat%20Summit%3A%20Lowering%20the%20risk%20of%20monolith%20to%20microservices" data-a2a-url="https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/" data-a2a-title="Red Hat Summit: Lowering the risk of monolith to microservices"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/"&gt;Red Hat Summit: Lowering the risk of monolith to microservices&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fz4rA-JfAIc" height="1" width="1" alt=""/&gt;</content><summary>Christian Posta, Chief Architect at Red Hat, presented the story of a fictitious company1 moving a monolithic application to microservices. When considering risk, we think we know the bad things that can happen and the probabilities of those bad things actually happening. Christian defines a monolith as a large application developed over many years by different teams that delivers proven business ...</summary><dc:creator>unknown</dc:creator><dc:date>2018-05-21T17:57:33Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/21/red-hat-summit-lowering-the-risk-of-monolith-to-microservices/</feedburner:origLink></entry><entry><title>Apache Camel URI completion: easy installation for Eclipse, VS Code, and OpenShift.io</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GTWE1fg4ESg/" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="Eclipse" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="FUSE" scheme="searchisko:content:tags" /><category term="Fuse Tooling" scheme="searchisko:content:tags" /><category term="ide" scheme="searchisko:content:tags" /><category term="JBoss Fuse" scheme="searchisko:content:tags" /><category term="language servers" scheme="searchisko:content:tags" /><category term="OpenShift.io" scheme="searchisko:content:tags" /><category term="Red Hat Fuse" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift.io" scheme="searchisko:content:tags" /><category term="VS Code" scheme="searchisko:content:tags" /><author><name>Aurélien Pupier</name></author><id>searchisko:content:id:jbossorg_blog-apache_camel_uri_completion_easy_installation_for_eclipse_vs_code_and_openshift_io</id><updated>2018-05-21T11:00:34Z</updated><published>2018-05-21T11:00:34Z</published><content type="html">&lt;p&gt;Discoverability and ease of installation of Apache Camel tooling based on the Language Server Protocol has been improved. Manual download and installation of binaries is no longer necessary!  For the Eclipse desktop IDE and the VS Code environment you can now find and install the Camel tooling directly from the marketplaces for each development environment.&lt;/p&gt; &lt;p&gt;Camel Language Server is now also available in &lt;a href="http://openshift.io/"&gt;Red Hat OpenShift.io!&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this article, I will show you how you can install Camel tooling via the marketplaces for Eclipse and VS Code.  I will also show how to enable Camel tooling in your OpenShift.io workspace.&lt;span id="more-494857"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Eclipse and VS Code Marketplaces&lt;/h2&gt; &lt;p&gt;The Camel tooling is available on the &lt;a href="https://marketplace.eclipse.org/content/apache-camel-language-server"&gt;Eclipse marketplace&lt;/a&gt;. This means that you can discover and install it directly from your &lt;a href="https://www.eclipse.org/ide/"&gt;Eclipse Desktop IDE&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-494867 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel.png" alt="Language Support for Apache Camel extension entry displayed when searching for Camel in Eclipse Marketplace" width="811" height="951" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel.png 811w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel-256x300.png 256w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/EclipseSearchCamel-768x901.png 768w" sizes="(max-width: 811px) 100vw, 811px" /&gt;&lt;/p&gt; &lt;p&gt;The tooling is also available on &lt;a href="https://marketplace.visualstudio.com/items?itemName=camel-tooling.vscode-apache-camel"&gt;VS Code marketplace&lt;/a&gt;. This means that you can discover and install it directly from your &lt;a href="https://code.visualstudio.com/"&gt;VS Code IDE&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-494877 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-1024x639.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel.png" alt="Language Support for Apache Camel extension entry displayed when searching for Camel in VS Code Extension manager" width="1086" height="678" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel.png 1086w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-300x187.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-768x479.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/VSCodeSearchCamel-1024x639.png 1024w" sizes="(max-width: 1086px) 100vw, 1086px" /&gt;&lt;/p&gt; &lt;h2&gt;Use in OpenShift.io&lt;/h2&gt; &lt;p&gt;Camel Language Server is now also available in &lt;a href="http://openshift.io/"&gt;OpenShift.io!&lt;/a&gt; It requires a workspace configuration to be turned on.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Go to your workspace&lt;br /&gt; &lt;img class=" aligncenter wp-image-494957 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/1-openShiftIOWorkspace-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on the yellow top left arrow&lt;img class=" aligncenter wp-image-494967 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/2-openShiftIOWorkspacePanel-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on &amp;#8220;Workspaces&amp;#8221;&lt;img class=" aligncenter wp-image-494977 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/3-openShiftIOWorkspaces-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on the &amp;#8220;configuration icon corresponding to your workspace&lt;img class=" aligncenter wp-image-494987 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/4-openShiftIOConfigurationWorkspace-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Click on &amp;#8220;Installers&amp;#8221; menu and turn on &amp;#8220;Apache Camel language server&lt;img class=" aligncenter wp-image-494997 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/5-turnOnCamelLanguageServer-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;li&gt;Apply and open the workspace&lt;/li&gt; &lt;li&gt;Reopen the xml file&lt;/li&gt; &lt;li&gt;Enjoy!&lt;br /&gt; &lt;img class=" aligncenter wp-image-495007 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-1024x555.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_.png" alt="" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-768x416.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/completionOpenShift.io_-1024x555.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;New home for project code&lt;/h2&gt; &lt;p&gt;Please note that the Camel Language Server has a new home. It is now co-hosted with the &lt;a href="https://github.com/camel-tooling/camel-idea-plugin"&gt;Camel Idea plugin&lt;/a&gt;. You can find it here: &lt;a href="https://github.com/camel-tooling/camel-language-server"&gt;github.com/camel-tooling/camel-language-server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Now that it is easily installable, it is time to give it a try!&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;linkname=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F05%2F21%2Fapache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io%2F&amp;#38;title=Apache%20Camel%20URI%20completion%3A%20easy%20installation%20for%20Eclipse%2C%20VS%20Code%2C%20and%20OpenShift.io" data-a2a-url="https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/" data-a2a-title="Apache Camel URI completion: easy installation for Eclipse, VS Code, and OpenShift.io"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/"&gt;Apache Camel URI completion: easy installation for Eclipse, VS Code, and OpenShift.io&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GTWE1fg4ESg" height="1" width="1" alt=""/&gt;</content><summary>Discoverability and ease of installation of Apache Camel tooling based on the Language Server Protocol has been improved. Manual download and installation of binaries is no longer necessary!  For the Eclipse desktop IDE and the VS Code environment you can now find and install the Camel tooling directly from the marketplaces for each development environment. Camel Language Server is now also availa...</summary><dc:creator>Aurélien Pupier</dc:creator><dc:date>2018-05-21T11:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/05/21/apache-camel-uri-completion-easy-installation-for-eclipse-vs-code-and-openshift-io/</feedburner:origLink></entry></feed>
