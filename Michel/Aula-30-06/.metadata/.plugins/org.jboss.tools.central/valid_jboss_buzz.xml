<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Why Kubernetes Is the New Application Server</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NYgl-DiBQfI/" /><category term="ci/cd" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="devops" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="istio" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="microprofile" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Application Runtimes" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><author><name>Rafael Benevides</name></author><id>searchisko:content:id:jbossorg_blog-why_kubernetes_is_the_new_application_server</id><updated>2018-06-28T11:00:34Z</updated><published>2018-06-28T11:00:34Z</published><content type="html">&lt;p&gt;Have you ever wondered why you are deploying your multi-platform applications using containers? Is it just a matter of “following the hype”? In this article, I&amp;#8217;m going to ask some provocative questions to make my case for &lt;em&gt;Why Kubernetes is the new application server&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;You might have noticed that the majority of languages are interpreted and use “runtimes” to execute your source code. In theory, most Node.js, Python, and Ruby code can be easily moved from one platform (Windows, Mac, Linux) to another platform. Java applications go even further by having the compiled Java class turned into a bytecode, capable of running anywhere that has a JVM (Java Virtual Machine).&lt;/p&gt; &lt;p&gt;The Java ecosystem provides a standard format to distribute all Java classes that are part of the same application. You can package these classes as a JAR (Java Archive), WAR (Web Archive), and EAR (Enterprise Archive) that contains the front end, back end, and libraries embedded. So I ask you: Why do you use containers to distribute your Java application? Isn’t it already supposed to be easily portable between environments?&lt;/p&gt; &lt;p&gt;&lt;span id="more-495187"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Answering this question from a developer perspective isn&amp;#8217;t always obvious. But think for a moment about your development environment and some possible issues caused by the difference between it and the production environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do you use Mac, Windows, or Linux? Have you ever faced an issue related to &lt;code&gt;\&lt;/code&gt; versus &lt;code&gt;/&lt;/code&gt; as the file path separator?&lt;/li&gt; &lt;li&gt;What version of JDK do you use? Do you use Java 10 in development, but production uses JRE 8? Have you faced any bugs introduced by  JVM differences?&lt;/li&gt; &lt;li&gt;What version of the application server do you use? Is the production environment using the same configuration, security patches, and library versions?&lt;/li&gt; &lt;li&gt;During production deployment, have you encountered a JDBC driver issue that you didn’t face in your development environment due to different versions of the driver or database server?&lt;/li&gt; &lt;li&gt;Have you ever asked the application server admin to create a datasource or a JMS queue and it had a typo?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All the issues above are caused by factors external to your application, and one of the greatest things about containers is that you can deploy everything (for example, a Linux distribution, the JVM, the application server, libraries, configurations and, finally, your application) inside a pre-built container. Plus, executing a single container that has everything built in is incredibly easier than moving your code to a production environment and trying to resolve the differences when it doesn&amp;#8217;t work. Since it’s easy to execute, it is also easy to scale the same container image to multiple replicas.&lt;/p&gt; &lt;h2&gt;Empowering Your Application&lt;/h2&gt; &lt;p&gt;Before containers became very popular, several &lt;a href="https://en.wikipedia.org/wiki/Non-functional_requirement#Examples"&gt;NFR (non-functional requirements)&lt;/a&gt; such as security, isolation, fault tolerance, configuration management, and others were provided by application servers. As an analogy, the application servers were planned to be to applications what CD (Compact Disc) players are to CDs.&lt;/p&gt; &lt;p&gt;As a developer, you would be responsible to follow a predefined standard and distribute the application in a specific format, while on the other hand the application server would “execute” your application and give additional capabilities that could vary from different “brands.”  Note: In the Java world, the standard for enterprise capabilities provided by an application server has recently moved under the Eclipse foundation. The work on Eclipse Enterprise for Java (&lt;a href="https://projects.eclipse.org/projects/ee4j"&gt;EE4J&lt;/a&gt;), has resulted in &lt;a href="https://jakarta.ee/"&gt;Jakarta EE&lt;/a&gt;.  (For more info, read the article &lt;a href="https://developers.redhat.com/blog/2018/04/24/jakarta-ee-is-officially-out/"&gt;&lt;em&gt;Jakarta EE is officially out&lt;/em&gt;&lt;/a&gt; or watch the &lt;a href="https://developers.redhat.com/videos/youtube/f2EwhTUmeOI/"&gt;DevNation video: &lt;em&gt;Jakarta EE: The future of Java EE&lt;/em&gt;&lt;/a&gt;.)&lt;/p&gt; &lt;p&gt;Following the same CD player analogy, with the ascension of containers, the &lt;a href="https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/"&gt;&lt;em&gt;container image&lt;/em&gt;&lt;/a&gt; has become the new CD format. In fact, a container image is nothing more than a format for distributing your containers. (If you need to get a better handle on what container images are and how they are distributed see &lt;em&gt;&lt;a href="https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/"&gt;A Practical Introduction to Container Terminology&lt;/a&gt;&lt;/em&gt;.)&lt;/p&gt; &lt;p&gt;The real benefits of containers happen when you need to add enterprise capabilities to your application. And the best way to provide these capabilities to a containerized application is by using Kubernetes as a platform for them. Additionally, the Kubernetes platform provides a great foundation for other projects such as &lt;a href="https://www.openshift.com/"&gt;Red Hat OpenShift&lt;/a&gt;, &lt;a href="https://istio.io/"&gt;Istio&lt;/a&gt;, and &lt;a href="https://openwhisk.apache.org/"&gt;Apache OpenWhisk&lt;/a&gt; to build on and make it easier to build and deploy robust production quality applications.&lt;/p&gt; &lt;p&gt;Let’s explore nine of these capabilities:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31.png"&gt;&lt;img src="https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31.png" alt="" width="1634" height="1254" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;1 – Service Discovery&lt;/h3&gt; &lt;p&gt;Service discovery is the process of figuring out how to connect to a service.  To get many of the benefits of containers and cloud-native applications, you need to remove configuration from your container images so you can use the same container image in all environments. Externalized configuration from applications is one of the key principles of the &lt;a href="https://developers.redhat.com/blog/2017/06/22/12-factors-to-cloud-success/"&gt;12-factor application&lt;/a&gt;. Service discovery is one of the ways to get configuration information from the runtime environment instead of it being hardcoded in the application. Kubernetes provides &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services"&gt;service discovery&lt;/a&gt; out of the box. Kubernetes also provides &lt;a href="https://kubernetes-v1-4.github.io/docs/user-guide/configmap/"&gt;ConfigMaps&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Secrets&lt;/a&gt; for removing configuration from your application containers.  Secrets solve some of the challenges that arise when you need to store the credentials for connecting to a service like a database in your runtime environment.&lt;/p&gt; &lt;p&gt;With Kubernetes, there’s no need to use an external server or framework.  While you can manage the environment settings for each runtime environment through Kubernetes YAML files, Red Hat OpenShift provides a GUI and CLI that can make it easier for DevOps teams to manage.&lt;/p&gt; &lt;h3&gt;2 – Basic Invocation&lt;/h3&gt; &lt;p&gt;Applications running inside containers can be accessed through &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt; access— in other words, routes from the outside world to the service you are exposing. OpenShift provides &lt;a href="https://docs.openshift.com/container-platform/3.9/architecture/networking/routes.html#overview"&gt;route objects&lt;/a&gt; using HAProxy, which has several capabilities and load-balancing strategies.  You can use the routing capabilities to do rolling deployments. This can be the basis of some very sophisticated CI/CD strategies. See &amp;#8220;6 – Build and Deployment Pipelines&amp;#8221; below.&lt;/p&gt; &lt;p&gt;What if you need to run a one-time job, such as a batch process, or simply leverage the cluster to compute a result (such as computing the digits of Pi)? Kubernetes provides &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/"&gt;job objects&lt;/a&gt; for this use case. There is also a &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/"&gt;cron job&lt;/a&gt; that manages time-based jobs.&lt;/p&gt; &lt;h3&gt;3 – Elasticity&lt;/h3&gt; &lt;p&gt;Elasticity is solved in Kubernetes by using &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"&gt;ReplicaSets&lt;/a&gt; (which used to be called Replication Controllers). Just like most configurations for Kubernetes, a ReplicaSet is a way to reconcile a desired state: you tell Kubernetes what state the system should be in and Kubernetes figures out how to make it so. A ReplicaSet controls the number of replicas or exact copies of the app that should be running at any time.&lt;/p&gt; &lt;p&gt;But what happens when you build a service that is even more popular than you planned for and you run out of compute? You can use the Kubernetes &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#what-is-the-horizontal-pod-autoscaler"&gt;Horizontal Pod Autoscaler&lt;/a&gt;, which scales the number of pods based on observed CPU utilization (or, with &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/custom-metrics-api.md"&gt;custom metrics&lt;/a&gt; support, on some other application-provided metrics).&lt;/p&gt; &lt;h3&gt;4 – Logging&lt;/h3&gt; &lt;p&gt;Since your Kubernetes cluster can and will run several replicas of your containerized application, it’s important that you aggregate these logs so they can be viewed in one place. Also, in order to utilize benefits like autoscaling (and other cloud-native capabilities), your containers need to be immutable. So you need to store your logs outside of your container so they will be persistent across runs. OpenShift allows you to deploy the EFK stack to aggregate logs from hosts and applications, whether they come from multiple containers or even from deleted pods.&lt;/p&gt; &lt;p&gt;The EFK stack is composed of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.elastic.co/products/elasticsearch"&gt;Elasticsearch&lt;/a&gt; (ES), an object store where all logs are stored&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.fluentd.org/architecture"&gt;Fluentd&lt;/a&gt;, which gathers logs from nodes and feeds them to Elasticsearch&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.elastic.co/guide/en/kibana/current/introduction.html"&gt;Kibana&lt;/a&gt;, a web UI for Elasticsearch&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;5 – Monitoring&lt;/h3&gt; &lt;p&gt;Although logging and monitoring seem to solve the same problem, they are different from each other. Monitoring is observation, checking, often alerting, as well as recording. Logging is recording only.&lt;/p&gt; &lt;p&gt;&lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; is an open-source monitoring system that includes time series database. It can be used for storing and querying metrics, alerting, and using visualizations to gain insights into your systems. Prometheus is perhaps the most popular choice for monitoring Kubernetes clusters. On the &lt;a href="https://developers.redhat.com/blog/"&gt;Red Hat Developers blog&lt;/a&gt;, there are several articles covering monitoring using &lt;a href="https://developers.redhat.com/blog/tag/prometheus/"&gt;Prometheus&lt;/a&gt;. You can also find Prometheus articles on the &lt;a href="https://blog.openshift.com/tag/prometheus/"&gt;OpenShift blog&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can also see Prometheus in action together with Istio at &lt;a href="https://learn.openshift.com/servicemesh/3-monitoring-tracing"&gt;https://learn.openshift.com/servicemesh/3-monitoring-tracing&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;6 – Build and Deployment Pipelines&lt;/h3&gt; &lt;p&gt;CI/CD (Continuous Integration/Continuous Delivery) pipelines are not a strict “must have” requirement for your applications. However, CI/CD are often cited as pillars of successful software development and &lt;a href="https://devops.com/optimizing-effective-cicd-pipeline/"&gt;DevOps&lt;/a&gt; practices.  No software should be deployed into production without a CI/CD pipeline. The book &lt;a href="https://www.amazon.com/dp/0321601912?tag=contindelive-20"&gt;&lt;em&gt;Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation&lt;/em&gt;&lt;/a&gt;, by Jez Humble and David Farley, says this about CD: “Continuous Delivery is the ability to get changes of all types—including new features, configuration changes, bug fixes and experiments—into production, or into the hands of users, safely and quickly in a sustainable way.”&lt;/p&gt; &lt;p&gt;OpenShift provides CI/CD pipelines out of the box as a &amp;#8220;&lt;a href="https://docs.openshift.com/container-platform/3.7/dev_guide/builds/build_strategies.html#pipeline-strategy-options"&gt;build strategy&lt;/a&gt;.&amp;#8221; Check out &lt;a href="https://www.youtube.com/watch?v=N8R3-eNVoEc"&gt;this video&lt;/a&gt; that I recorded two years ago, which has an example of a Jenkins CI/CD pipeline that deploys a new microservice.&lt;/p&gt; &lt;h3&gt;7 – Resilience&lt;/h3&gt; &lt;p&gt;While Kubernetes provides resilience options for the &lt;a href="https://docs.openshift.com/container-platform/3.9/admin_guide/high_availability.html"&gt;cluster itself&lt;/a&gt;, it can also help the application be resilient by providing &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;PersistentVolumes&lt;/a&gt; that support replicated volumes. Kubernetes&amp;#8217; &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/"&gt;ReplicationControllers&lt;/a&gt;/deployments ensure that the specified numbers of pod replicas are consistently deployed across the cluster, which automatically handles any possible &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#what-is-a-node"&gt;node&lt;/a&gt; failure.&lt;/p&gt; &lt;p&gt;Together with resilience, fault tolerance serves as an effective means to address users&amp;#8217; reliability and availability concerns. Fault tolerance can also be provided to an application that is running on Kubernetes through &lt;a href="https://istio.io/"&gt;Istio&lt;/a&gt; by its retries rules, circuit breaker, and pool ejection. Do you want to see it for yourself? Try the Istio Circuit Breaker tutorial at &lt;a href="https://learn.openshift.com/servicemesh/7-circuit-breaker"&gt;https://learn.openshift.com/servicemesh/7-circuit-breaker&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;8 – Authentication&lt;/h3&gt; &lt;p&gt;Authentication in Kubernetes can also be provided by Istio through its &lt;a href="https://istio.io/docs/concepts/security/mutual-tls.html"&gt;mutual TLS authentication&lt;/a&gt;, which aims to enhance the security of microservices and their communication without requiring service code changes. It is responsible for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Providing each service with a strong identity that represents its role to enable interoperability across clusters and clouds&lt;/li&gt; &lt;li&gt;Securing service-to-service communication and end user-to-service communication&lt;/li&gt; &lt;li&gt;Providing a key management system to automate key and certificate generation, distribution, rotation, and revocation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Additionally, it is worth mentioning that you can also run &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; inside a Kubernetes/OpenShift cluster to provide both authentication and authorization. Keycloak is the upstream product for Red Hat Single Sign-on. For more information, read &lt;a href="https://developers.redhat.com/blog/tag/keycloak/"&gt;Single-Sign On Made Easy with Keycloak&lt;/a&gt;. If you are using Spring Boot, watch the DevNation video: &lt;a href="https://developers.redhat.com/videos/youtube/Bdg_DjuoX0A/"&gt;Secure Spring Boot Microservices with Keycloak&lt;/a&gt; or &lt;a href="https://developers.redhat.com/blog/tag/keycloak/"&gt;read the blog article&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;9 – Tracing&lt;/h3&gt; &lt;p&gt;Istio-enabled applications can be configured to collect trace spans using &lt;a href="https://zipkin.io/"&gt;Zipkin&lt;/a&gt; or &lt;a href="https://www.jaegertracing.io/docs/"&gt;Jaeger&lt;/a&gt;. Regardless of what language, framework, or platform you use to build your application, Istio can enable distributed tracing. Check it out at &lt;a href="https://learn.openshift.com/servicemesh/3-monitoring-tracing"&gt;https://learn.openshift.com/servicemesh/3-monitoring-tracing&lt;/a&gt;.  See also &lt;a href="https://developers.redhat.com/blog/2018/05/08/getting-started-with-istio-and-jaeger-on-your-laptop/"&gt;Getting Started with Istio and Jaeger on your laptop&lt;/a&gt; and the recent DevNation video: &lt;a href="https://developers.redhat.com/blog/2018/06/20/next-devnation-live-advanced-microservices-tracing-with-jaeger-june-21st-12pm-edt/"&gt;Advanced microservices tracing with Jaeger&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Are Application Servers Dead?&lt;/h2&gt; &lt;p&gt;Going through these capabilities, you can realize how Kubernetes + OpenShift + Istio can really empower your application and provide features that used to be the responsibility of an application server or a software framework such as &lt;a href="https://netflix.github.io/"&gt;Netflix OSS&lt;/a&gt;. Does that mean application servers are dead?&lt;/p&gt; &lt;p&gt;In this new containerized world, application servers are mutating into becoming more like frameworks. It&amp;#8217;s natural that the evolution of software development caused the evolution of application servers. A great example of this evolution is the &lt;a href="http://microprofile.io/"&gt;Eclipse MicroProfile&lt;/a&gt; specification having &lt;a href="http://wildfly-swarm.io"&gt;WildFly Swarm&lt;/a&gt; as the application server, which provides to the developer features such as fault tolerance, configuration, tracing, REST (client and server), and so on. However, WildFly Swarm and the MicroProfile specification are designed to be very lightweight. WildFly Swarm doesn&amp;#8217;t have the vast array of components required by a full Java enterprise application server. Instead, it focuses on microservices and having just enough of the application server to build and run your application as a simple executable .jar file.  You can read more about &lt;a href="https://developers.redhat.com/blog/tag/microprofile/"&gt;MicroProfile&lt;/a&gt; on this blog.&lt;/p&gt; &lt;p&gt;Furthermore, Java applications can have features such as the Servlet engine, a datasource pool, dependency injection, transactions, messaging, and so forth. Of course, frameworks can provide these features, but an application server must also have everything you need to build, run, deploy, and manage enterprise applications in any environment, regardless of whether they are inside containers. In fact, application servers can be executed anywhere, for instance, on bare metal, on virtualization platforms such as &lt;a href="https://www.redhat.com/en/technologies/virtualization/enterprise-virtualization"&gt;Red Hat Virtualization&lt;/a&gt;, on private cloud environments such as &lt;a href="https://www.openstack.org/"&gt;Red Hat OpenStack Platform&lt;/a&gt;, and also on public cloud environments such as &lt;a href="https://azure.microsoft.com/en-us/"&gt;Microsoft Azure&lt;/a&gt; or &lt;a href="https://aws.amazon.com/"&gt;Amazon Web Services&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A good application server ensures consistency between the APIs that are provided and their implementations. Developers can be sure that deploying their business logic, which requires certain capabilities, will work because the application server developers (and the defined standards) have ensured that these components work together and have evolved together. Furthermore, a good application server is also responsible for maximizing throughput and scalability, because it will handle all the requests from the users; having reduced latency and improved load times, because it will help your application&amp;#8217;s &lt;a href="https://12factor.net/disposability"&gt;disposability&lt;/a&gt;; be lightweight with a small footprint that minimizes hardware resources and costs; and finally, be secure enough to avoid any security breach. For Java developers, Red Hat provides &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/application-platform"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt;, which fulfills all the requirements of a modern, modular application server.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Container images have become the standard packaging format to distribute cloud-native applications. While containers “per se” don’t provide real business advantages to applications, Kubernetes and its related projects, such as OpenShift and Istio, provide the non-functional requirements that used to be part of an application server.&lt;/p&gt; &lt;p&gt;Most of these non-functional requirements that developers used to get from an application server or from a library such as &lt;a href="https://netflix.github.io/"&gt;Netflix OSS&lt;/a&gt; were bound to a specific language, for example, Java. On the other hand, when developers choose to meet these requirements using Kubernetes + OpenShift + Istio, they are not attached to any specific language, which can encourage the use of the best technology/language for each use case.&lt;/p&gt; &lt;p&gt;Finally, application servers still have their place in software development. However, they are mutating into becoming more like language-specific frameworks that are a great shortcut when developing applications, since they contain lots of already written and tested functionality.&lt;/p&gt; &lt;p&gt;One of the best things about moving to containers, Kubernetes, and microservices is that you don&amp;#8217;t have to choose a single application server, framework, architectural style or even language for your application. You can easily deploy a container with JBoss EAP running your existing Java EE application, alongside other containers that have new microservices using Wildfly Swarm, or Eclipse Vert.x for reactive programming. These containers can all be managed through Kubernetes. To see this concept in action, take a look at &lt;a href="https://developers.redhat.com/products/rhoar/overview/"&gt;Red Hat OpenShift Application Runtimes&lt;/a&gt;. Use the &lt;a href="https://developers.redhat.com/launch/"&gt;Launch service&lt;/a&gt; to build and deploy a sample app online using WildFly Swarm, Vert.x, Spring Boot, or Node.js. Select the Externalized Configuration mission to learn how to use Kubernetes ConfigMaps. This will get you started on your path to cloud-native applications.&lt;/p&gt; &lt;p&gt;You can say that &lt;a href="https://www.linkedin.com/pulse/openshift-new-enterprise-linux-daniel-riek/"&gt;Kubernetes/OpenShift is the new Linux&lt;/a&gt; or even that &amp;#8220;Kubernetes is the new application server.&amp;#8221; But the fact is that an application server/runtime + OpenShift/Kubernetes + Istio has become the &amp;#8220;de facto&amp;#8221; cloud-native application platform!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;If you haven&amp;#8217;t been to the Red Hat Developer site lately, you should check out the pages on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes and container management&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/microservices/"&gt;Microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/service-mesh/"&gt;Service mesh and Istio&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a name="rbenevides"&gt;&lt;/a&gt;&lt;/p&gt; &lt;div class="author"&gt; &lt;p&gt;&lt;img class="author-photo alignleft" title="Rafael" src="http://rafabene.com/images/rafaelbenevides.jpg" alt="Rafael Benevides" width="119" height="119" /&gt;&lt;/p&gt; &lt;h3&gt;About the author:&lt;/h3&gt; &lt;p&gt;Rafael Benevides is Director of Developer Experience at &lt;a href="http://www.redhat.com"&gt;Red Hat&lt;/a&gt;. With many years of experience in several fields of the IT industry, he helps developers and companies all over the world to be more effective in software development. Rafael considers himself a problem solver who has a big love for sharing. He is a member of Apache DeltaSpike PMC—a Duke’s Choice Award winner project—and a speaker in conferences such as JavaOne, Devoxx, TDC, DevNexus, and many others.| &lt;a href="https://www.linkedin.com/in/rafaelbenevides" target="_blank" rel="noopener"&gt;LinkedIn&lt;/a&gt; | &lt;a href="http://rafabene.com/" target="_blank" rel="noopener"&gt;rafabene.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;linkname=Why%20Kubernetes%20is%20The%20New%20Application%20Server" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F28%2Fwhy-kubernetes-is-the-new-application-server%2F&amp;#38;title=Why%20Kubernetes%20is%20The%20New%20Application%20Server" data-a2a-url="https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/" data-a2a-title="Why Kubernetes is The New Application Server"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/"&gt;Why Kubernetes is The New Application Server&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NYgl-DiBQfI" height="1" width="1" alt=""/&gt;</content><summary>Have you ever wondered why you are deploying your multi-platform applications using containers? Is it just a matter of “following the hype”? In this article, I’m going to ask some provocative questions to make my case for Why Kubernetes is the new application server. You might have noticed that the majority of languages are interpreted and use “runtimes” to execute your source code. In theory, mos...</summary><dc:creator>Rafael Benevides</dc:creator><dc:date>2018-06-28T11:00:34Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/</feedburner:origLink></entry><entry><title>Narayana Commit Markable Resource: a faultless LRCO for JDBC datasources</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qq44hyr6kbA/narayana-commit-markable-resource.html" /><category term="feed_group_name_jbosstransactions" scheme="searchisko:content:tags" /><category term="feed_name_transactions" scheme="searchisko:content:tags" /><author><name>Ondřej Chaloupka</name></author><id>searchisko:content:id:jbossorg_blog-narayana_commit_markable_resource_a_faultless_lrco_for_jdbc_datasources</id><updated>2018-06-28T06:11:15Z</updated><published>2018-06-28T05:17:00Z</published><content type="html">&lt;p&gt; CMR is neat Narayana feature enabling full XA transaction capability for one non-XA JDBC resource. This gives you a way to engage a database resource to XA transaction even the JDBC driver is not fully XA capable (or you just have a design restriction on it) while transaction data consistency is kept. &lt;/p&gt; &lt;h2&gt;Last resource commit optimization (aka. LRCO)&lt;/h2&gt; &lt;p&gt; Maybe you will say "&lt;i&gt;adding one non-XA resource to a transaction is well-known LRCO optimization&lt;/i&gt;". And you are right. But just partially. The last resource commit optimization (abbreviated as LRCO) provides a way to enlist and process one non-XA datasource to the global transaction managed by the transaction manager. But LRCO contains a pitfall. When the crash of the system (or the connection) happens in particular point of the time, &lt;a href="https://developer.jboss.org/wiki/TwoPhaseCommit2PC"&gt;during two-phase commit processing&lt;/a&gt;, it causes data inconsistency. Namely, the LRCO could be committed while the rest of the resources will be rolled-back. &lt;/p&gt;&lt;p&gt; Let's elaborate a bit on the LRCO failure. Let's say we have a JMS resource where we send a message to a message broker and non-XA JDBC datasource where we save information to the database. &lt;p&gt; &lt;p&gt; &lt;i&gt;NOTE: The example refers to the Narayana two-phase commit implemenation.&lt;/i&gt; &lt;br/&gt;&lt;br/&gt; &lt;ol&gt; &lt;li&gt;updating the database with &lt;code&gt;INSERT INTO&lt;/code&gt; SQL command, enlisting LRCO resource under the transaction&lt;/li&gt; &lt;li&gt;sending a message to the JMS broker, enlisting the JMS resource to the transaction&lt;/li&gt; &lt;li&gt;Narayana starts the two phase commit processing&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; is called to JMS XA resource, the transaction log is stored at the JMS broker side&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; phase for the LRCO means to call &lt;code&gt;commit&lt;/code&gt; at the non-XA datasource. That call makes the data changes visible to the outer world.&lt;/li&gt; &lt;li&gt;crash of the Narayana JVM occurs before the Narayana can preserve information of commit to its transaction log store&lt;/li&gt; &lt;li&gt;after the Narayana restarts there is no notion about the existence of any transaction thus the prepared JMS resource is rolled-back during transaction recovery&lt;/li&gt; &lt;/ol&gt; &lt;p&gt; &lt;i&gt;Note:&lt;/i&gt; roll-backing of the JMS resource is caused by &lt;a href="http://narayana.io//docs/product/index.html#two-phase-variants"&gt;presumed abort strategy&lt;/a&gt; applied in the Narayana. If transaction manager does do not apply the presumed abort then you end ideally not better than in the &lt;a href="http://jbossts.blogspot.com/2011/03/heuristics-and-why-you-need-to-know.html"&gt;transaction heuristic state&lt;/a&gt;. &lt;/p&gt;&lt;/p&gt; &lt;p&gt; The LRCO processing is about ordering the LRCO resource as the last during the &lt;a href="https://developer.jboss.org/wiki/TwoPhaseCommit2PC"&gt;transaction manager 2PC&lt;/a&gt; &lt;code&gt;prepare&lt;/code&gt; phase. At place where transaction normally calls &lt;code&gt;prepare&lt;/code&gt; at &lt;code&gt;XAResource&lt;/code&gt;s there is called &lt;code&gt;commit&lt;/code&gt; at the LRCO's underlaying non-XA resource. &lt;br/&gt; Then during the transaction manager &lt;code&gt;commit&lt;/code&gt; phase there is called nothing for the LRCO. &lt;/p&gt; &lt;h2&gt;Commit markable resource (aka. CMR)&lt;/h2&gt;&lt;p&gt; The Commit Markable Resource, abbreviated as &lt;code&gt;CMR&lt;/code&gt;, is an enhancement of the last resource commit optimization applicable on the JDBC resources. The CMR approach achieves capabilities similar to XA by demanding special database table (normally named &lt;code&gt;xids&lt;/code&gt;) that is accessible for transaction manager to write and to read via the configured CMR datasource. &lt;/p&gt; &lt;p&gt; Let's demonstrate the CMR behavior at the example (reusing setup from the previous one). &lt;ol&gt; &lt;li&gt;updating the database with &lt;code&gt;INSERT INTO&lt;/code&gt; SQL command, enlisting the CMR resource under the transaction&lt;/li&gt; &lt;li&gt;sending a message to the JMS broker, enlisting the JMS resource to the transaction&lt;/li&gt; &lt;li&gt;Narayana starts the two phase commit processing&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; on CMR saves information about prepare to the &lt;code&gt;xids&lt;/code&gt; table&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; is called to JMS XA resource, the transaction log is stored at the JMS broker side&lt;/li&gt; &lt;li&gt;&lt;code&gt;commit&lt;/code&gt; on CMR means calling commit on underlaying non-XA datasource&lt;/li&gt; &lt;li&gt;&lt;code&gt;commit&lt;/code&gt; on JMS XA resource means commit on the XA JMS resource and thus the message being visible at the queue, the proper transaction log is removed at the JMS broker side&lt;/li&gt; &lt;li&gt;Narayana two phase commit processing ends&lt;/li&gt; &lt;/ol&gt;&lt;/p&gt; &lt;p&gt; From what you can see here the difference from the LRCO example is that the CMR resource is not ordered as last in the resource processing but it's ordered as the first one. The CMR prepare does not mean committing the work as in case of the LRCO but it means saving information about that CMR is considered to be prepared into the database &lt;code&gt;xids&lt;/code&gt; table. &lt;br/&gt; As the CMR is ordered as the first resource for processing it's taken as first during the commit phase too. The commit call then means to call &lt;code&gt;commit&lt;/code&gt; at the underlying database connection. The &lt;code&gt;xids&lt;/code&gt; table is not cleaned at that phase and it's normally responsibility of &lt;code&gt;CommitMarkableResourceRecordRecoveryModule&lt;/code&gt; to process the garbage collection of records in the &lt;code&gt;xids&lt;/code&gt; table (see more below). &lt;/p&gt; &lt;p&gt; The main fact to understand is that CMR resource is considered as &lt;i&gt;fully prepared&lt;/i&gt; only after the &lt;code&gt;commit&lt;/code&gt; is processed (meaning commit on the underlaying non-XA JDBC datasource). Till that time the transaction is considered as &lt;b&gt;not&lt;/b&gt; prepared and will be processed with rollback by the transaction recovery. &lt;/p&gt; &lt;p&gt; &lt;i&gt;NOTE:&lt;/i&gt; the term &lt;i&gt;fully prepared&lt;/i&gt; considers the standard XA two-phase commit processing. If the transaction manager finishes with the &lt;code&gt;prepare&lt;/code&gt; phase, aka. prepare is called on all transaction participants, the transaction is counted as prepared and &lt;code&gt;commit&lt;/code&gt; is expected to be called on each participant. &lt;/p&gt; &lt;p&gt; It's important to note that the correct processing of failures in transactions which contain CMR resources is responsibility of the special &lt;a href="https://jbossts.blogspot.com/2018/01/narayana-periodic-recovery-of-xa.html"&gt;periodic recovery module&lt;/a&gt; &lt;code&gt;CommitMarkableResourceRecordRecoveryModule&lt;/code&gt;. It has to be configured as &lt;b&gt;the first&lt;/b&gt; in the recovery module list as it needs to check and eventually process all the XA resources belonging to the transaction which contains the CMR resource (the recovery modules are processed in the order they were configured). You can check &lt;a href="https://github.com/wildfly/wildfly/blob/13.0.0.Final/transactions/src/main/java/org/jboss/as/txn/service/ArjunaRecoveryManagerService.java#L104"&gt;here how this is set up in WildFly&lt;/a&gt;. &lt;br/&gt; The CMR recovery module knows about the existence of the CMR resource from the record saved in the &lt;code&gt;xids&lt;/code&gt; table. From that it's capable to pair all the resources belonging to the same transaction where CMR was involved. &lt;/p&gt; &lt;h4&gt;xids: database table to save CMR processing data&lt;/h4&gt; &lt;p&gt; As said Narayana needs a special database table (usually named &lt;code&gt;xids&lt;/code&gt;) to save information that CMR was prepared. You may wonder what is content of that table. &lt;br/&gt; The table consists of three columns. &lt;ul&gt; &lt;li&gt;&lt;i&gt;xid&lt;/i&gt; : id of the transaction branch belonging to the CMR resource&lt;/li&gt; &lt;li&gt;&lt;i&gt;transactionManagerID&lt;/i&gt; : id of transaction manager, this serves to distinguish more transaction managers (WildFly servers) working with the same database. There is a strict rule that each transaction manager must be defined with unique transaction id (&lt;a href="https://wildscribe.github.io/WildFly/13.0/subsystem/transactions/index.html"&gt;see description of the node-identifer&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;i&gt;actionuid&lt;/i&gt; : global transaction id which unites all the resources belonging to the one particular transaction&lt;/li&gt; &lt;/ul&gt;&lt;/p&gt; &lt;h4&gt;LRCO failure case with CMR&lt;/h4&gt; &lt;p&gt; In the example, we presented as problematic for LRCO, the container crashed just before prepare phase finished. In such case, the CMR is not committed yet. The other transaction participants are then rolled-back as the transaction was not &lt;i&gt;fully prepared&lt;/i&gt;. The CMR brings the consistent rollback outcome for all the resources. &lt;/p&gt; &lt;h2&gt;Commit markable resource configured in WildFly&lt;/h2&gt; &lt;p&gt; We have sketched the principle of the CMR and now it's time to check how to configure it for your application running at the &lt;a href="http://wildfly.org"&gt;WildFly&lt;/a&gt; application server. &lt;br/&gt; The configuration consists of three steps. &lt;ol&gt; &lt;li&gt;The JDBC datasource needs to be marked as &lt;i&gt;connectable&lt;/i&gt;&lt;/li&gt; &lt;li&gt;The database, the connectable datasource points to, has to be enriched with the &lt;code&gt;xids&lt;/code&gt; table where Narayana can saves the data about CMR processing&lt;/li&gt; &lt;li&gt;Transaction subsystem needs to be configured to be aware of the CMR capable resource&lt;/li&gt; &lt;/ol&gt;&lt;/p&gt; &lt;p&gt; In our example, I use the H2 database as it's good for the showcase. You can find it in quickstart I prepared too. Check out the &lt;a href="https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource"&gt; https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource&lt;/a&gt;. &lt;/p&gt; &lt;h3&gt;Mark JDBC datasource as connectable&lt;/h3&gt; &lt;p&gt; You will mark the resource as &lt;code&gt;connectable&lt;/code&gt; when you use attribute &lt;code&gt;connectable="true"&lt;/code&gt; in your datasource declaration in &lt;code&gt;standalone*.xml&lt;/code&gt; configuration file. When you use jboss cli for the app server configuration you will use commands &lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;/subsystem=datasources/data-source=jdbc-cmr:write-attribute(name=connectable, value=true)&lt;br /&gt;:reload&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; The whole datasource configuration then looks like &lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&amp;lt;datasource jndi-name="java:jboss/datasources/jdbc-cmr" pool-name="jdbc-cmr-datasource"&lt;br /&gt; enabled="true" use-java-context="true" connectable="true"&amp;gt;&lt;br /&gt; &amp;lt;connection-url&amp;gt;jdbc:h2:mem:cmrdatasource&amp;lt;/connection-url&amp;gt;&lt;br /&gt; &amp;lt;driver&amp;gt;h2&amp;lt;/driver&amp;gt;&lt;br /&gt; &amp;lt;security&amp;gt;&lt;br /&gt; &amp;lt;user-name&amp;gt;sa&amp;lt;/user-name&amp;gt;&lt;br /&gt; &amp;lt;password&amp;gt;sa&amp;lt;/password&amp;gt;&lt;br /&gt; &amp;lt;/security&amp;gt;&lt;br /&gt;&amp;lt;/datasource&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; When datasource is marked as connectable then the IronJacamar (JCA layer of WildFly) creates the datasource instance as implementing &lt;a href="https://github.com/ochaloup/jboss-transaction-spi/blob/master/src/main/java/org/jboss/tm/ConnectableResource.java"&gt;&lt;code&gt;org.jboss.tm.ConnectableResource&lt;/code&gt;&lt;/a&gt; (defined in the &lt;a href="https://github.com/ochaloup/jboss-transaction-spi"&gt;jboss-transaction-spi project&lt;/a&gt;). This resource defines that the class provides method &lt;code&gt;getConnection() throws Throwable&lt;/code&gt;. That's how the transaction manager is capable to obtain the connection to the database and works with the &lt;code&gt;xids&lt;/code&gt; table inside it. &lt;/p&gt; &lt;h3&gt;Xids database table creation&lt;/h3&gt; &lt;p&gt; The database configured to be &lt;code&gt;connectable&lt;/code&gt; has to ensure existence of the &lt;code&gt;xids&lt;/code&gt; before transaction manager starts. As described above the &lt;code&gt;xids&lt;/code&gt; allows to save the cruical information about the non-XA datasource during prepare. The shape of the SQL command depends on the SQL syntax of the database you use. The example of the &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/resources/arjunacore/CommitMarkableResourceRecord.java#L74"&gt; table cleation commands is (see more commands under this link)&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="sql"&gt;-- Oracle&lt;br /&gt;CREATE TABLE xids (&lt;br /&gt; xid RAW(144), transactionManagerID VARCHAR(64), actionuid RAW(28)&lt;br /&gt;);&lt;br /&gt;CREATE UNIQUE INDEX index_xid ON xids (xid);&lt;br /&gt;&lt;br /&gt;-- PostgreSQL&lt;br /&gt;CREATE TABLE xids (&lt;br /&gt; xid bytea, transactionManagerID varchar(64), actionuid bytea&lt;br /&gt;);&lt;br /&gt;CREATE UNIQUE INDEX index_xid ON xids (xid);&lt;br /&gt;&lt;br /&gt;-- H2&lt;br /&gt;CREATE TABLE xids (&lt;br /&gt; xid VARBINARY(144), transactionManagerID VARCHAR(64), actionuid VARBINARY(28)&lt;br /&gt;);&lt;br /&gt;CREATE UNIQUE INDEX index_xid ON xids (xid);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; I addressed the need of the table definition in &lt;a href="https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource"&gt;the CMR quickstart&lt;/a&gt; by adding the JPA schema generation create script which contains &lt;a href="https://github.com/jbosstm/quickstart/blob/master/wildfly/commit-markable-resource/src/main/resources/META-INF/persistence.xml#L44"&gt;the SQL to initialize the database&lt;/a&gt;. &lt;/p&gt; &lt;h3&gt;Transaction manager CMR configuration&lt;/h3&gt; &lt;p&gt; The last part is to configure the CMR for the transaction subsystem. The declaration puts the datasource under the list &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/jta/common/JTAEnvironmentBean.java#L104"&gt;JTAEnvironmentBean#commitMarkableResourceJNDINames&lt;/a&gt; which is then used in code of &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/transaction/arjunacore/TransactionImple.java#L798"&gt;TransactionImple#createResource&lt;/a&gt;. &lt;br/&gt; The xml element used in the transaction subsystem and the jboss cli commands look like &lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&amp;lt;commit-markable-resources&amp;gt;&lt;br /&gt; &amp;lt;commit-markable-resource jndi-name="java:jboss/datasources/jdbc-cmr"/&amp;gt;&lt;br /&gt;&amp;lt;/commit-markable-resources&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="bash"&gt;/subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr":add()&lt;br /&gt;:reload&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;CMR configuration options&lt;/h4&gt; &lt;p&gt; In addition to such simple CMR declaration, the CMR can be configured with following parameters &lt;/p&gt; &lt;p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;jndi-name&lt;/b&gt; : as it could be seen above the jndi-name is way to point to the datasource which we mark as CMR ready&lt;/li&gt; &lt;li&gt;&lt;b&gt;name&lt;/b&gt; : defines the name of the table which is used for storing the CMR state during prepare while used during recovery. &lt;br/&gt;The default value (and we've reffered to it in this way above) is &lt;code&gt;xids&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;b&gt;immediate-cleanup&lt;/b&gt; : If configured to true then there is registered &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/resources/arjunacore/CommitMarkableResourceRecord.java#L191"&gt;a synchronization&lt;/a&gt; which removes proper value from the &lt;code&gt;xids&lt;/code&gt; table immediatelly after the transaction is committed. &lt;br/&gt; When synchronization is not set up then the clean-up of the &lt;code&gt;xids&lt;/code&gt; table is responsibility of the recovery by the code at &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/recovery/arjunacore/CommitMarkableResourceRecordRecoveryModule.java#L76"&gt;&lt;code&gt;CommitMarkableResourceRecordRecoveryModule&lt;/code&gt;&lt;/a&gt;. It checks about finished xids and it removes those which are free for garbage collection. &lt;br/&gt;The default value is &lt;code&gt;false&lt;/code&gt; (using only recovery garbage collection).&lt;/li&gt; &lt;li&gt;&lt;b&gt;batch-size&lt;/b&gt; : This parameter influences the process of the garbage collection (as described above). The garbage collection takes finished xids and runs &lt;code&gt;DELETE&lt;/code&gt; SQL command. The &lt;code&gt;DELETE&lt;/code&gt; contains the &lt;code&gt;WHERE xid in (...)&lt;/code&gt; clause with maximum of &lt;code&gt;batch-size&lt;/code&gt; entries provided. When there is still some finished xids left after deletion, another SQL command is assembled with maximum number of &lt;code&gt;batch-size&lt;/code&gt; entries again. &lt;br/&gt;The default value is &lt;code&gt;100&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; The &lt;code&gt;commit-markable-resource&lt;/code&gt; xml element configured with all the parameters looks like &lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&amp;lt;subsystem xmlns="urn:jboss:domain:transactions:4.0"&amp;gt;&lt;br /&gt; &amp;lt;core-environment&amp;gt;&lt;br /&gt; &amp;lt;process-id&amp;gt;&lt;br /&gt; &amp;lt;uuid/&amp;gt;&lt;br /&gt; &amp;lt;/process-id&amp;gt;&lt;br /&gt; &amp;lt;/core-environment&amp;gt;&lt;br /&gt; &amp;lt;recovery-environment socket-binding="txn-recovery-environment" status-socket-binding="txn-status-manager"/&amp;gt;&lt;br /&gt; &amp;lt;object-store path="tx-object-store" relative-to="jboss.server.data.dir"/&amp;gt;&lt;br /&gt; &amp;lt;commit-markable-resources&amp;gt;&lt;br /&gt; &amp;lt;commit-markable-resource jndi-name="java:jboss/datasources/jdbc-cmr"&amp;gt;&lt;br /&gt; &amp;lt;xid-location name="myxidstable" batch-size="10" immediate-cleanup="true"/&amp;gt;&lt;br /&gt; &amp;lt;/commit-markable-resource&amp;gt;&lt;br /&gt; &amp;lt;/commit-markable-resources&amp;gt;&lt;br /&gt;&amp;lt;/subsystem&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the jboss cli commands for the same are&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;/subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr"\&lt;br /&gt; :write-attribute(name=name, value=myxidstable)&lt;br /&gt; /subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr"\&lt;br /&gt; :write-attribute(name=immediate-cleanup, value=true)&lt;br /&gt;/subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr"\&lt;br /&gt; :write-attribute(name=batch-size, value=10)&lt;br /&gt;:reload&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;i&gt;NOTE:&lt;/i&gt; the JBoss EAP documentation about the CMR resource configuration can be found at section &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.1/html/development_guide/java_transaction_api_jta#about_the_lrco_optimization_for_single_phase_commit_1pc"&gt; About the LRCO Optimization for Single-phase Commit (1PC)&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt; The article explained what is the Narayana Commit Markable resource (CMR), it compared it with LRCO and presented its advantages. In the latter part of the article you found how to configure the CMR resource in your application deployed at the &lt;a href="http://wildfly.org/"&gt;WildFly application server&lt;/a&gt;. &lt;br/&gt; If you like to run an application using the commit markable resource feature, check our Narayana quickstart at &lt;a href="https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource"&gt; &lt;b&gt;https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource&lt;/b&gt;&lt;/a&gt;. &lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qq44hyr6kbA" height="1" width="1" alt=""/&gt;</content><summary>CMR is neat Narayana feature enabling full XA transaction capability for one non-XA JDBC resource. This gives you a way to engage a database resource to XA transaction even the JDBC driver is not fully XA capable (or you just have a design restriction on it) while transaction data consistency is kept. Last resource commit optimization (aka. LRCO) Maybe you will say "adding one non-XA resource to a...</summary><dc:creator>Ondřej Chaloupka</dc:creator><dc:date>2018-06-28T05:17:00Z</dc:date><feedburner:origLink>http://jbossts.blogspot.com/2018/06/narayana-commit-markable-resource.html</feedburner:origLink></entry><entry><title>Building Syndesis platform with Apache Camel snapshot</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_kktvrw9pE8/" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_oscerd" scheme="searchisko:content:tags" /><author><name>Andrea Cosentino</name></author><id>searchisko:content:id:jbossorg_blog-building_syndesis_platform_with_apache_camel_snapshot</id><updated>2018-06-28T00:00:00Z</updated><published>2018-06-28T00:00:00Z</published><content type="html">&lt;p&gt;In the last months I worked on &lt;a href="https://syndesis.io/"&gt;Syndesis&lt;/a&gt; project. Syndesis is an hybrid integration platform based on Apache Camel. During this time I had the need to build this platform against a Camel Snapshot version to test some new features I added into the Apache Camel project and it wasn’t truly easy. Adding the possibility to build the platform against different Camel snapshots and versions can be very useful to test Camel master and also to have an idea of how new/updated Camel components behave in this platform. I think it would be useful for end users too.&lt;/p&gt; &lt;h3 id="normal-workflow"&gt;Normal Workflow&lt;/h3&gt; &lt;p&gt;Building Syndesis platform is not super easy at first sight, but it’s very well documented and complete.&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis build &lt;span class="nt"&gt;--help&lt;/span&gt; Run Syndesis builds Usage: syndesis build &lt;span class="o"&gt;[&lt;/span&gt;... options ...] Options &lt;span class="k"&gt;for &lt;/span&gt;build: &lt;span class="nt"&gt;-b&lt;/span&gt; &lt;span class="nt"&gt;--backend&lt;/span&gt; Build only backend modules &lt;span class="o"&gt;(&lt;/span&gt;core, extension, integration, connectors, server, meta&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;--images&lt;/span&gt; Build only modules with Docker images &lt;span class="o"&gt;(&lt;/span&gt;ui, server, meta, s2i&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;-m&lt;/span&gt; &lt;span class="nt"&gt;--module&lt;/span&gt; &amp;lt;m1&amp;gt;,&amp;lt;m2&amp;gt;, .. Build modules Modules: ui, server, connector, s2i, meta, integration, extension, common &lt;span class="nt"&gt;-d&lt;/span&gt; &lt;span class="nt"&gt;--dependencies&lt;/span&gt; Build also all project the specified module depends on &lt;span class="nt"&gt;--skip-tests&lt;/span&gt; Skip unit and system &lt;span class="nb"&gt;test &lt;/span&gt;execution &lt;span class="nt"&gt;--skip-checks&lt;/span&gt; Disable all checks &lt;span class="nt"&gt;-f&lt;/span&gt; &lt;span class="nt"&gt;--flash&lt;/span&gt; Skip checks and tests execution &lt;span class="o"&gt;(&lt;/span&gt;fastest mode&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;-i&lt;/span&gt; &lt;span class="nt"&gt;--image-mode&lt;/span&gt; &amp;lt;mode&amp;gt; &amp;lt;mode&amp;gt; can be - &lt;span class="s2"&gt;"none"&lt;/span&gt; : No images are build &lt;span class="o"&gt;(&lt;/span&gt;default&lt;span class="o"&gt;)&lt;/span&gt; - &lt;span class="s2"&gt;"openshift"&lt;/span&gt; : Build &lt;span class="k"&gt;for &lt;/span&gt;OpenShift image streams - &lt;span class="s2"&gt;"docker"&lt;/span&gt; : Build against a plain Docker daemon - &lt;span class="s2"&gt;"auto"&lt;/span&gt; : Automatically detect whether to use &lt;span class="s2"&gt;"openshift"&lt;/span&gt; or &lt;span class="s2"&gt;"docker"&lt;/span&gt; &lt;span class="nt"&gt;--docker&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nt"&gt;--image-mode&lt;/span&gt; docker &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nt"&gt;--image-mode&lt;/span&gt; openshift &lt;span class="nt"&gt;-p&lt;/span&gt; &lt;span class="nt"&gt;--project&lt;/span&gt; &amp;lt;project&amp;gt; Specifies the project to create images &lt;span class="k"&gt;in &lt;/span&gt;when using &lt;span class="s1"&gt;'--openshift'&lt;/span&gt; &lt;span class="nt"&gt;-k&lt;/span&gt; &lt;span class="nt"&gt;--kill-pods&lt;/span&gt; Kill pods after the image has been created. Useful when building with image-mode docker &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="nt"&gt;--clean&lt;/span&gt; Run clean builds &lt;span class="o"&gt;(&lt;/span&gt;mvn clean&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;--batch-mode&lt;/span&gt; Run mvn &lt;span class="k"&gt;in &lt;/span&gt;batch mode &lt;span class="nt"&gt;--camel-snapshot&lt;/span&gt; Run a build with a specific Camel snapshot. You&lt;span class="s1"&gt;'ll need to set an environment variable CAMEL_SNAPSHOT_VERSION with the SNAPSHOT version you want to use. --man Open HTML documentation in the Syndesis Developer Handbook &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;I use &lt;a href="https://github.com/minishift/minishift"&gt;Minishift&lt;/a&gt; to play with Syndesis and my normal workflow is the following: First I spin up a Minishift instance&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; minishift start &lt;span class="nt"&gt;--memory&lt;/span&gt; 8384 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;You could need to specify a vm-driver too with the –vm-driver flag.&lt;/p&gt; &lt;p&gt;Then I set the docker environment coming from Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;eval&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;minishift docker-env&lt;span class="k"&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;At this point I’m able to build&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis build &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;span class="nt"&gt;--clean&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Once the build it’s done (it may take a while) we are able to deploy Syndesis platform on Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis minishift &lt;span class="nt"&gt;--install&lt;/span&gt; &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Once the deployment is finished we are able to start using Syndesis platform.&lt;/p&gt; &lt;h3 id="building-with-a-camel-snapshot"&gt;Building with a Camel Snapshot&lt;/h3&gt; &lt;p&gt;The option you’ll need in this case will be –camel-snapshot, in combination with an environment variable called &lt;code class="highlighter-rouge"&gt;CAMEL_SNAPSHOT_VERSION&lt;/code&gt;. In my case I need to test a new feature in a component from Camel 2.21.2-SNAPSHOT. The workflow to obtain a running Syndesis instance based on Camel 2.21.2-SNAPSHOT is the following (supposing you have a running Minishift).&lt;/p&gt; &lt;p&gt;Set the docker environment coming from Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;eval&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;minishift docker-env&lt;span class="k"&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Export &lt;code class="highlighter-rouge"&gt;CAMEL_SNAPSHOT_VERSION&lt;/code&gt; environment variable&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;CAMEL_SNAPSHOT_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"2.21.2-SNAPSHOT"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Run a Syndesis build with –camel-snapshot flag enabled&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis build &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;span class="nt"&gt;--clean&lt;/span&gt; &lt;span class="nt"&gt;--camel-snapshot&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Once the build finished, you can deploy your Syndesis on Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis minishift &lt;span class="nt"&gt;--install&lt;/span&gt; &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This is all you need to test a Syndesis platform based on Camel snapshot.&lt;/p&gt; &lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt; &lt;p&gt;In the future I’ll blog about the Syndesis platform a bit more. If you want to contribute you can start from the &lt;a href="https://github.com/syndesisio/syndesis/"&gt;Github project&lt;/a&gt;, the &lt;a href="https://syndesis.io/"&gt;site&lt;/a&gt; or an &lt;a href="https://github.com/syndesisio/syndesis-extensions"&gt;extension&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_kktvrw9pE8" height="1" width="1" alt=""/&gt;</content><summary>In the last months I worked on Syndesis project. Syndesis is an hybrid integration platform based on Apache Camel. During this time I had the need to build this platform against a Camel Snapshot version to test some new features I added into the Apache Camel project and it wasn’t truly easy. Adding the possibility to build the platform against different Camel snapshots and versions can be very use...</summary><dc:creator>Andrea Cosentino</dc:creator><dc:date>2018-06-28T00:00:00Z</dc:date><feedburner:origLink>http://oscerd.github.io/2018/06/28/building-syndesis-with-camel-snapshot/</feedburner:origLink></entry><entry><title>DesOps is “DevOps 2.0”</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JU5Mi4El6_w/" /><category term="agile" scheme="searchisko:content:tags" /><category term="design" scheme="searchisko:content:tags" /><category term="DesignOps" scheme="searchisko:content:tags" /><category term="DesOPs" scheme="searchisko:content:tags" /><category term="devops" scheme="searchisko:content:tags" /><category term="domain-driven design" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="SDLC" scheme="searchisko:content:tags" /><category term="UI/UX" scheme="searchisko:content:tags" /><author><name>Samir Dash</name></author><id>searchisko:content:id:jbossorg_blog-desops_is_devops_2_0</id><updated>2018-06-27T21:32:36Z</updated><published>2018-06-27T21:32:36Z</published><content type="html">&lt;p&gt;As we discussed in the &lt;a href="https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/"&gt;last post&lt;/a&gt;, most of &lt;em&gt;DevOps&lt;/em&gt; today focuses on the process blocks that mostly impact engineering or technical aspects of a product rather than the design aspect. Even though &lt;em&gt; DesOps&lt;/em&gt; was primarily born out of the primary need of how to design at scale, the factors that shaped it are of a similar nature to the factors that shaped &lt;em&gt;DevOps&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;With recent software delivery processes, for example, the Agile process and Continuous Integration (CI) and Continuous Deployment (CD)of code, the &lt;em&gt;DevOps&lt;/em&gt; approach provided a faster highway to ensure faster delivery with low risks. So the earlier SDLC model got redefined over time with Agile and then with &lt;em&gt;DevOps&lt;/em&gt; to its current shape.&lt;/p&gt; &lt;p&gt;However, because design is an integral part of any product delivered, there is a need to ensure that gaps are bridged between the traditional design lifecycle and the fast track of the &lt;em&gt;DevOps&lt;/em&gt; development lifecycle. &lt;em&gt;DesOps&lt;/em&gt; and &lt;em&gt;DevOps&lt;/em&gt; both are complementary to each other. The design delivery process improvements try to optimize the overall delivery process and thereby contribute to &lt;em&gt;DevOps&lt;/em&gt;, for example, in aspects such as testing of the product that involves design aspects, usability, accessibility, etc.&lt;/p&gt; &lt;p&gt;The need for tighter integration between the design team and the engineering team became a necessity to ensure to design at scale. During the past two to three years, the top five big companies have made heavy investments in this area that have paved the way for other organizations and design communities to be more explorative in this area.&lt;/p&gt; &lt;p&gt;&lt;span id="more-503557"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;img class="wp-image-503577 size-full aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0.png" alt="DesOps = DevOps 2.0" width="1492" height="708" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0.png 1492w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0-300x142.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0-768x364.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0-1024x486.png 1024w" sizes="(max-width: 1492px) 100vw, 1492px" /&gt;&lt;/p&gt; &lt;p&gt;The implications of &lt;em&gt;DesOps&lt;/em&gt; are reflected in the outcome, where the silos among the teams and disciplines get reduced. Along with this, &lt;em&gt;DesOps&lt;/em&gt; improves the collaboration among cross-functional teams and work practices, which contributes to minimizing waste in the delivery process.&lt;/p&gt; &lt;p&gt;Every product lifecycle has one core goal towards which it strives: reaching customer delight by delivering the value. The design process associated with &lt;em&gt;DesOps&lt;/em&gt; helps in understanding, capturing, and delivering that value.&lt;/p&gt; &lt;p&gt;But conventional business processes were more keen on getting the outputs of each process block, which can be fed into the next block, thereby reaching a stage that ultimately delivered the value. Many such practices failed in achieving customer delight because the processes used were not based on the customer shift from outputs to outcomes.&lt;/p&gt; &lt;p&gt;If we see the &lt;em&gt;DesOps&lt;/em&gt; processes in terms of a value system, we will see at a high level that &lt;em&gt;DesOps&lt;/em&gt; touches upon three major areas:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Understanding value&lt;/li&gt; &lt;li&gt;Creating value&lt;/li&gt; &lt;li&gt;Capturing and delivering value&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As discussed in the last post, in the value system approach,&lt;em&gt; understanding value&lt;/em&gt; is about reaching a vision. &lt;em&gt;Creating value&lt;/em&gt; is broadly about reaching a roadmap with &lt;em&gt;Minimum Viable Product&lt;/em&gt; (MVP). &lt;em&gt;Capturing and delivery value &lt;/em&gt;is about running the backlog and sprints and ensuring delivery until the end users have access.&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;img class="alignnone size-full wp-image-503607" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap.png" alt="" width="1423" height="704" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap.png 1423w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap-300x148.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap-768x380.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap-1024x507.png 1024w" sizes="(max-width: 1423px) 100vw, 1423px" /&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;Because design is associated with entities and attributes beyond the quantifiable science of connecting with disciplines that are associated with the emotional aspects of the qualitative approach, the implementation of &lt;em&gt;DesOps&lt;/em&gt; is more fluid than the case of &lt;em&gt;DevOps&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;For instance, if we plot the techniques and practices of any domain according to the left brain–right brain analogy, we can see that most of the practices having soft-attributes or dealing with human emotions, purpose, and behavior will fall into the creative aspect, which involves some kind of design approach to problem-solving.&lt;/p&gt; &lt;p&gt;As one example, you can see in the following figure that the right side shows the mapping of practices involved with software incident reporting. Here you can easily notice the pattern.&lt;/p&gt; &lt;p&gt;&lt;img class="size-full wp-image-503797 aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright.png" alt="" width="1224" height="803" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright.png 1224w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright-300x197.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright-768x504.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright-1024x672.png 1024w" sizes="(max-width: 1224px) 100vw, 1224px" /&gt;&lt;/p&gt; &lt;p class="selectionShareable"&gt;The typical belief is that the left side of the brain mostly processes logical thinking, while the right side is more about emotional thinking. Based on this popular analogy, when we map across a straight line from left to right (refer to the previous diagram) the different aspects involved in different stages of SDLC for a digital product, we will notice the logical and more human-centered aspects are divided by an imaginary line from the center. We will also notice the gradual progression of the emotional index for the components from left to right. This helps to demonstrate how the more-human angle is involved as we move from the areas that DevOps touches upon to the areas that &lt;em&gt;DesOps&lt;/em&gt; touches, because &lt;em&gt;DesOps&lt;/em&gt; touches upon the design and business aspects of the value chain.&lt;/p&gt; &lt;p&gt;From foundational guidelines, &lt;em&gt;DevOps&lt;/em&gt; inspires the &lt;em&gt;DesOps&lt;/em&gt; mindset at a high level:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Partnering with customers for improving business value&lt;/li&gt; &lt;li&gt;Working together towards a shared vision&lt;/li&gt; &lt;li&gt;Delivering incremental value&lt;/li&gt; &lt;li&gt;Investing in quality&lt;/li&gt; &lt;li&gt; Empowering team members&lt;/li&gt; &lt;li&gt;Setting up clear accountability in teams&lt;/li&gt; &lt;li&gt;Learning from experiences&lt;/li&gt; &lt;li&gt; Advocating open communications and transparency&lt;/li&gt; &lt;li&gt;Being agile and adapting to change&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In such a context, if we look into &lt;em&gt;DesOps&lt;/em&gt;, it makes a lot of sense in the following key principles:&lt;/p&gt; &lt;p&gt;&lt;img class="size-full wp-image-503807 aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222.png" alt="" width="1920" height="1329" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222-300x208.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222-768x532.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222-1024x709.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Implement &lt;em&gt;DesOps&lt;/em&gt; to follow service design methodologies.&lt;/li&gt; &lt;li&gt;The feedback loop should cut across the product lifecycle.&lt;/li&gt; &lt;li&gt;Empower stakeholders for better decision-making—hypothesis and data-driven decision-making for design and development.&lt;/li&gt; &lt;li&gt;Empower design thinking.&lt;/li&gt; &lt;li&gt;Advocate lean methodologies and Agile philosophies.&lt;/li&gt; &lt;li&gt;Translate user-centered design into an actual process that can be used on the ground.&lt;/li&gt; &lt;li&gt;Advocate cohesive designers, stakeholders, and developers into team play.&lt;/li&gt; &lt;li&gt;Technology decisions should be guided by lowering the boundaries between roles and automation to reduce waste and reduce repetitive jobs to work for the product and the project.&lt;/li&gt; &lt;li&gt;Redesign and re-engineer the processes.&lt;/li&gt; &lt;li&gt;Enable reviews based on data-driven benchmarks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So how does this all fit in? At a crude level, the &lt;em&gt;DesOps&lt;/em&gt; processes may have something like the following structure when they are translated in terms of technology and ecosystems, in a similar fashion as DevOps.&lt;/p&gt; &lt;p&gt;&lt;img class="aligncenter size-full wp-image-503827" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2.png" alt="" width="1500" height="323" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2.png 1500w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2-300x65.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2-768x165.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2-1024x221.png 1024w" sizes="(max-width: 1500px) 100vw, 1500px" /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, we should create the design benchmarks (that includes qualitative as well as quantitative metrics) from the information at the design stage that can be used in comparing the product features against metrics based on this design benchmark.&lt;/li&gt; &lt;li&gt;Then, automate and perform manual tracking of the product during runtime (in real time and in the true context), and then categorize and collate this data.&lt;/li&gt; &lt;li&gt;This involves creating features to support the user feedback cycle and user testing aspects (exploratory, split testing capabilities).&lt;/li&gt; &lt;li&gt;Collect all standards and specifications on different aspects of heuristics to ensure that at least at the basic level the standard principles are followed.&lt;/li&gt; &lt;li&gt;On the ground, in the context of the eco-system and technologies, build the critical components that would collect and process all the data collected in all these stages and generate the desired metrics and inferences and also contribute in bringing continuous integration and continuous delivery blocks to run the process.&lt;/li&gt; &lt;li&gt;Build the unit to generate the model to map the data and compare it against the metrics.&lt;/li&gt; &lt;li&gt;Build the cognitive unit that can compare the data and apply the correct models and metrics to carry out the filtering of the data and generate the insights which can be shared as actionable output to the end-user/customer.&lt;/li&gt; &lt;li&gt;And ensure in all these stages that the feedback loop is connected spatially and acting as a meaningful neural network that helps in informed decision-making.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keep tuned in to stay with me on this journey into &lt;em&gt;DesOps&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Note: Based on my book &lt;em&gt;The DesOps Enterprise: Overview &amp;#38; Culture&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;title=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" data-a2a-url="https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/" data-a2a-title="DesOps is “DevOps 2.0”"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/"&gt;DesOps is &amp;#8220;DevOps 2.0&amp;#8221;&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JU5Mi4El6_w" height="1" width="1" alt=""/&gt;</content><summary>As we discussed in the last post, most of DevOps today focuses on the process blocks that mostly impact engineering or technical aspects of a product rather than the design aspect. Even though  DesOps was primarily born out of the primary need of how to design at scale, the factors that shaped it are of a similar nature to the factors that shaped DevOps. With recent software delivery processes, fo...</summary><dc:creator>Samir Dash</dc:creator><dc:date>2018-06-27T21:32:36Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/</feedburner:origLink></entry><entry><title>Keycloak on Kubernetes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-EIEUoVLx2s/keycloak-on-kubernetes.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Stian Thorgersen</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_on_kubernetes</id><updated>2018-06-27T20:15:37Z</updated><published>2018-06-27T20:15:00Z</published><content type="html">&lt;p&gt;If you'd like to get started with using Keycloak on Kubernetes check out &lt;a href="https://youtu.be/A_BYZ7hHWXE"&gt;this screencast&lt;/a&gt;. If you'd rather try it out yourself check out &lt;a href="https://github.com/stianst/demo-kubernetes"&gt;this GitHub repository&lt;/a&gt; that contains the instructions as well as all the bits you'll need to reproduce what is shown in the screencast.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-EIEUoVLx2s" height="1" width="1" alt=""/&gt;</content><summary>If you'd like to get started with using Keycloak on Kubernetes check out this screencast. If you'd rather try it out yourself check out this GitHub repository that contains the instructions as well as all the bits you'll need to reproduce what is shown in the screencast.</summary><dc:creator>Stian Thorgersen</dc:creator><dc:date>2018-06-27T20:15:00Z</dc:date><feedburner:origLink>http://blog.keycloak.org/2018/06/keycloak-on-kubernetes.html</feedburner:origLink></entry><entry><title>Making Java objects queryable by Infinispan remote clients</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/i3zXH1Mdwcw/making-java-objects-queryable-by.html" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="remote query" scheme="searchisko:content:tags" /><author><name>Galder Zamarreño</name></author><id>searchisko:content:id:jbossorg_blog-making_java_objects_queryable_by_infinispan_remote_clients</id><updated>2018-06-27T11:47:22Z</updated><published>2018-06-27T11:46:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;The following is a common question amongst Infinispan community users:&lt;br /&gt;&lt;blockquote class="tr_bq"&gt;&lt;b&gt;How do I make my Java objects queryable by remote clients?&lt;/b&gt;&amp;nbsp;&lt;/blockquote&gt;&lt;div style="text-align: left;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style="text-align: left;"&gt;Annotation Method&lt;/h2&gt;&lt;br /&gt;The simplest way is to take advantage &lt;a href="https://github.com/infinispan/protostream"&gt;Infinispan Protostream&lt;/a&gt; annotations to mark your objects queryable and decide how each object field should be indexed. Example:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/6ba1e043b6ab7f8c5e0971500d30ff12.js"&gt;&lt;/script&gt; Then, the &lt;a href="https://github.com/infinispan/protostream/blob/master/core/src/main/java/org/infinispan/protostream/annotations/ProtoSchemaBuilder.java"&gt;ProtoSchemaBuilder&lt;/a&gt; can inspect the annotated class and derive a &lt;a href="https://developers.google.com/protocol-buffers/"&gt;Google Protocol Buffers&lt;/a&gt; schema file from it. Example:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/66b2b60305bd1f5d4e7762b4c21b366a.js"&gt;&lt;/script&gt; Finally, the schema file needs to be registered in the “&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;___protobuf_metadata&lt;/span&gt;” cache:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/da67bac08b3e5e6aa02da26f76821a50.js"&gt;&lt;/script&gt; Although this is by far the easiest way to make your Java objects queryable, this method might not always be viable. For example, you might not be able to modify the Java object classes to add the annotations. For such use cases, a more verbose method is available that does not require modifying the source code of the Java object.&lt;br /&gt;&lt;br /&gt;&lt;h2 style="text-align: left;"&gt;Plain Object Method&lt;/h2&gt;&lt;br /&gt;For example, given this Java object:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/b217b71a47ebd16a1fcb8f1ddb744c7a.js"&gt;&lt;/script&gt; A Protocol Buffers schema must be defined where comments are used to define the object as queryable and decide how each field is indexed:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/8921fd5325305e7f6a5d980523b6f5be.js"&gt;&lt;/script&gt; This method also requires a &lt;a href="https://github.com/infinispan/protostream/blob/master/core/src/main/java/org/infinispan/protostream/MessageMarshaller.java"&gt;Protostream message marshaller&lt;/a&gt; to be defined which specifies how each field is serialized/deserialized:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/4170abb6dd522ac3250989d8e842ab90.js"&gt;&lt;/script&gt; This method still requires the Protocol Buffers schema to be registered remotely, but on top of that, the schema and marshaller need to be registered in the client:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/debff7f6d84a942183dc4b879dbf28c1.js"&gt;&lt;/script&gt; Clearly, this second method is a lot more verbose and more laborious when refactoring. If any changes are made to the Java object, the marshaller and Protocol Buffer schema need to also be changed accordingly. This is done automatically in the first method.&lt;br /&gt;&lt;br /&gt;Both methods are demonstrated in full in the &lt;a href="https://github.com/infinispan-demos/queryable-pojos"&gt;queryable-pojos demo&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Cheers&lt;br /&gt;Galder&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/Infinispan/~4/zMYxbORR2AE" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/i3zXH1Mdwcw" height="1" width="1" alt=""/&gt;</content><summary>The following is a common question amongst Infinispan community users: How do I make my Java objects queryable by remote clients?  Annotation Method The simplest way is to take advantage Infinispan Protostream annotations to mark your objects queryable and decide how each object field should be indexed. Example: Then, the ProtoSchemaBuilder can inspect the annotated class and derive a Google Proto...</summary><dc:creator>Galder Zamarreño</dc:creator><dc:date>2018-06-27T11:46:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/Infinispan/~3/zMYxbORR2AE/making-java-objects-queryable-by.html</feedburner:origLink></entry><entry><title>Using Red Hat Data Grid to power a multi-cloud real-time game</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xt37-YNpkAI/" /><category term="Data Grid" scheme="searchisko:content:tags" /><category term="Demo" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="infinispan" scheme="searchisko:content:tags" /><category term="JBoss Data Grid" scheme="searchisko:content:tags" /><category term="OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="Red Hat Data Grid" scheme="searchisko:content:tags" /><category term="Red Hat JBoss Data Grid" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="red hat summit" scheme="searchisko:content:tags" /><category term="Red Hat Summit 2018" scheme="searchisko:content:tags" /><author><name>Galder Zamarreno</name></author><id>searchisko:content:id:jbossorg_blog-using_red_hat_data_grid_to_power_a_multi_cloud_real_time_game</id><updated>2018-06-26T11:00:30Z</updated><published>2018-06-26T11:00:30Z</published><content type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/blog/2018/05/10/red-hat-summit-2018-burr-sutter-demo/"&gt;scavenger hunt game&lt;/a&gt; developed for the audience to play during the Red Hat Summit 2018 demo used Red Hat Data Grid as storage for everything except the pictures taken by the participants. Data was stored &lt;a href="https://developers.redhat.com/blog/2018/06/19/red-hat-data-grid-on-three-clouds/"&gt;across three different cloud environments using cross-site replication&lt;/a&gt;. In this blog post, we will look at how data was flowing through Data Grid and explain the Data Grid features powering different aspects of the game&amp;#8217;s functionality.&lt;/p&gt; &lt;p&gt;In its simplest form, Data Grid exposes a key/value map API. This API was used throughout the game for storing and retrieving information such as game tasks, per-site active users, players, picture/task attempts (also called transactions), and scores.&lt;/p&gt; &lt;p&gt;Some of the data, such as player information including each player’s score was indexed for speed of retrieval. This was done in order to easily calculate the leaderboard, that is, the top 10 players with the highest score. So, from the player’s data, we only needed to index its score. We were able to do that very easily using Infinispan Protostream annotations (full source code can be found &lt;a href="https://github.com/rhdemo/scavenger/blob/master/microservices/scavenger-microservice/src/main/java/me/escoffier/keynote/Player.java"&gt;here&lt;/a&gt;), example:&lt;/p&gt; &lt;pre&gt;@ProtoDoc("@Indexed") @ProtoMessage(name = "Player") public class Player { private int score; … @ProtoDoc("@IndexedField") @ProtoField(number = 10, required = true) public int getScore() { return score; } ... } &lt;/pre&gt; &lt;p&gt;Calculating the leaderboard was easy. The Data Grid client simply had to build the query for it, send it to the Data Grid and handle results. The query looked something like this:&lt;/p&gt; &lt;pre&gt;Query query = queryFactory.from(Player.class) .orderBy("score", SortOrder.DESC) .maxResults(10) .build(); &lt;/pre&gt; &lt;p&gt;It’s worth noting that both the player data and the index itself were replicated across sites. This meant that no matter which cloud you’d hit, you’d get the same, consistent result.&lt;/p&gt; &lt;p&gt;Even though it didn’t make the final cut, we had also planned for users to be able to figure out their individual position in the leaderboard. Coming up with such a query was a bit more tricky, but we could achieve that by using this query:&lt;/p&gt; &lt;pre&gt;int playerScore = ... Query query = qf.from(Player.class) .select(count("score")) .orderBy("score", SortOrder.DESC) .having("score").gt(playerScore) .groupBy("score") .build(); List list = query.list(); final long playerRank = list.size() + 1; &lt;/pre&gt; &lt;p&gt;The query groups the number of different scores, sort them in descending order and count how many are bigger than a given player’s score. The number returned, plus 1, would give us the players position. This meant that if several players had the same score, they’d all share the same position but this is quite common, e.g. golf tournament rankings.&lt;/p&gt; &lt;p&gt;Keeping indexed data across different clouds was not always easy. One issue we had to deal with is the lack of default cross-site replication for the indexed data schema. To be able to index data, Data Grid must be able to decompose binary data received from the client to discover individual fields, figure out which ones to index, etc. To solve this problem, we created a schema keeper component (code can be found &lt;a href="https://github.com/rhdemo/scavenger-schemaer"&gt;here&lt;/a&gt;) which checked if the player’s schema was present in a given cloud, and if it wasn’t, register it. This component was deployed as a sidecar with each of the Data Grid instances. The Data Grid team is working to avoid the need for such component in the future.&lt;/p&gt; &lt;p&gt;Whenever a picture was uploaded and it was scored, the score would be stored individually inside Data Grid. This would trigger the Data Grid to send an event using remote client listeners, which would be picked by the game and would forward it to the user. Bearing in mind that Data Grid stores data in a key/value pair structure, the score would be stored in the value part. However, by default remote client listener events only ship key (and version) information. To avoid an extra lookup, the remote client listener was configured with a converter factory named &lt;code&gt;key-value-with-previous-converter-factory&lt;/code&gt; which is available out-of-the-box. By doing this, each event was transformed to contain the value part as well as the key. Example:&lt;/p&gt; &lt;pre&gt;@ClientListener(converterFactoryName = "key-value-with-previous-converter-factory", ...) public class RemoteCacheListener { ... }&lt;/pre&gt; &lt;p&gt;For each new score, we wanted a single event to be fired. However, due to how remote client listeners work, each score would, by default, fire as many events as different cloud/sites available. To avoid this, we used remote client listener filters to create a server-side deployed filter which would compare the score’s cloud of origin (AWS, Azure or Private) with the cloud where the filter was being executed.&lt;/p&gt; &lt;p&gt;For example, if a score originated in AWS, only the filter running inside the AWS cloud would allow the event to be fired to the client. When this score arrived in Azure or Private clouds, the filter would detect the event originated in a different cloud and would not fire it. The code for the filter can be found &lt;a href="https://github.com/rhdemo/infinispan-listener-optimizations/blob/master/src/main/java/fn/dg/os/filters/SiteFilterFactory.java"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To add it to the server, the filter needs to be deployed into it. For the demo, we extended the base Data Grid image with JAR containing the filter and related files.&lt;/p&gt; &lt;p&gt;Finally, to apply the filter to the listener, we added the name of the filter factory to the &lt;code&gt;filterFactoryName&lt;/code&gt; property of the client listener annotation. Example:&lt;/p&gt; &lt;pre&gt;@ClientListener( converterFactoryName = "key-value-with-previous-converter-factory", filterFactoryName = "site-filter-factory" ) public class RemoteCacheListener { … }&lt;/pre&gt; &lt;p&gt;This concludes this blog post where we looked at how the scavenger hunt game’s application layer used Red Had Data Grid to store and expose metadata information used throughout the game.&lt;/p&gt; &lt;p&gt;The Red Hat Data Grid Team&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;title=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" data-a2a-url="https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/" data-a2a-title="Using Red Hat Data Grid to power a multi-cloud real-time game"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/"&gt;Using Red Hat Data Grid to power a multi-cloud real-time game&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xt37-YNpkAI" height="1" width="1" alt=""/&gt;</content><summary>The scavenger hunt game developed for the audience to play during the Red Hat Summit 2018 demo used Red Hat Data Grid as storage for everything except the pictures taken by the participants. Data was stored across three different cloud environments using cross-site replication. In this blog post, we will look at how data was flowing through Data Grid and explain the Data Grid features powering dif...</summary><dc:creator>Galder Zamarreno</dc:creator><dc:date>2018-06-26T11:00:30Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/</feedburner:origLink></entry><entry><title>Infinispan 9.3.0.Final is out!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rvx4rV1zi7U/infinispan-930final-is-out.html" /><category term="9.3" scheme="searchisko:content:tags" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="final" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><author><name>Galder Zamarreño</name></author><id>searchisko:content:id:jbossorg_blog-infinispan_9_3_0_final_is_out</id><updated>2018-06-26T07:12:29Z</updated><published>2018-06-26T07:12:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;We're delighted to announce the release of Infinispan 9.3.0.Final, which is a culmination of several months of hard work by the entire Infinispan community. Here's a summary of what you can find within it:&lt;br /&gt;&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;First final release to work with both Java 8 and Java 10. Note that Infinispan only works in classpath mode.&lt;/li&gt;&lt;li&gt;Transaction support Hot Rod.&amp;nbsp;The java Hot Rod client can participate in Java transactions via Synchronization or XA enlistment. Note that recovery isn't supported yet.&lt;/li&gt;&lt;li&gt;Caches can now configure the maximum number of attempts to start a CacheWriter/CacheLoader on startup before cache creation fails.&lt;/li&gt;&lt;li&gt;Write-behind stores are now fault-tolerant by default.&lt;/li&gt;&lt;li&gt;Segmented On Heap Data Container. It improves performance of stream operations.&lt;/li&gt;&lt;li&gt;Server upgraded to Wildfly 13.&lt;/li&gt;&lt;li&gt;We have introduced several WildFly feature packs to make it easier for Infinispan to be utilised on WildFly instances via the Server Provisioning Plugin. The following feature packs have been created, most notably:&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-client&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;All of the modules required to connect to a hotrod server via the client&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-embedded&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;The modules required for embedded instances of Infinispan&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-embedded-query&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;The same as above but with query capabilities&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-wf-modules&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;This is equivalent to the Wildfly-modules.zip&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;li&gt;Hibernate second-level cache provider works with Hibernate ORM 5.3.&lt;/li&gt;&lt;li&gt;The Hot Rod Server allows now to use multiple protocols with a Single Port. The initial version supports HTTP/1.1, HTTP/2 and Hot Rod. Switching protocols can be done using TLS/ALPN and HTTP/1.1 Upgrade header.&lt;/li&gt;&lt;li&gt;Admin console - improved all editors (schema, scripts, JSON data) to include syntax highlighting.&lt;/li&gt;&lt;li&gt;Several enhancements in the Java Hot Rod client allowing to read and write data in different formats such as JSON, for cache operations and deployed filters/converters.&lt;/li&gt;&lt;li&gt;Cluster wide max idle expiration.&lt;/li&gt;&lt;li&gt;Component Upgrades&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Hibernate Search 5.10&lt;/li&gt;&lt;li&gt;Hibernate ORM 5.3&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Numerous bug fixes which improve stability&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;For more details, please check our &lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12310799&amp;amp;version=12336209"&gt;issue tracking release notes&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Thanks to everyone involved in this release! Onward to Infinispan 9.4!&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Cheers,&lt;/div&gt;&lt;div&gt;Galder&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/Infinispan/~4/7_fbY5uj-Uk" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rvx4rV1zi7U" height="1" width="1" alt=""/&gt;</content><summary>We're delighted to announce the release of Infinispan 9.3.0.Final, which is a culmination of several months of hard work by the entire Infinispan community. Here's a summary of what you can find within it: First final release to work with both Java 8 and Java 10. Note that Infinispan only works in classpath mode. Transaction support Hot Rod. The java Hot Rod client can participate in Java transact...</summary><dc:creator>Galder Zamarreño</dc:creator><dc:date>2018-06-26T07:12:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/Infinispan/~3/7_fbY5uj-Uk/infinispan-930final-is-out.html</feedburner:origLink></entry><entry><title>Second maintenance release for Hibernate Search 5.10</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/G20HoSfHRWI/" /><category term="elasticsearch" scheme="searchisko:content:tags" /><category term="feed_group_name_hibernate" scheme="searchisko:content:tags" /><category term="feed_name_inrelationto" scheme="searchisko:content:tags" /><category term="hibernate search" scheme="searchisko:content:tags" /><category term="releases" scheme="searchisko:content:tags" /><author><name>Yoann Rodière</name></author><id>searchisko:content:id:jbossorg_blog-second_maintenance_release_for_hibernate_search_5_10</id><updated>2018-06-25T00:00:00Z</updated><published>2018-06-25T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We just published Hibernate Search version 5.10.2.Final, the second maintenance release of Hibernate Search 5.10. This release adds previously missing handling of &lt;code&gt;minimumShouldMatch&lt;/code&gt; constraints in boolean junctions.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-s-new"&gt;&lt;a class="anchor" href="#what-s-new"&gt;&lt;/a&gt;What’s new?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Most of the changes are purely internal and related to building Hibernate Search from sources.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The only change impacting users is the introduction of &lt;code&gt;minimumShouldMatch&lt;/code&gt; constraints in boolean junctions. This was an issue for users of the Elasticsearch integration in particular, since they could create a Lucene &lt;code&gt;BooleanQuery&lt;/code&gt; with a &lt;code&gt;minimumShouldMatch&lt;/code&gt; constraint, but the constraint was ignored when the query was translated to Elasticsearch. This is no longer the case.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We took this opportunity to backport the parts of the Query DSL that allow to set &lt;code&gt;minimumShouldMatch&lt;/code&gt; constraints from the next version of Hibernate Search. You can now simply use code similar to this:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code&gt;FullTextEntityManager fullTextEntityManager = Search.getFullTextEntityManager( entityManager ); QueryBuilder queryBuilder = fullTextEntityManager.getSearchFactory() .buildQueryBuilder().forEntity( MyEntity.class ).get(); Query luceneQuery = queryBuilder.bool() .minimumShouldMatchNumber( 2 ) // This is new .should( queryBuilder.keyword().onField( "foo" ).matching( "text1" ).createQuery() ) .should( queryBuilder.keyword().onField( "bar" ).matching( "text2" ).createQuery() ) .should( queryBuilder.keyword().onField( "foobar" ).matching( "text3" ).createQuery() ) .createQuery(); FullTextQuery query = fullTextEntityManager.createFullTextQuery( query, MyEntity.class );&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;... and the resulting query will only match documents that match at least two should clauses.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There are more advanced syntaxes, please refer to the javadoc of &lt;code&gt;org.hibernate.search.query.dsl.BooleanJunction&lt;/code&gt; for more detailed information.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="how-to-get-this-release"&gt;&lt;a class="anchor" href="#how-to-get-this-release"&gt;&lt;/a&gt;How to get this release&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;All details are available and up to date on &lt;a href="http://hibernate.org/search/releases/5.10/#get-it"&gt;the dedicated page on hibernate.org&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="feedback-issues-ideas"&gt;&lt;a class="anchor" href="#feedback-issues-ideas"&gt;&lt;/a&gt;Feedback, issues, ideas?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To get in touch, use the following channels:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://stackoverflow.com/questions/tagged/hibernate-search"&gt;hibernate-search tag on Stackoverflow&lt;/a&gt; (usage questions)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/c/hibernate-search"&gt;User forum&lt;/a&gt; (usage questions, general feedback)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://hibernate.atlassian.net/browse/HSEARCH"&gt;Issue tracker&lt;/a&gt; (bug reports, feature requests)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://lists.jboss.org/pipermail/hibernate-dev/"&gt;Mailing list&lt;/a&gt; (development-related discussions)&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/G20HoSfHRWI" height="1" width="1" alt=""/&gt;</content><summary>We just published Hibernate Search version 5.10.2.Final, the second maintenance release of Hibernate Search 5.10. This release adds previously missing handling of minimumShouldMatch constraints in boolean junctions. What’s new? Most of the changes are purely internal and related to building Hibernate Search from sources. The only change impacting users is the introduction of minimumShouldMatch con...</summary><dc:creator>Yoann Rodière</dc:creator><dc:date>2018-06-25T00:00:00Z</dc:date><feedburner:origLink>http://in.relation.to/2018/06/25/hibernate-search-5-10-2-Final/</feedburner:origLink></entry><entry><title>Version 1.4 of Apiman is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aPwnEgq5TFc/release-1.4.html" /><category term="feed_group_name_apiman" scheme="searchisko:content:tags" /><category term="feed_name_apiman" scheme="searchisko:content:tags" /><author><name>Marc Savy</name></author><id>searchisko:content:id:jbossorg_blog-version_1_4_of_apiman_is_released</id><updated>2018-06-22T18:40:00Z</updated><published>2018-06-22T18:40:00Z</published><content type="html">&lt;div class="paragraph"&gt; &lt;p&gt;I&amp;#8217;m delighted to announce that Apiman 1.4 has been released (actually, 1.4.1.Final as of this blog post &lt;sup class="footnote"&gt;[&lt;a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote."&gt;1&lt;/a&gt;]&lt;/sup&gt;).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The most important change in this release is that we&amp;#8217;ve upgraded support for Elasticsearch from 1.x to 5.x. It may also support Elasticsearch 2.x, but this isn&amp;#8217;t officially supported (let us know your experiences).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A significant number of changes across the ES platform were needed to bring this improvement; including in Apiman Gateway, Apiman Manager, Apiman Metrics, test harnesses, and the ES distribution.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you experience any issues, please report them to us via &lt;a href="https://issues.jboss.org/browse/APIMAN/"&gt;JIRA&lt;/a&gt;, &lt;a href="https://github.com/apiman/apiman"&gt;GitHub&lt;/a&gt;, or &lt;a href="https://lists.jboss.org/mailman/listinfo/apiman-user"&gt;the mailing list&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="upgrading"&gt;Upgrading&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Depending on your approach, to upgrade you can simply use Apiman&amp;#8217;s &lt;a href="http://www.apiman.io/blog/apiman/introduction/overview/backup/export/import/2016/01/27/export-import.html"&gt;export-import feature&lt;/a&gt;, or upgrade the indices by following Elasticsearch&amp;#8217;s upgrade guides (likely trickier; I recommend export-import).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We also now launch and manage ES in a significantly different way than previously (as an external process), as the &lt;a href="https://www.elastic.co/blog/elasticsearch-the-server"&gt;old method is no longer supported&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="policies-can-suppressallow-headers-in-connectors"&gt;Policies can suppress/allow headers in connectors.&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new function has been added to &lt;code&gt;IPolicyContext&lt;/code&gt; which enables policy authors to explicitly suppress or allow headers that may otherwise have different treatment by default.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="java"&gt;IConnectorConfig getConnectorConfiguration();&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Using &lt;code&gt;IConnectorConfig&lt;/code&gt; a policy author may explicitly override the connector&amp;#8217;s default filtering of headers. These may vary slightly by platform, but generally would by default filter out headers such as &lt;code&gt;X-Api-Key&lt;/code&gt;. This is applied at the &lt;strong&gt;end of the policy chain&lt;/strong&gt; right before the connection is established.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This feature is useful to unblock certain headers that may otherwise be disallowed, or block headers in such a way that it would even apply to subsequent policies.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Here&amp;#8217;s an example, using the &lt;code&gt;suppressRequestHeader&lt;/code&gt; method:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="java"&gt;&lt;span class="type"&gt;void&lt;/span&gt; doApply(ApiRequest request, IPolicyContext context, ...) { &lt;span class="comment"&gt;// Get connector config&lt;/span&gt; IConnectorConfig connectorConfig = context.getConnectorConfiguration(); &lt;span class="comment"&gt;// Ban header. Connector will filter this out.&lt;/span&gt; connectorConfig.suppressRequestHeader(&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;X-SECRET&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;); &lt;span class="local-variable"&gt;super&lt;/span&gt;.doApply(request, context, config, chain); }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Even if another policy in the chain added an &lt;code&gt;X-SECRET&lt;/code&gt; header, it would still be filtered out &lt;sup class="footnote"&gt;[&lt;a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote."&gt;2&lt;/a&gt;]&lt;/sup&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We hope to expand the functionality of &lt;code&gt;IConnectorConfig&lt;/code&gt; in future to allow more control of the connector by policies than is possible presently.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="download-1-4-1-final"&gt;Download 1.4.1.Final&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-vertx-1.4.1.Final.zip"&gt;Vert.x (Gateway Only)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-wildfly10-1.4.1.Final-overlay.zip"&gt;WildFly 10 or EAP 7.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-wildfly11-1.4.1.Final-overlay.zip"&gt;WildFly 11&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-eap7-1.4.1.Final-overlay.zip"&gt;EAP 7&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-tomcat8-1.4.1.Final-overlay.zip"&gt;Tomcat 8+&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="release-notes"&gt;Release Notes&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12314121&amp;amp;version=12337953"&gt;1.4.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12314121&amp;amp;version=12338072"&gt;1.4.1.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Enhancements&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1334"&gt;APIMAN-1334&lt;/a&gt; - Allow policies to suppress/allow headers in connector.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Bugs&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1266"&gt;APIMAN-1266&lt;/a&gt; - Error when adding plugin that has previously been deleted (Oracle 12C database)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1296"&gt;APIMAN-1296&lt;/a&gt; - The API Key policy plugin (apikey-policy) expects the requestHeader property to be all lowercase as of 1.3.1&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1318"&gt;APIMAN-1318&lt;/a&gt; - Export/Import of a plugin with policies does not work because of its ID&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1320"&gt;APIMAN-1320&lt;/a&gt; - Gateway API: Clients still inserted even when invalid&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1321"&gt;APIMAN-1321&lt;/a&gt; - Elasticsearch data is deleted at tomcat shutdown&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1324"&gt;APIMAN-1324&lt;/a&gt; - index_already_exists_exception when starting WF quickstart&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1335"&gt;APIMAN-1335&lt;/a&gt; - ApiKeyPolicy from apikey-policy is throwing an NPE on null connectorConfig using the Vert.x gateway&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1337"&gt;APIMAN-1337&lt;/a&gt; - SoapAuthorizationPolicy missing i18n messages&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Tasks&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-892"&gt;APIMAN-892&lt;/a&gt; - Upgrade to Elasticsearch 5.x&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Sub-tasks&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1323"&gt;APIMAN-1323&lt;/a&gt; - Rework test harness to cope better with out of order JSON&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1325"&gt;APIMAN-1325&lt;/a&gt; - Upgrade Jest HTTP Client for ES&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1326"&gt;APIMAN-1326&lt;/a&gt; - Upgrade Gateway to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1327"&gt;APIMAN-1327&lt;/a&gt; - Upgrade Manager to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1328"&gt;APIMAN-1328&lt;/a&gt; - Upgrade metrics to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1329"&gt;APIMAN-1329&lt;/a&gt; - Update test harness and tests to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1330"&gt;APIMAN-1330&lt;/a&gt; - Update QueryBuilders to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1331"&gt;APIMAN-1331&lt;/a&gt; - Switch to EmbeddedElastic instead of (actually) embedded&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1332"&gt;APIMAN-1332&lt;/a&gt; - Update ES distro to include Elasticsearch distro zip&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1333"&gt;APIMAN-1333&lt;/a&gt; - Handle EmbeddedElastic hanging process when JVM killed ungracefully.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id="footnotes"&gt; &lt;hr&gt; &lt;div class="footnote" id="_footnotedef_1"&gt; &lt;a href="#_footnoteref_1"&gt;1&lt;/a&gt;. We fixed a couple of bugs spotted in 1.4.0.Final by the community before the blog was written &lt;/div&gt; &lt;div class="footnote" id="_footnotedef_2"&gt; &lt;a href="#_footnoteref_2"&gt;2&lt;/a&gt;. Unless it explicitly overrode your suppression! &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aPwnEgq5TFc" height="1" width="1" alt=""/&gt;</content><summary>I’m delighted to announce that Apiman 1.4 has been released (actually, 1.4.1.Final as of this blog post [1]). The most important change in this release is that we’ve upgraded support for Elasticsearch from 1.x to 5.x. It may also support Elasticsearch 2.x, but this isn’t officially supported (let us know your experiences). A significant number of changes across the ES platform were needed to bring...</summary><dc:creator>Marc Savy</dc:creator><dc:date>2018-06-22T18:40:00Z</dc:date><feedburner:origLink>http://apiman.io/blog/apiman/release/2018/06/22/release-1.4.html</feedburner:origLink></entry></feed>
